<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gaussian Process in Action | Richard Cornelius Suwandi </title> <meta name="author" content="Richard Cornelius Suwandi"> <meta name="description" content="Building a Gaussian process model with GPyTorch"> <meta name="keywords" content="academic, personal, phd"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%96%A5%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://richardcsuwandi.github.io/blog/2024/gpr/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Richard Cornelius Suwandi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">Services </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gaussian Process in Action</h1> <p class="post-meta"> Created in September 15, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/gaussian-process"> <i class="fa-solid fa-hashtag fa-sm"></i> gaussian-process</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="introduction">Introduction</h4> <p>Gaussian processes (GPs) are a powerful yet often underappreciated model in machine learning. As a non-parametric and Bayesian approach, GPs are particularly effective for supervised learning tasks such as regression and classification. Compared to other algorithms, GPs offer several practical advantages:</p> <ul> <li>They perform well even with small datasets.</li> <li>They provide uncertainty quantification for predictions.</li> </ul> <p>In this tutorial, I will demonstrate how to implement GP regression using GPyTorch. GPyTorch is a Gaussian process library built on PyTorch, designed for creating scalable and flexible GP models. To learn more about GPyTorch, I recommend visiting their <a href="https://gpytorch.ai/" rel="external nofollow noopener" target="_blank">official website</a>.</p> <p><em>Note: If you want to follow along with this tutorial, you can find the notebook of this tutorial <a href="https://github.com/richardcsuwandi/gp/blob/main/GP%20Regression%20using%20GPyTorch.ipynb" rel="external nofollow noopener" target="_blank">here</a>.</em></p> <h4 id="setup">Setup</h4> <p>Before we start, we first need to install the <code class="language-plaintext highlighter-rouge">gpytorch</code> library. You can do this either by pip or conda using the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>gpytorch <span class="c"># using pip</span>
conda <span class="nb">install </span>gpytorch <span class="nt">-c</span> gpytorch <span class="c"># using conda</span>
</code></pre></div></div> <h4 id="generating-the-data">Generating the data</h4> <p>Next, we need to generate a training data for our model. We will be modeling the following function:</p> \[y = \sin{(2\pi x)} + \epsilon, \epsilon \sim \mathcal{N}(0,0.04)\] <p>We will evaluate the above function on 15 equally-spaced points from \([0,1]\). The generated training data is depicted in the following plot:</p> <p align="center"> <img src="/assets/img/gpr_data.png"> </p> <h4 id="building-the-model">Building the model</h4> <p>Now that we have generated our training data, we can start building our GP model. GPyTorch offers a flexible way for us to build GP models, by constructing the components of the model by ourselves. This is analogous to building neural networks in the standard PyTorch library. For most GP regression models, you will need to construct the following components:</p> <ul> <li> <strong>A GP Model:</strong> For exact (i.e. non-variational) GP models we will use <code class="language-plaintext highlighter-rouge">gpytorch.models.ExactGP</code>.</li> <li> <strong>A likelihood function:</strong> The likelihood function for GP regression, we commonly use <code class="language-plaintext highlighter-rouge">gpytorch.likelihoods.GaussianLikelihood</code>.</li> <li> <strong>A mean function:</strong> The prior mean of the GP. If you don’t know which mean function to use, <code class="language-plaintext highlighter-rouge">gpytorch.means.ConstantMean</code> is usually a good place to start.</li> <li> <strong>A kernel function:</strong> The prior covariance of the GP. We will use the <a href="https://arxiv.org/pdf/1302.4245.pdf" rel="external nofollow noopener" target="_blank">spectral mixture (SM)</a> kernel (<code class="language-plaintext highlighter-rouge">gpytorch.kernels.SpectralMixtureKernels</code>) for this tutorial.</li> <li> <strong>A multivariate normal distribution:</strong> The multivariate normal distribution in GP (<code class="language-plaintext highlighter-rouge">gpytorch.distributions.MultivariateNormal</code>).</li> </ul> <p>We can build our GP model by constructing the above components as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SpectralMixtureGP</span><span class="p">(</span><span class="n">gpytorch</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">ExactGP</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SpectralMixtureGP</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">gpytorch</span><span class="p">.</span><span class="n">means</span><span class="p">.</span><span class="nc">ConstantMean</span><span class="p">()</span> <span class="c1"># Construct the mean function
</span>        <span class="n">self</span><span class="p">.</span><span class="n">cov</span> <span class="o">=</span> <span class="n">gpytorch</span><span class="p">.</span><span class="n">kernels</span><span class="p">.</span><span class="nc">SpectralMixtureKernel</span><span class="p">(</span><span class="n">num_mixtures</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># Construct the kernel function
</span>        <span class="n">self</span><span class="p">.</span><span class="n">cov</span><span class="p">.</span><span class="nf">initialize_from_data</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># Initialize the hyperparameters from data
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Evaluate the mean and kernel function at x
</span>        <span class="n">mean_x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">cov_x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Return the multivariate normal distribution using the evaluated mean and kernel function
</span>        <span class="k">return</span> <span class="n">gpytorch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">MultivariateNormal</span><span class="p">(</span><span class="n">mean_x</span><span class="p">,</span> <span class="n">cov_x</span><span class="p">)</span> 

<span class="c1"># Initialize the likelihood and model
</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">gpytorch</span><span class="p">.</span><span class="n">likelihoods</span><span class="p">.</span><span class="nc">GaussianLikelihood</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">SpectralMixtureGP</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">)</span>
</code></pre></div></div> <p>Let me breakdown the above code line-by-line:</p> <ul> <li>The above GP model has two main components: the <code class="language-plaintext highlighter-rouge">__init__</code> and <code class="language-plaintext highlighter-rouge">forward</code> method.</li> <li>The <code class="language-plaintext highlighter-rouge">__init__</code> method takes the training data and a likelihood as the inputs and constructs whatever objects are necessary for the model’s forward method. This will most commonly include objects like a mean function and a kernel function.</li> <li>The forward method takes in the data <code class="language-plaintext highlighter-rouge">x</code> and returns a multivariate normal distribution with the prior mean and covariance evaluated at <code class="language-plaintext highlighter-rouge">x</code>.</li> <li>Finally, we initialize the likelihood function for the GP model. Here, we use the Gaussian likelihood, which is the simplest likelihood function that assumes a homoskedastic noise model (i.e. all inputs have the same noise).</li> </ul> <h4 id="training-the-model">Training the model</h4> <p>Now that we have built the model, we can train the model to find the optimal hyperparameters. Training a GP model in GPyTorch is also analogous to training a neural network in the standard PyTorch library. The training loop mainly consists of the following steps:</p> <ul> <li>Setting all the parameter gradients to zero</li> <li>Calling the model and computing the loss</li> <li>Calling backward on the loss to fill in gradients</li> <li>Taking a step on the optimizer</li> </ul> <p><em>Remark: By defining our custom training loop, we can have greater flexibility in training our model. For example, it is easy to save the parameters at each step of training or use different learning rates for different parameters.</em></p> <p>The code for the training loop is given below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Put the model into training mode
</span><span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
<span class="n">likelihood</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="c1"># Use the Adam optimizer, with learning rate set to 0.1
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Use the negative marginal log-likelihood as the loss function
</span><span class="n">mll</span> <span class="o">=</span> <span class="n">gpytorch</span><span class="p">.</span><span class="n">mlls</span><span class="p">.</span><span class="nc">ExactMarginalLogLikelihood</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="c1"># Set the number of training iterations
</span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="c1"># Set the gradients from previous iteration to zero
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="c1"># Output from model
</span>    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
    <span class="c1"># Compute loss and backprop gradients
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="nf">mll</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Iter %d/%d - Loss: %.3f</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()))</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p>In the above code, we first put our model into training mode by calling <code class="language-plaintext highlighter-rouge">model.train()</code> and <code class="language-plaintext highlighter-rouge">likelihood.train()</code>. Then, we define the loss function and optimizer that we want to use in the training process. Here, we use the negative marginal log-likelihood as the loss function and Adam as the optimizer. We also need to set the number of iterations for the training loop, say 50 iterations.</p> <h4 id="making-predictions">Making predictions</h4> <p>Finally, we can make predictions using the trained model. The basic routine of evaluating the model and making predictions is given in the following code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The test data is 50 equally-spaced points from [0,5]
</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="c1"># Put the model into evaluation mode
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">likelihood</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="c1"># The gpytorch.settings.fast_pred_var flag activates LOVE (for fast variances)
# See https://arxiv.org/abs/1803.06058
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">(),</span> <span class="n">gpytorch</span><span class="p">.</span><span class="n">settings</span><span class="p">.</span><span class="nf">fast_pred_var</span><span class="p">():</span>
    <span class="c1"># Obtain the predictive mean and covariance matrix
</span>    <span class="n">f_preds</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">f_mean</span> <span class="o">=</span> <span class="n">f_preds</span><span class="p">.</span><span class="n">mean</span>
    <span class="n">f_cov</span> <span class="o">=</span> <span class="n">f_preds</span><span class="p">.</span><span class="n">covariance_matrix</span>

    <span class="c1"># Make predictions by feeding model through likelihood
</span>    <span class="n">observed_pred</span> <span class="o">=</span> <span class="nf">likelihood</span><span class="p">(</span><span class="nf">model</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span>

    <span class="c1"># Initialize plot
</span>    <span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="c1"># Get upper and lower confidence bounds
</span>    <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span> <span class="o">=</span> <span class="n">observed_pred</span><span class="p">.</span><span class="nf">confidence_region</span><span class="p">()</span>
    <span class="c1"># Plot training data as black stars
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="sh">'</span><span class="s">k*</span><span class="sh">'</span><span class="p">)</span>
    <span class="c1"># Plot predictive means as blue line
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">observed_pred</span><span class="p">.</span><span class="n">mean</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">)</span>
    <span class="c1"># Shade between the lower and upper confidence bounds
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">lower</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">upper</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">Observed Data</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Mean</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Confidence</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>There are a few things going on in the above code:</p> <ul> <li>We first generate the test data using 50 equally-spaced points from \([0, 5]\).</li> <li>We put the model into evaluation mode by calling <code class="language-plaintext highlighter-rouge">model.eval()</code> and <code class="language-plaintext highlighter-rouge">likelihood.eval()</code>.</li> <li>We use <code class="language-plaintext highlighter-rouge">gpytorch.settings.fast_pred_var()</code> to get faster predictive distributions using LOVE.</li> <li>When put into the eval mode, the trained GP model returns a <code class="language-plaintext highlighter-rouge">MultivariateNormal</code> containing the posterior mean and covariance. Thus, we can obtain the predictive mean and covariance matrix from the multivariate normal distribution.</li> <li>Finally, we plot the mean and confidence region of the fitted GP model. The <code class="language-plaintext highlighter-rouge">confidence_region()</code> method is a helper method that returns 2 standard deviations above and below the mean.</li> </ul> <p>The resulting plot is depicted below:</p> <p align="center"> <img src="/assets/img/gpr_pred.png"> </p> <p>The black stars in the above plot represent the training (observed) data, while the blue line and the shaded area represent the mean and the confidence bounds respectively. Notice how the uncertainty is reduced close to the observed points. If more data points were added, we would see the mean function adjust itself to pass through these points and the uncertainty would reduce close to the observations.</p> <h3 id="takeaways">Takeaways</h3> <p>In this tutorial, we have learned how to build a GP model using GPyTorch. Like any other libraries, there are still a lot of <a href="https://docs.gpytorch.ai/en/latest/" rel="external nofollow noopener" target="_blank">cool things</a> that you can do with GPyTorch that I didn’t cover in this tutorial. I hope this tutorial has given you a good starting point to explore GPyTorch and Gaussian processes further.</p> </div> </article> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Richard Cornelius Suwandi. Last updated: September 16, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?fa0110e8b42cec56ce96d912fd4bde74"></script> <script>addBackToTop();</script> </body> </html>