<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> No world model, no general AI | Richard Cornelius Suwandi </title> <meta name="author" content="Richard Cornelius Suwandi"> <meta name="description" content="From Ilya's prediction to Google DeepMind's proof."> <meta name="keywords" content="academic, personal, phd, richard cornelius suwandi, cuhk, cuhksz, machine learning, artificial intelligence, data science, bayesian optimization"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="/assets/css/blog.css?05e12be222e82cdeaea35e734b7e588d"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&amp;display=swap" rel="stylesheet"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700;900&amp;family=Open+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_dark.PNG?14ecb2d6d14ba0d3c8d0e1f2823062e2"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://richardcsuwandi.github.io/blog/2025/agents-world-models/"> <script src="/assets/js/theme.js?6ba506522a0f931c10c11ff001dde98f"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.center{display:block;margin-left:auto;margin-right:auto}.framed{border:1px var(--global-text-color) dashed!important;padding:20px}d-article{overflow-x:visible}.underline{text-decoration:underline}.todo{display:block;margin:12px 0;font-style:italic;color:red}.todo:before{content:"TODO: ";font-weight:bold;font-style:normal}summary{color:steelblue;font-weight:bold}summary-math{text-align:center;color:black}[data-theme="dark"] summary-math{text-align:center;color:white}details[open]{--bg:#e2edfc;color:black;border-radius:15px;padding-left:8px;background:var(--bg);outline:.5rem solid var(--bg);margin:0 0 2rem 0;font-size:80%;line-height:1.4}[data-theme="dark"] details[open]{--bg:#112f4a;color:white;border-radius:15px;padding-left:8px;background:var(--bg);outline:.5rem solid var(--bg);margin:0 0 2rem 0;font-size:80%}.box-note,.box-warning,.box-error,.box-important{padding:15px 15px 15px 10px;margin:20px 20px 20px 5px;border:1px solid #f9f9f9;border-left-width:5px;border-radius:5px 3px 3px 5px;position:relative}.box-note[title]::before,.box-warning[title]::before,.box-error[title]::before,.box-important[title]::before{content:attr(title);display:block;font-weight:bold;font-size:1.1em;margin-bottom:8px;padding-bottom:5px;border-bottom:1px solid rgba(0,0,0,0.1)}d-article .box-note{background-color:#f9f9f9;border-left-color:#9db2d8}d-article .box-note[title]::before{color:#000}d-article .box-warning{background-color:#f9f9f9;border-left-color:#f8de92}d-article .box-warning[title]::before{color:#000}d-article .box-error{background-color:#f9f9f9;border-left-color:#ddb4be}d-article .box-error[title]::before{color:#000}d-article .box-important{background-color:#f9f9f9;border-left-color:#a8c08a}d-article .box-important[title]::before{color:#000}html[data-theme='dark'] d-article .box-note{background-color:#2f2f2f;border-left-color:#9db2d8}html[data-theme='dark'] d-article .box-note[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-warning{background-color:#2f2f2f;border-left-color:#f8de92}html[data-theme='dark'] d-article .box-warning[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-error{background-color:#2f2f2f;border-left-color:#ddb4be}html[data-theme='dark'] d-article .box-error[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-important{background-color:#2f2f2f;border-left-color:#a8c08a}html[data-theme='dark'] d-article .box-important[title]::before{color:#fff;border-bottom-color:#686868}d-article aside{border:1px solid #aaa;border-radius:4px;padding:.5em .5em 0;font-size:90%}.caption{font-size:80%;line-height:1.2;text-align:left}d-citation-list .references .title{margin-bottom:-5px;line-height:1.3}d-citation-list .references .authors{margin-top:-5px;line-height:1.3}d-citation-list .references{line-height:1.3}code{background-color:#f5f5f5;color:#d73027;padding:2px 6px;border-radius:4px;font-family:'SF Mono',Monaco,'Inconsolata','Fira Code','Droid Sans Mono','Source Code Pro',monospace;font-size:.9em;font-weight:500;border:1px solid #e1e1e1}.highlight{background-color:#f8f8f8;border:1px solid #e1e1e1;border-radius:8px;margin:20px 0;box-shadow:0 1px 3px rgba(0,0,0,0.1);position:relative}.highlight pre{background-color:transparent!important;border:none!important;border-radius:0;padding:16px 20px;margin:0!important;overflow-x:auto;box-shadow:none!important}pre:not(.highlight pre){background-color:#f8f8f8;border:1px solid #e1e1e1;border-radius:8px;margin:20px 0;padding:16px 20px;overflow-x:auto;box-shadow:0 1px 3px rgba(0,0,0,0.1);position:relative}pre code{background-color:transparent;color:#2d3748;padding:0;border:0;font-size:.85em;line-height:1.6;font-weight:400;display:block}pre code{color:#2d3748}pre code .token.keyword,pre code .language-python .hljs-keyword{color:#1976d2;font-weight:600}pre code .token.string,pre code .language-python .hljs-string{color:#388e3c}pre code .token.comment,pre code .language-python .hljs-comment{color:#757575;font-style:italic}pre code .token.number,pre code .language-python .hljs-number{color:#d32f2f}pre code .token.function,pre code .token.class-name,pre code .language-python .hljs-title{color:#7b1fa2;font-weight:600}pre code .token.builtin,pre code .language-python .hljs-built_in{color:#1976d2}pre code{white-space:pre;line-height:1.6}html[data-theme='dark'] code{background-color:#2d3748;color:#fbb6ce;border:1px solid #4a5568}html[data-theme='dark'] .highlight{background-color:#1a202c!important;border:1px solid #2d3748!important;box-shadow:none!important;border-radius:8px}
html[data-theme='dark'] pre,html[data-theme='dark'] .highlight pre{background-color:transparent!important;border:none!important;box-shadow:none!important;margin:0!important;padding:16px 20px!important}html[data-theme='dark'] pre:not(.highlight pre){background-color:#1a202c!important;border:1px solid #2d3748!important;border-radius:8px}html[data-theme='dark'] pre code{background-color:transparent!important;color:#e2e8f0!important;border:none!important}html[data-theme='dark'] pre code,html[data-theme='dark'] .highlight code{background-color:transparent!important;color:#e2e8f0!important;border:none!important}html[data-theme='dark'] pre code .token.keyword,html[data-theme='dark'] pre code .language-python .hljs-keyword{color:#81d4fa;font-weight:600}html[data-theme='dark'] pre code .token.string,html[data-theme='dark'] pre code .language-python .hljs-string{color:#a5d6a7}html[data-theme='dark'] pre code .token.comment,html[data-theme='dark'] pre code .language-python .hljs-comment{color:#90a4ae;font-style:italic}html[data-theme='dark'] pre code .token.number,html[data-theme='dark'] pre code .language-python .hljs-number{color:#ffab91}html[data-theme='dark'] pre code .token.function,html[data-theme='dark'] pre code .token.class-name,html[data-theme='dark'] pre code .language-python .hljs-title{color:#ce93d8;font-weight:600}html[data-theme='dark'] pre code .token.builtin,html[data-theme='dark'] pre code .language-python .hljs-built_in{color:#81d4fa}pre::before{content:"Python";position:absolute;top:8px;right:12px;background-color:rgba(0,0,0,0.1);color:#666;padding:2px 8px;border-radius:4px;font-size:.75em;font-weight:500;text-transform:uppercase;letter-spacing:.5px;backdrop-filter:blur(5px)}pre.bibtex::before,pre.language-bibtex::before{content:"BibTeX"!important}html[data-theme='dark'] pre::before{background-color:rgba(255,255,255,0.1);color:#a0aec0}.citation-box{background-color:#f5f5f5;border-radius:8px;padding:16px 20px;margin:20px 0;font-family:'SF Mono',Monaco,'Inconsolata','Fira Mono',monospace;font-size:.9em;line-height:1.6;overflow-x:auto}html[data-theme='dark'] .citation-box{background-color:#2d3748}@media(max-width:768px){pre{margin:15px -10px;border-radius:0;border-left:0;border-right:0}code{font-size:.85em}pre code{font-size:.8em}}</style> <style type="text/css">d-byline{max-width:100%!important;width:100%!important}d-byline .byline{max-width:100%!important;width:100%!important;display:grid!important;grid-template-columns:minmax(150px,1fr) minmax(250px,2fr) minmax(200px,1fr);gap:4rem;align-items:start}d-byline h3{margin-bottom:.5rem!important;font-weight:bold;white-space:nowrap}d-byline .author,d-byline .authors{white-space:nowrap;overflow:visible}d-byline .affiliations,d-byline .affiliation{white-space:nowrap;overflow:visible}d-byline .published{white-space:nowrap}d-byline>div{margin-right:0!important}@media(min-width:1024px){d-contents{position:sticky!important;top:80px!important;z-index:500!important;max-height:calc(100vh - 120px)!important;overflow-y:auto!important;margin-top:0!important}}@media(max-width:1023px){d-contents{position:static!important;top:auto!important;max-height:none!important;overflow-y:visible!important}}d-contents nav>div>a{font-weight:bold!important}d-contents nav ul li a{font-weight:normal!important}h1,h2,h3,h4,h5,h6{scroll-margin-top:100px!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "No world model, no general AI",
            "description": "From Ilya's prediction to Google DeepMind's proof.",
            "published": "June 11, 2025",
            "authors": [
              
              {
                "author": "Richard Cornelius Suwandi",
                "authorURL": "https://richardcsuwandi.github.io/",
                "affiliations": [
                  {
                    "name": "The Chinese University of Hong Kong, Shenzhen",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <style>#navbar{padding:.25rem 0!important;min-height:auto!important}#navbar .container{max-width:75%!important;padding:0 2rem!important}@media(max-width:768px){#navbar .container{max-width:95%!important;padding:0 1rem!important}}#navbar .navbar-nav .nav-link{padding:.25rem .75rem!important;margin:0!important}#navbar .navbar-brand.social{margin:0!important;padding:.25rem 0!important}#navbar .navbar-brand.social a{margin:0 .25rem!important}#navbar .toggle-container{margin:0!important;padding:.25rem 0!important;margin-top:-0.5rem!important}@media(max-width:991px){#navbar .toggle-container{margin-top:0!important;display:flex!important;align-items:center!important;height:100%!important}#navbar .toggle-container button{margin:0!important;padding:.5rem!important}}#navbar .navbar-toggler{padding:.25rem 0!important;border:none!important}#navbar .navbar-nav .nav-link{font-weight:normal!important}</style> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%72%69%63%68%61%72%64%73%75%77%61%6E%64%69@%6C%69%6E%6B.%63%75%68%6B.%65%64%75.%63%6E" title="email" class="social-icon" style="margin-right: 15px;"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=28o0CkgAAAAJ" title="Google Scholar" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/richardcsuwandi" title="GitHub" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/richardcsuwandi" title="LinkedIn" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/richardcsuwandi" title="X" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>No world model, no general AI</h1> <p>From Ilya's prediction to Google DeepMind's proof.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#do-we-need-a-world-model">Do we need a world model?</a> </div> <ul> <li> <a href="#ilya-was-right-all-along">Ilya was right all along?</a> </li> </ul> <div> <a href="#agents-and-world-models">Agents and world models</a> </div> <ul> <li> <a href="#environment">Environment</a> </li> <li> <a href="#goals">Goals</a> </li> <li> <a href="#agents">Agents</a> </li> <li> <a href="#world-models">World Models</a> </li> </ul> <div> <a href="#how-to-recover-the-world-model">How to recover the world model?</a> </div> <div> <a href="#experiments">Experiments</a> </div> <ul> <li> <a href="#connection-to-related-works">Connection to related works</a> </li> </ul> <div> <a href="#takeaways">Takeaways</a> </div> </nav> </d-contents> <p>Imagine if we could build an AI that thinks and plans like a human. Recent breakthroughs in large language models (LLMs) have brought us closer to this goal. As these models grow larger and trained on more data, they develop the so-called <em>emergent abilities</em><d-cite key="wei2022emergent"></d-cite> that significantly improve their performance on a wide range of downstream tasks. This has sparked a new wave of research into creating general AI agents that can tackle complex, long-horizon tasks in the real world environments<d-cite key="yao2023react"></d-cite><d-cite key="hao2023reasoning"></d-cite>. But here is the fascinating part: humans do not just react to what they see, we build rich <a href="https://www.youtube.com/watch?v=OKkEdTchsiE" rel="external nofollow noopener" target="_blank">mental models</a><d-cite key="johnson1983mental"></d-cite><d-cite key="lecun2022path"></d-cite> of how the world works. These <a href="https://worldmodels.github.io" rel="external nofollow noopener" target="_blank">world models</a><d-cite key="ha2018world"></d-cite> help us set ambitious goals<d-cite key="locke2013goal"></d-cite> and make thoughtful plans<d-cite key="bratman1987intention"></d-cite>. Hence, based on this observation, it is natural to ask:</p> <blockquote> <p>‚ÄúIs learning a world model useful to achieve a human-level AI?‚Äù</p> </blockquote> <p>Recently, researchers at Google DeepMind showed that learning a world model is not only beneficial, but also <em>necessary</em> for general agents<d-cite key="richens2025generalagentsneedworld"></d-cite>. In this post, we will discuss the key findings from the paper and the implications for the future of AI agents.</p> <h2 id="do-we-need-a-world-model">Do we need a world model?</h2> <p>In 1991, <a href="https://en.wikipedia.org/wiki/Rodney_Brooks" rel="external nofollow noopener" target="_blank">Rodney Brooks</a> made a famous claim that ‚Äúthe world is its own best model‚Äù<d-cite key="brooks1991intelligence"></d-cite>.</p> <p><img src="/assets/img/brooks_intelligence.png" alt="Intelligence without representation" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> In <em>Intelligence without representation</em>, Rodney Brooks famously proposed that ‚Äúthe world is its own best model‚Äù.</p> </div> <p>He argued that intelligent behavior could emerge naturally from model-free agents simply by interacting with their environment through a cycle of actions and perceptions, without needing to build explicit representations of how the world works. Brooks‚Äô argument has been strongly supported by the remarkable success of <em>model-free</em> agents, which have demonstrated impressive generalization capabilities across diverse tasks and environments<d-cite key="reed2022generalist"></d-cite><d-cite key="driess2023palm"></d-cite><d-cite key="raad2024scaling"></d-cite>. This model-free approach offers an appealing path to creating general AI agents while avoiding the complexities of learning explicit world models. However, recent works suggest an intriguing possibility: even these supposedly model-free agents might be learning <em>implicit</em> world models<d-cite key="li2023emergent"></d-cite> and planning algorithms<d-cite key="hou2023towards"></d-cite> beneath the surface.</p> <h3 id="ilya-was-right-all-along">Ilya was right all along?</h3> <p>Looking back to March 2023, <a href="https://en.wikipedia.org/wiki/Ilya_Sutskever" rel="external nofollow noopener" target="_blank">Ilya Sutskever</a> made a profound claim that large neural networks are doing far more than just next-word prediction and are actually <a href="https://www.youtube.com/watch?v=I6qQinoY9WM" rel="external nofollow noopener" target="_blank">learning ‚Äúworld models‚Äù</a>. The way he put it,</p> <div class="center"> <blockquote class="twitter-tweet" data-media-max-width="560"> <p lang="en" dir="ltr">In March 2023, <a href="https://twitter.com/ilyasut?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@ilyasut</a> made a profound claim about LLMs learning "world models". Now, researchers at <a href="https://twitter.com/GoogleDeepMind?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">@GoogleDeepMind</a> have proven he glimpsed something even deeper; a fundamental law governing ALL intelligent agents. üßµ <a href="https://t.co/MsMx8snUZs" rel="external nofollow noopener" target="_blank">pic.twitter.com/MsMx8snUZs</a></p>‚Äî ohqay (@ohqayy) <a href="https://twitter.com/ohqayy/status/1930418880696201317?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">June 5, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>He believed that what neural networks learn are not just textual information, but rather a compressed representation of our world. Thus, the more accurately we can predict the next word, the higher fidelity of the world model we can achieve.</p> <h2 id="agents-and-world-models">Agents and world models</h2> <p>While Ilya‚Äôs claim was intriguing, it was not clear how to formalize it at that time. But now, <a href="https://x.com/jonathanrichens/status/1930221408199516657" rel="external nofollow noopener" target="_blank">researchers at Google DeepMind</a> have proven that what Ilya said is not just a hypothesis, but a fundamental law governing <em>all</em> general agents<d-cite key="richens2025generalagentsneedworld"></d-cite>. In the paper, the authors showed that,</p> <blockquote> <p>‚ÄúAny agent capable of generalizing to a broad range of simple goal-directed tasks must have learned a predictive model capable of simulating its environment, and this model can always be recovered from the agent.‚Äù</p> </blockquote> <p><img src="/assets/img/agents_learn_world_models.jpeg" alt="Agents learn world models" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Any agent satisfying a regret bound must have learned an environment transition function, which can be extracted from its goal-conditional policy. This holds true for agents that can handle basic tasks like reaching specific states.</p> </div> <ol> <li> <em>There is no ‚Äúmodel-free shortcut‚Äù to building general AI agents</em>. If we want agents that generalize to diverse tasks, we cannot avoid learning world models.</li> <li> <em>Better performance requires better world models</em>. The only path to lower regret or handling more complex goals is through learning increasingly accurate world models.</li> </ol> <aside class="l-body box-note" title="Note"> <p>The above only holds true for agents that plan over multi-step horizons, as they need to understand how actions affect future states. However, ‚Äúmyopic‚Äù agents that only consider immediate rewards can potentially avoid learning world models since they do not need to predict long-term consequences.</p> </aside> <p>To make the above claims more precise, the authors develop a rigorous mathematical framework built on 4 key components: environments, goals, agents, and world models.</p> <h3 id="environments">Environments</h3> <p>The environment is assumed to be a controlled Markov process (cMP)<d-cite key="sutton1998reinforcement"></d-cite>, which is essentially a Markov decision process without a specified reward function. A cMP consists of a state space \(\boldsymbol{S}\), an action space \(\boldsymbol{A}\), and a transition function \(P_{ss'}(a) = P(S_{t+1} = s' \mid A_t = a, S_t = s)\). The authors assume the environment is irreducible<d-footnote>A Markov process is irreducible if every state is reachable from every other state.</d-footnote> and stationary<d-footnote>A Markov process is stationary if the transition probabilities do not change over time.</d-footnote>.</p> <h3 id="goals">Goals</h3> <p>Rather than defining complex goal structures, the paper focused on simple, intuitive goals expressed in Linear Temporal Logic (LTL)<d-cite key="pnueli1977temporal"></d-cite><d-cite key="baier2008principles"></d-cite>. A goal \(\varphi\) has the form \(\varphi = \mathcal{O}([(s,a) \in \boldsymbol{g}])\) where \(\boldsymbol{g}\) is a set of goal states and \(\mathcal{O} \in \{\bigcirc, \diamond, \top\}\) specifies the time horizon (\(\bigcirc\) = next, \(\diamond\) = eventually, \(\top\) = now). More complex composite goals \(\psi\) can be formed by combining sequential goals in ordered sequences: \(\psi = \langle\varphi_1, \varphi_2, \ldots, \varphi_n\rangle\) where the agent must achieve each sub-goal in order. The depth of a goal as the number of sub-goals: \(\text{depth}(\psi) = n\).</p> <h3 id="agents">Agents</h3> <p>The authors focused on <em>goal-conditioned agents</em><d-cite key="liu2022goal"></d-cite>, which are defined as a policy \(\pi(a_t \mid h_t; \psi)\) that maps a history \(h_t\) to an action \(a_t\) conditioned on a goal \(\psi\). This leads to a natural definition of an optimal goal-conditioned agent for a given environment and set of goals \(\boldsymbol{\Psi}\), which is a policy that maximizes the probability that \(\psi\) is achieved, for all \(\psi \in \boldsymbol{\Psi}\). However, real agents are rarely optimal, especially when operating in complex environments and for tasks that require coordinating many sub-goals over long time horizons. Instead of requiring perfect optimality, the authors define a <em>bounded</em> agent that is capable of achieving goals of some maximum goal depth with a failure rate that is bounded relative to the optimal agent. A bounded goal-conditioned agent \(\pi(a_t \mid h_t; \psi)\) satisfies:</p> \[P(\tau \models \psi \mid \pi, s_0) \geq \max_{\pi'} P(\tau \models \psi \mid \pi', s_0)(1-\delta)\] <p>for all goals \(\psi \in \boldsymbol{\Psi}_n\), where \(\boldsymbol{\Psi}_n\) is the set of all composite goals with depth at most \(n\) and \(\delta \in [0,1]\) is the failure rate parameter.</p> <h3 id="world-models">World Models</h3> <p>The authors considered the predictive world models, which can be used by agents to plan. They defined a world model as any approximation \(\hat{P}_{ss'}(a)\) of the transition function of the environment \(P_{ss'}(a) = P(S_{t+1} = s' \mid A_t = a, S_t = s)\), with bounded error \(\left|\hat{P}_{ss'}(a) - P_{ss'}(a)\right| \leq \varepsilon\). The authors showed that, for any such bounded goal-conditioned agent, an approximation of the environment‚Äôs transition function (a world model) can be recovered from the agent‚Äôs policy alone:</p> <div class="box-important" title="Theorem"> <p>Let \(\pi\) be a goal-conditioned agent with maximum failure rate \(\delta\) for all goals \(\psi \in \boldsymbol{\Psi}_n\) where \(n &gt; 1\). Then \(\pi\) fully determines a model \(\hat{P}_{ss'}(a)\) for the environment transition probabilities with bounded error:</p> \[\left|\hat{P}_{ss'}(a) - P_{ss'}(a)\right| \leq \sqrt{\frac{2P_{ss'}(a)(1-P_{ss'}(a))}{(n-1)(1-\delta)}}\] <p>For \(\delta \ll 1\) and \(n \gg 1\), the error scales as \(\mathcal{O}(\delta/\sqrt{n}) + \mathcal{O}(1/n)\).</p> </div> <p>The above result reveals two crucial insights:</p> <ol> <li>As agents become more competent (\(\delta \to 0\)), the recoverable world model becomes more accurate.</li> <li>As agents handle longer-horizon goals (larger \(n\)), they must learn increasingly precise world models.</li> </ol> <p>It also implies that learning a sufficiently general goal-conditioned policy is <em>informationally equivalent</em> to learning an accurate world model.</p> <h2 id="how-to-recover-the-world-model">How to recover the world model?</h2> <p>The authors also derived an algorithm to recover the world model from a bounded agent. The algorithm works by querying the agent with carefully designed composite goals that correspond to ‚Äúeither-or‚Äù decisions. For instance, it presents goals like ‚Äúachieve transition \((s,a) \to s'\) at most \(r\) times out of \(n\) attempts‚Äù versus ‚Äúachieve it more than \(r\) times‚Äù. The agent‚Äôs choice of action reveals information about which outcome has higher probability, allowing us to estimate \(P_{ss'}(a)\).</p> <p><img src="/assets/img/recovering_world_models.png" alt="Algorithm for recovering world models" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> The derived algorithm for recovering a world model from a bounded agent.</p> </div> <h2 id="experiments">Experiments</h2> <p>To test the effectiveness of the algorithm, the authors conducted experiments on a randomly generated controlled Markov process with 20 states and 5 actions, featuring a sparse transition function to make learning more challenging. They trained agents using trajectories sampled from the environment under a random policy, increasing agent competency by extending the training trajectory length (\(N_{\text{samples}}\)). The results show that:</p> <ul> <li>Even when agents strongly violated the theoretical assumptions (achieving worst-case regret \(\delta = 1\) for some goals), their algorithm still recovered accurate world models.</li> <li>The average error in recovered world models decreased as \(\mathcal{O}(n^{-1/2})\), matching the theoretical scaling relationship between error bounds and goal depth.</li> <li>As agents learned to handle longer-horizon goals (larger maximum depth \(n\)), the extracted world models became increasingly accurate. This confirms the fundamental link between agent capabilities and world model quality.</li> </ul> <p><img src="/assets/img/world_model_error.png" alt="Error in recovered world models" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> a) Mean error in recovered world model decreases as agent handles deeper goals. b) Mean error scales with agent‚Äôs regret at depth 50. Error bars show 95% confidence intervals over 10 experiments.</p> </div> <h3 id="connection-to-related-works">Connection to related works</h3> <p>The results of this work complement several other areas of AI research:</p> <ul> <li>The proposed algorithm completes a the ‚Äútriangle‚Äù between environment, goal, and policy. Planning determines an optimal policy given a world model and goal (world model + goal ‚Üí policy), while inverse reinforcement learning (IRL)<d-cite key="ng2000algorithms"></d-cite> recovers goals given a world model and policy (world model + policy ‚Üí goal). The proposed algorithm fills in the remaining direction by recovering the world model given the agent‚Äôs policy and goal (policy + goal ‚Üí world model). Just as IRL requires observing policies across multiple environments to fully determine goals<d-cite key="amin2016towards"></d-cite>, the algorithm needs to observe the agent‚Äôs behavior across multiple goals to fully recover the world model.</li> </ul> <p><img src="/assets/img/triangle_planning.jpeg" alt="The triangle of environment, goal, and policy" class="l-body rounded z-depth-1 center" width="60%"></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> While planning uses a world model and a goal to determine a policy, and IRL and inverse planning use an agent‚Äôs policy and a world model to identify its goal, the proposed algorithm uses an agent‚Äôs policy and its goal to identify a world model.</p> </div> <ul> <li> <p>Traditional mechanistic interpretability (MI) typically relies on analyzing neural network activations<d-cite key="li2023emergent"></d-cite> or using supervised probes<d-cite key="alain2016understanding"></d-cite>, on the other hand, the proposed algorithm provides a novel approach that extracts world models directly from an agent‚Äôs policy behavior, making it applicable even when model internals are inaccessible. This unsupervised and architecture-agnostic approach works for any agent that satisfies the bounded regret condition, regardless of its specific implementation. For LLMs, this means we can potentially uncover their implicit world models by analyzing their goal-directed behavior, without needing to access their internal representations.</p> </li> <li> <p>A recent work<d-cite key="richens2024robust"></d-cite> showed that agents adapting to distributional shifts must learn causal world models. This work complements it by focusing on task generalization rather than domain generalization. Interestingly, domain generalization requires deeper causal understanding than task generalization. For example, in a system where state variables \(X\) and \(Y\) are causally related (\(X \to Y\)), an agent can achieve optimal task performance by learning just the transition probabilities, without needing to understand the underlying causal relationship. This suggests an agential version of Pearl‚Äôs causal hierarchy<d-cite key="bareinboim2022pearl"></d-cite>, where different agent capabilities (like domain or task generalization) require different levels of causal knowledge.</p> </li> </ul> <aside class="l-body box-warning" title="Remark"> <p>The findings also have significant implications for AI development and safety. The emergence of new capabilities in large language models and other AI systems could be explained by implicit world models learned while optimizing for diverse training tasks. The ability to extract world models from capable agents provides a new tool for verification and alignment, as model fidelity scales with agent capability. However, the inherent difficulty of learning accurate world models of complex real-world systems also places fundamental limits on general agent capabilities.</p> </aside> <h2 id="takeaways">Takeaways</h2> <p>Perhaps Ilya‚Äôs 2023 prediction was more prophetic than we realized. If the above results hold true, then the current race toward artificial superintelligence (ASI) through scaling language models might be secretly a race toward building more sophisticated world models. It is also possible that we may be witnessing something even more profound: the transition from what David Silver and Richard Sutton call the <a href="https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf" rel="external nofollow noopener" target="_blank">‚ÄúEra of Human Data‚Äù to the ‚ÄúEra of Experience‚Äù</a>. While current AI systems have achieved remarkable capabilities by imitating human-generated data, Silver and Sutton argue that superhuman intelligence will emerge through agents learning predominantly from their own experience<d-cite key="silver2025welcome"></d-cite>. For example, with recent developments in foundation world models like <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/" rel="external nofollow noopener" target="_blank">Genie 2</a>, we can generate endless 3D environments from single images<d-cite key="parker2024genie"></d-cite> and allow agents to inhabit ‚Äústreams of experience‚Äù in richly grounded environments that adapt and evolve with their capabilities.</p> <video src="/assets/img/genie2.mp4" alt="Genie 2" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> Genie 2, a foundation world model capable of generating an endless variety of action-controllable, playable 3D environments for training and evaluating embodied agents. Based on a single prompt image, it can be played by a human or AI agent using keyboard and mouse inputs.</p> </div> <p>If general agents must learn world models, and superhuman intelligence requires learning from experience rather than human data, then foundation world models like Genie 2 might be the ultimate scaling law for the Era of Experience. Rather than hitting the ceiling of human knowledge, we are entering a phase where the quality of AI agents is fundamentally limited by the fidelity of the worlds they can simulate and explore. The agent that can dream the most accurate dreams, and learn the most from those dreams, might just be the most intelligent.</p> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (Jun 2025). No world model, no general AI. https://richardcsuwandi.github.io/blog/2025/agents-world-models/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2025agentsworldmodels</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"No world model, no general AI"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"richardcsuwandi.github.io"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"Jun"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2025/agents-world-models/"</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-06-10-agents-world-models.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"richardcsuwandi/richardcsuwandi.github.io","data-repo-id":"R_kgDOL_rwfw","data-category":"General","data-category-id":"DIC_kwDOL_rwf84CigRl","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <style>footer{background-color:#fff!important;border-top:1px solid #fff!important}[data-theme="dark"] footer{background-color:#1a1a1a!important;border-top:1px solid #1a1a1a!important;color:#1a1a1a!important}.back-to-top{background-color:#6c757d!important;border-color:#6c757d!important;color:white!important}.back-to-top:hover{background-color:#5a6268!important;border-color:#545b62!important;color:white!important}[data-theme="dark"] .back-to-top{background-color:#6c757d!important;border-color:#6c757d!important;color:white!important}[data-theme="dark"] .back-to-top:hover{background-color:#5a6268!important;border-color:#545b62!important;color:white!important}</style> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> ¬© Copyright 2025 Richard Cornelius Suwandi. Last updated: July 22, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SYSQ70M95W"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SYSQ70M95W");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>