<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Science of Intelligent Exploration | Richard Cornelius Suwandi </title> <meta name="author" content="Richard Cornelius Suwandi"> <meta name="description" content="Why we need to re-center exploration in AI"> <meta name="keywords" content="academic, personal, phd, richard cornelius suwandi, cuhk, cuhksz, machine learning, artificial intelligence, data science, bayesian optimization"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="/assets/css/blog.css?299c94cb50b62039c064ffe297befe4e"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&amp;display=swap" rel="stylesheet"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700;900&amp;family=Open+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_dark.PNG?14ecb2d6d14ba0d3c8d0e1f2823062e2"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://richardcsuwandi.github.io/blog/2025/exploration-in-ai/"> <script src="/assets/js/theme.js?6ba506522a0f931c10c11ff001dde98f"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.center{display:block;margin-left:auto;margin-right:auto}.framed{border:1px var(--global-text-color) dashed!important;padding:20px}d-article{overflow-x:visible}.underline{text-decoration:underline}.todo{display:block;margin:12px 0;font-style:italic;color:red}.todo:before{content:"TODO: ";font-weight:bold;font-style:normal}summary{color:steelblue;font-weight:bold}summary-math{text-align:center;color:black}[data-theme="dark"] summary-math{text-align:center;color:white}details[open]{--bg:#e2edfc;color:black;border-radius:15px;padding-left:8px;background:var(--bg);outline:.5rem solid var(--bg);margin:0 0 2rem 0;font-size:80%;line-height:1.4}[data-theme="dark"] details[open]{--bg:#112f4a;color:white;border-radius:15px;padding-left:8px;background:var(--bg);outline:.5rem solid var(--bg);margin:0 0 2rem 0;font-size:80%}.box-note,.box-warning,.box-error,.box-important{padding:15px 15px 15px 10px;margin:20px 20px 20px 5px;border:1px solid #f9f9f9;border-left-width:5px;border-radius:5px 3px 3px 5px;position:relative}.box-note[title]::before,.box-warning[title]::before,.box-error[title]::before,.box-important[title]::before{content:attr(title);display:block;font-weight:bold;font-size:1.1em;margin-bottom:8px;padding-bottom:5px;border-bottom:1px solid rgba(0,0,0,0.1)}d-article .box-note{background-color:#f9f9f9;border-left-color:#9db2d8}d-article .box-note[title]::before{color:#000}d-article .box-warning{background-color:#f9f9f9;border-left-color:#f8de92}d-article .box-warning[title]::before{color:#000}d-article .box-error{background-color:#f9f9f9;border-left-color:#ddb4be}d-article .box-error[title]::before{color:#000}d-article .box-important{background-color:#f9f9f9;border-left-color:#a8c08a}d-article .box-important[title]::before{color:#000}html[data-theme='dark'] d-article .box-note{background-color:#2f2f2f;border-left-color:#9db2d8}html[data-theme='dark'] d-article .box-note[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-warning{background-color:#2f2f2f;border-left-color:#f8de92}html[data-theme='dark'] d-article .box-warning[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-error{background-color:#2f2f2f;border-left-color:#ddb4be}html[data-theme='dark'] d-article .box-error[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-important{background-color:#2f2f2f;border-left-color:#a8c08a}html[data-theme='dark'] d-article .box-important[title]::before{color:#fff;border-bottom-color:#686868}d-article aside{border:1px solid #aaa;border-radius:4px;padding:.5em .5em 0;font-size:90%}.caption{font-size:80%;line-height:1.2;text-align:left}d-citation-list .references .title{margin-bottom:-5px;line-height:1.3}d-citation-list .references .authors{margin-top:-5px;line-height:1.3}d-citation-list .references{line-height:1.3}code{background-color:#f5f5f5;color:#d73027;padding:2px 6px;border-radius:4px;font-family:'SF Mono',Monaco,'Inconsolata','Fira Code','Droid Sans Mono','Source Code Pro',monospace;font-size:.9em;font-weight:500;border:1px solid #e1e1e1}.highlight{background-color:#f8f8f8;border:1px solid #e1e1e1;border-radius:8px;margin:20px 0;box-shadow:0 1px 3px rgba(0,0,0,0.1);position:relative}.highlight pre{background-color:transparent!important;border:none!important;border-radius:0;padding:16px 20px;margin:0!important;overflow-x:auto;box-shadow:none!important}pre:not(.highlight pre){background-color:#f8f8f8;border:1px solid #e1e1e1;border-radius:8px;margin:20px 0;padding:16px 20px;overflow-x:auto;box-shadow:0 1px 3px rgba(0,0,0,0.1);position:relative}pre code{background-color:transparent;color:#2d3748;padding:0;border:0;font-size:.85em;line-height:1.6;font-weight:400;display:block}pre code{color:#2d3748}pre code .token.keyword,pre code .language-python .hljs-keyword{color:#1976d2;font-weight:600}pre code .token.string,pre code .language-python .hljs-string{color:#388e3c}pre code .token.comment,pre code .language-python .hljs-comment{color:#757575;font-style:italic}pre code .token.number,pre code .language-python .hljs-number{color:#d32f2f}pre code .token.function,pre code .token.class-name,pre code .language-python .hljs-title{color:#7b1fa2;font-weight:600}pre code .token.builtin,pre code .language-python .hljs-built_in{color:#1976d2}pre code{white-space:pre;line-height:1.6}html[data-theme='dark'] code{background-color:#2d3748;color:#fbb6ce;border:1px solid #4a5568}html[data-theme='dark'] .highlight{background-color:#1a202c!important;border:1px solid #2d3748!important;box-shadow:none!important;border-radius:8px}
html[data-theme='dark'] pre,html[data-theme='dark'] .highlight pre{background-color:transparent!important;border:none!important;box-shadow:none!important;margin:0!important;padding:16px 20px!important}html[data-theme='dark'] pre:not(.highlight pre){background-color:#1a202c!important;border:1px solid #2d3748!important;border-radius:8px}html[data-theme='dark'] pre code{background-color:transparent!important;color:#e2e8f0!important;border:none!important}html[data-theme='dark'] pre code,html[data-theme='dark'] .highlight code{background-color:transparent!important;color:#e2e8f0!important;border:none!important}html[data-theme='dark'] pre code .token.keyword,html[data-theme='dark'] pre code .language-python .hljs-keyword{color:#81d4fa;font-weight:600}html[data-theme='dark'] pre code .token.string,html[data-theme='dark'] pre code .language-python .hljs-string{color:#a5d6a7}html[data-theme='dark'] pre code .token.comment,html[data-theme='dark'] pre code .language-python .hljs-comment{color:#90a4ae;font-style:italic}html[data-theme='dark'] pre code .token.number,html[data-theme='dark'] pre code .language-python .hljs-number{color:#ffab91}html[data-theme='dark'] pre code .token.function,html[data-theme='dark'] pre code .token.class-name,html[data-theme='dark'] pre code .language-python .hljs-title{color:#ce93d8;font-weight:600}html[data-theme='dark'] pre code .token.builtin,html[data-theme='dark'] pre code .language-python .hljs-built_in{color:#81d4fa}pre::before{content:"Python";position:absolute;top:8px;right:12px;background-color:rgba(0,0,0,0.1);color:#666;padding:2px 8px;border-radius:4px;font-size:.75em;font-weight:500;text-transform:uppercase;letter-spacing:.5px;backdrop-filter:blur(5px)}pre.bibtex::before,pre.language-bibtex::before{content:"BibTeX"!important}html[data-theme='dark'] pre::before{background-color:rgba(255,255,255,0.1);color:#a0aec0}.citation-box{background-color:#f5f5f5;border-radius:8px;padding:16px 20px;margin:20px 0;font-family:'SF Mono',Monaco,'Inconsolata','Fira Mono',monospace;font-size:.9em;line-height:1.6;overflow-x:auto}html[data-theme='dark'] .citation-box{background-color:#2d3748}@media(max-width:768px){pre{margin:15px -10px;border-radius:0;border-left:0;border-right:0}code{font-size:.85em}pre code{font-size:.8em}}</style> <style type="text/css">d-byline{max-width:100%!important;width:100%!important}d-byline .byline{max-width:100%!important;width:100%!important;display:grid!important;grid-template-columns:minmax(150px,1fr) minmax(250px,2fr) minmax(200px,1fr);gap:4rem;align-items:start}d-byline h3{margin-bottom:.5rem!important;font-weight:bold;white-space:nowrap}d-byline .author,d-byline .authors{white-space:nowrap;overflow:visible}d-byline .affiliations,d-byline .affiliation{white-space:nowrap;overflow:visible}d-byline .published{white-space:nowrap}d-byline>div{margin-right:0!important}@media(min-width:1024px){d-contents{position:sticky!important;top:80px!important;z-index:500!important;max-height:calc(100vh - 120px)!important;overflow-y:auto!important;margin-top:0!important}}@media(max-width:1023px){d-contents{position:static!important;top:auto!important;max-height:none!important;overflow-y:visible!important}}d-contents nav>div>a{font-weight:bold!important}d-contents nav ul li a{font-weight:normal!important}h1,h2,h3,h4,h5,h6{scroll-margin-top:100px!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Science of Intelligent Exploration",
            "description": "Why we need to re-center exploration in AI",
            "published": "July 23, 2025",
            "authors": [
              
              {
                "author": "Richard Cornelius Suwandi",
                "authorURL": "https://richardcsuwandi.github.io/",
                "affiliations": [
                  {
                    "name": "The Chinese University of Hong Kong, Shenzhen",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <style>#navbar{padding:.25rem 0!important;min-height:auto!important}#navbar .container{max-width:75%!important;padding:0 2rem!important}@media(max-width:768px){#navbar .container{max-width:95%!important;padding:0 1rem!important}}#navbar .navbar-nav .nav-link{padding:.25rem .75rem!important;margin:0!important}#navbar .navbar-brand.social{margin:0!important;padding:.25rem 0!important}#navbar .navbar-brand.social a{margin:0 .25rem!important}#navbar .toggle-container{margin:0!important;padding:.25rem 0!important;margin-top:-0.5rem!important}@media(max-width:991px){#navbar .toggle-container{margin-top:0!important;display:flex!important;align-items:center!important;height:100%!important}#navbar .toggle-container button{margin:0!important;padding:.5rem!important}}#navbar .navbar-toggler{padding:.25rem 0!important;border:none!important}#navbar .navbar-nav .nav-link{font-weight:normal!important}</style> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%72%69%63%68%61%72%64%73%75%77%61%6E%64%69@%6C%69%6E%6B.%63%75%68%6B.%65%64%75.%63%6E" title="email" class="social-icon" style="margin-right: 15px;"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=28o0CkgAAAAJ" title="Google Scholar" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/richardcsuwandi" title="GitHub" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/richardcsuwandi" title="LinkedIn" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/richardcsuwandi" title="X" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Science of Intelligent Exploration</h1> <p>Why we need to re-center exploration in AI</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#embracing-the-unexpected">Embracing the unexpected</a> </div> <div> <a href="#beyond-a-single-solution">Beyond a single solution</a> </div> <div> <a href="#towards-endless-innovation">Towards endless innovation</a> </div> <div> <a href="#a-path-to-general-intelligence">A path to general intelligence</a> </div> <ul> <li> <a href="#meta-learning-architectures">Meta-learning architectures</a> </li> <li> <a href="#meta-learning-learning-algorithms">Meta-Learning learning algorithms</a> </li> <li> <a href="#generating-effective-learning-environments">Generating effective learning environments</a> </li> </ul> <div> <a href="#exploration-is-the-future">Exploration is the future</a> </div> <div> <a href="#takeaways">Takeaways</a> </div> </nav> </d-contents> <p>One of the most thought-provoking moments for me at <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML 2025</a> didn’t come from a new architecture or a scaling law. It emerged from a simple, unsettling question: What happens when AI stops exploring? Recent breakthroughs in AI—especially in LLMs—have been fueled not by curiosity, but by curation. By training on vast amounts of human-generated data, models like LLMs bypass the messy, uncertain process of active exploration. Instead, they absorb a pre-digested version of our collective knowledge, effectively “pre-exploring” the world through the lens of what’s already been written<d-footnote>Eric Jang once called these models excellent "data sponges", as they are really great at memorizing vast amounts of data and can do this quickly by training with batch sizes in the tens of thousands.</d-footnote>. While these models can recombine, paraphrase, and simulate, they rarely discover new things. This is the core mission of the <a href="https://exait-workshop.github.io/" rel="external nofollow noopener" target="_blank">Exploration in AI Today (EXAIT)</a> Workshop at ICML 2025: to confront the quiet crisis of over-exploitation and re-center exploration to enable progress in modern AI. Because whether it’s a robot learning to walk, a recommender system fighting filter bubbles<d-footnote>"Filter bubbles" refers to the phenomenon where users are only shown familiar content.</d-footnote>, or an AI searching for a drug in a vast space of possible molecules, the path to breakthroughs isn’t paved by more data alone. In this post, we will explore the science of intelligent exploration, from the basics of novelty search to the cutting-edge of open-endedness.</p> <p><img src="/assets/img/exait.png" alt="EXAIT Workshop" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> List of research questions at the <a href="https://exait-workshop.github.io/" rel="external nofollow noopener" target="_blank">EXAIT</a> Workshop at ICML 2025.</p> </div> <h2 id="embracing-the-unexpected">Embracing the unexpected</h2> <p>Let’s start with a counterintuitive concept that flips traditional optimization on its head: novelty search<d-cite key="lehman2011abandoning"></d-cite>. Imagine trying to solve a maze by obsessively chasing the exit, only to hit dead end after dead end. Now imagine wandering the maze, seeking out new paths regardless of the goal—and stumbling upon the exit by accident. This is the essence of novelty search, a paradigm that prioritizes exploring new behaviors over optimizing for a specific objective. The novelty of a new solution is measured by its distance (typically Euclidean) from previously discovered behaviors in a so-called behavior characterization (BC)<d-footnote>Behavior characterization (BC) refers to a set of features that describe how an agent behaves. For a robot, this might be the sequence of positions it visits; for a neural network, it could be the activation patterns it produces.</d-footnote> space. The algorithm maintains an <strong>archive</strong> of all discovered behaviors and calculates novelty as:</p> \[\text{Novelty}(x) = \frac{1}{k} \sum_{i=1}^{k} \text{distance}(x, x_i)\] <p>where $k$ is the number of nearest neighbors in the archive. This encourages agents to venture into unexplored regions of behavior space, creating a diverse portfolio of solutions.</p> <p>In a classic maze experiment<d-cite key="lehman2008exploiting"></d-cite>, algorithms chasing rewards failed to escape complex mazes, getting trapped in local optima. But those maximizing novelty—exploring diverse paths without fixating on the goal—succeeded. Why? Because chasing ambitious objectives can lead to <strong>deception</strong>, where the objective function becomes a false compass. Consider the “deceptive maze” where the path to the goal requires initially moving away from it. Traditional fitness-based search gets trapped in dead ends that appear promising (high fitness) but lead nowhere. Novelty search, by ignoring the goal entirely, naturally explores the entire maze and discovers the true path.</p> <p><img src="/assets/img/fitness_vs_novelty.png" alt="Fitness vs Novelty" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> Novelty search vs fitness-based search in a maze.</p> </div> <p>The stepping stones to success often look nothing like the goal itself. For example, the path from abacuses to laptops involved seemingly unrelated innovations like electricity and vacuum tubes. An interesting experiment that demonstrates this is <a href="https://nbenko1.github.io/#/" rel="external nofollow noopener" target="_blank">Picbreeder</a>, a platform where users evolve images through novelty search. A user aiming for a car might end up with a spaceship-like form that, through further exploration, morphs into a car. The lesson? Ignoring the objective can sometimes get you there faster.</p> <p><img src="/assets/img/picbreeder.png" alt="Picbreeder" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> What Picbreeder shows: The stepping stones almost never resemble the final product! You can only find things by not looking for them.</p> </div> <p>But novelty alone isn’t enough. What if we could balance exploration with quality? That’s where quality diversity comes in.</p> <h2 id="beyond-a-single-solution">Beyond a single solution</h2> <p>While novelty search embraces exploration, quality diversity (QD) takes it a step further by seeking diverse solutions that are also high-performing. Instead of finding a single “best” solution, QD algorithms like MAP-Elite<d-cite key="mouret2015illuminating"></d-cite> or Go-Explore<d-cite key="ecoffet2019go"></d-cite> illuminate the entire space of possibilities, collecting a portfolio of solutions that solve a problem in different ways. The MAP-Elites<d-cite key="mouret2015illuminating"></d-cite> discretizes the behavior space into a grid (the “map”), with each cell representing a unique combination of behavioral features. The algorithm seeks to fill each cell with the highest-performing solution found for that behavior type, creating a diverse “archive” of elite solutions. The process is elegantly simple:</p> <ol> <li> <strong>Initialize</strong>: Create an empty map with predefined behavioral dimensions</li> <li> <strong>Generate</strong>: Create new solutions through mutation or crossover</li> <li> <strong>Evaluate</strong>: Measure both performance (fitness) and behavior characteristics</li> <li> <strong>Place</strong>: Assign each solution to its corresponding map cell</li> <li> <strong>Select</strong>: Keep only the best-performing solution in each cell</li> <li> <strong>Repeat</strong>: Continue until the map is sufficiently filled</li> </ol> <p><img src="/assets/img/map_elites.png" alt="MAP-Elites" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> MAP-Elites in action.</p> </div> <p>The result is a comprehensive “atlas” of high-quality solutions across the entire behavioral landscape<d-footnnote>This connects to the concept of **illumination**—the goal is not just to find good solutions, but to understand the entire fitness landscape.</d-footnnote>. This approach answers the question: “What is the best possible performance achievable for each way of solving this problem?” By explicitly maintaining diversity, they prevent the population from collapsing to a single solution type<d-footnote>This is also known as "premature convergence" or "convergence to a local optimum" in the context of optimization.</d-footnote>.</p> <p>One of QD’s most striking successes came in robotics, where MAP-Elites generated diverse walking gaits for six-legged robots<d-cite key="cully2015robots"></d-cite>. When a robot loses a leg, it can immediately switch to a pre-evolved gait adapted to that specific damage pattern—recovering in under two minutes without any learning or simulation.</p> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/KFDMm666QBU" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <p>Another notable QD algorithm is Go-Explore<d-cite key="ecoffet2019go"></d-cite>. Go-Explore works in two phases: first, it systematically explores and remembers promising states—even those that seem irrelevant at first. Then, in a second phase, it robustifies these solutions to ensure they work reliably in the real, noisy environment. By explicitly separating exploration from exploitation, Go-Explore was able to crack <a href="https://en.wikipedia.org/wiki/Montezuma%27s_Revenge_(video_game)" rel="external nofollow noopener" target="_blank">Montezuma’s Revenge</a>, discovering not just one way to win, but mapping out a constellation of valuable approaches.</p> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/L_E3w_gHBOY" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <p>Yet, QD still operates within a finite, predefined domain. What if we could go beyond finding what’s possible and start inventing new possibilities? This brings us to open-ended algorithms.</p> <h2 id="towards-endless-innovation">Towards endless innovation</h2> <p>Open-ended algorithms aim to mimic the boundless creativity of natural evolution or human culture. Unlike traditional algorithms that converge on a solution, open-ended systems <strong>diverge</strong>, endlessly generating new challenges and solving them. The goal? To keep learning and innovating, no matter how much time or compute is available.</p> <div class="box-note" title="Note"> <p>Interested readers can refer to my previous <a href="https://richardcsuwandi.github.io/blog/2025/open-endedness/">post</a> for a comprehensive overview of open-endedness.</p> </div> <p>A prime example of modern open-endedness is the <a href="https://www.uber.com/en-HK/blog/poet-open-ended-deep-learning/" rel="external nofollow noopener" target="_blank">Paired Open-Ended Trailblazer (POET)</a><d-cite key="wang2019paired"></d-cite> algorithm. POET creates a dynamic ecosystem of tasks and agents, where each agent evolves to tackle new challenges generated by the system itself. The process is as follows:</p> <ol> <li> <strong>Environment Generation</strong>: Create new training environments by mutating existing ones (e.g., changing terrain difficulty, adding obstacles)</li> <li> <strong>Agent Training</strong>: Each environment trains its own population of agents using standard RL</li> <li> <strong>Transfer Evaluation</strong>: Regularly test agents on environments other than their native ones</li> <li> <strong>Selective Transfer</strong>: Move high-performing agents to environments where they can contribute</li> <li> <strong>Environment Selection</strong>: Preserve environments that are “minimal criteria” (not too easy, not impossibly hard)</li> </ol> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/D1WWhQY9N4g" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <p>The most interesting part lies in the <strong>co-evolutionary arms race</strong>: as agents get better, environments become more challenging; as environments become harder, agents must develop more sophisticated strategies. Open-endedness is about more than solving problems—it’s about creating a system that generates its own problems and learns from them. This brings us to the next frontier: AI-generating algorithms (AI-GAs)<d-cite key="clune2019ai"></d-cite>.</p> <h2 id="a-path-to-general-intelligence">A path to general intelligence</h2> <p>In his 2019 paper, Jeff Clune proposed AI-GAs as a path to AGI, built on three pillars:</p> <ol> <li> <strong>Meta-learning architectures</strong>: Automatically designing neural network structures tailored to specific tasks.</li> <li> <strong>Meta-learning learning algorithms</strong>: Evolving the rules of learning itself, like how gradients are updated.</li> <li> <strong>Generating effective learning environments</strong>: Creating diverse, challenging environments to train AI systems.</li> </ol> <h3 id="meta-learning-architectures">Meta-learning architectures</h3> <p>Traditional <a href="https://en.wikipedia.org/wiki/Neural_architecture_search" rel="external nofollow noopener" target="_blank">neural architecture search (NAS)</a> focuses on finding good architectures for specific datasets. AI-GA approaches go further by evolving architectures that can quickly adapt to new tasks. Recent examples include:</p> <ul> <li> <strong>ENAS (Efficient neural architecture search)</strong><d-cite key="pham2018efficient"></d-cite>: Uses reinforcement learning to discover architectures, dramatically reducing search time from thousands to single GPU-days.</li> <li> <strong>DARTS (Differentiable architecture search)</strong><d-cite key="liu2018darts"></d-cite>: Makes architecture search differentiable, enabling gradient-based optimization of network topology.</li> <li> <strong>AutoML-Zero</strong><d-cite key="real2020automl"></d-cite>: Evolves entire machine learning algorithms from scratch, starting with mathematical primitives and building up to complex architectures and optimizers.</li> </ul> <div class="box-important" title="Insight"> <p>Instead of hand-designing architectures, let evolution discover designs optimized for specific problem classes or computational constraints.</p> </div> <h3 id="meta-learning-learning-algorithms">Meta-learning learning algorithms</h3> <p>This involves evolving not just what the network learns, but <em>how</em> it learns. Examples include:</p> <ul> <li> <strong>Learned optimizers</strong><d-cite key="metz2019understanding"></d-cite>: Instead of using SGD or Adam, train neural networks to optimize other neural networks. These “learned optimizers” can adapt their strategy based on the loss landscape.</li> <li> <strong>Meta-learning with gradient descent</strong><d-cite key="finn2017model"></d-cite>: Model-agnostic meta-learning (MAML) trains models to be good at learning new tasks with just a few gradient steps.</li> <li> <strong>Evolutionary strategy for RL</strong><d-cite key="salimans2017evolution"></d-cite>: Replace backpropagation entirely with evolution strategies that can discover entirely new learning rules.</li> </ul> <h3 id="generating-effective-learning-environments">Generating effective learning environments</h3> <p>Traditional AI training has relied on fixed datasets or hand-crafted environments. While this approach has enabled progress, it is fundamentally limited: hand-coding environments is brittle, and it is notoriously difficult to define what makes a task “interesting” or “useful” for learning. Early attempts to automate environment generation often used simple heuristics, such as:</p> <ul> <li> <strong>Goldilocks Principle</strong>: Environments should be neither too easy (boring) nor too hard (impossible)</li> <li> <strong>Learning progress</strong>: Prioritize environments where agents are improving fastest</li> <li> <strong>Behavioral diversity</strong>: Generate environments that elicit different behaviors</li> </ul> <p>But these approaches often miss the nuanced understanding of what makes a problem genuinely interesting or valuable for developing intelligence. A recent example is <a href="https://www.jennyzhangzt.com/omni" rel="external nofollow noopener" target="_blank">OMNI</a><d-cite key="zhang2023omni"></d-cite><d-cite key="faldor2024omni"></d-cite>. OMNI uses foundation models (FMs) to propose and implement new reinforcement learning tasks that maximize agent learning progress and align with human intuitions about what is “interesting.” The core idea is to use the FM’s broad knowledge<d-footnote>FMs are trained on vast internet data, and they implicitly understand what humans find interesting—they've read our blogs, tweets, and papers, after all.</d-footnote> to guide the creation of a diverse and ever-expanding set of environments.</p> <p><img src="/assets/img/omni.png" alt="OMNI" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> OMNI combines a learning progress auto-curriculum and a model of interestingness, to train an RL agent in a task-conditioned manner.</p> </div> <p>Despite this, OMNI is still fundamentally limited by the scope of their environment generators—typically confined to a narrow, predefined distribution of tasks. This limitation restricts the true potential of open-ended learning, which aspires to create agents capable of tackling an unbounded variety of challenges. On the other hand, the grand vision of open-endedness in AI is to continuously generate and solve increasingly complex and diverse tasks, much like the creative explosion seen in biological evolution and human culture Achieving this would require algorithms that can operate within a truly vast—ideally infinite—space of possible environments. A key concept here is Darwin Completeness.</p> <div class="box-note" title="Definition"> <p>Darwin Completeness is the ability of an environment generator to, in principle, create <em>any</em> possible learning environment. This means not just tweaking parameters within a fixed simulator, but being able to generate entirely new worlds, rules, and reward structures.</p> </div> <p><a href="https://omni-epic.vercel.app/" rel="external nofollow noopener" target="_blank">OMNI-EPIC</a><d-cite key="faldor2024omni"></d-cite> is a recent framework that takes a major step toward Darwin Completeness. It augments the OMNI approach by leveraging foundation models not just to select the next interesting and learnable task, but also to generate the code for entirely new environments and reward functions. In OMNI-EPIC, the FM can write Python code to specify new simulated worlds, define novel reward and termination conditions, and even modify or create new simulators if needed. This enables OMNI-EPIC to, in principle, generate any computable environment—ranging from physical obstacle courses to logic puzzles or even quests in virtual worlds.</p> <p><img src="/assets/img/omni-epic.jpeg" alt="OMNI-EPIC" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> Examples of environments generated by OMNI-EPIC. All of these are generated using only 3 initial seeds!</p> </div> <p>Another notable advance in this direction is Genie<d-cite key="bruce2024genie"></d-cite>, a foundation world model developed by Google DeepMind. Genie, and especially its latest version <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/" rel="external nofollow noopener" target="_blank">Genie 2</a>, represents a significant leap forward in the automatic generation of diverse, interactive environments for both human and AI agents. Genie 2 is designed to generate an endless variety of action-controllable, playable 3D environments from a single prompt image. Unlike earlier world models that were limited to narrow domains or 2D settings, Genie 2 can create rich, fully interactive 3D worlds<d-footnote>These environments are not just visually diverse—they are also physically consistent and can be explored and manipulated by agents in real time.</d-footnote> with emergent properties such as object interactions, complex character animations, realistic physics (including gravity, water, smoke, and lighting effects), and dynamic environmental responses. A key feature of Genie 2 is its ability to rapidly prototype new interactive experiences. Researchers and designers can prompt Genie 2 with concept art, drawings, or even real-world images, and the model will generate a corresponding 3D world that can be immediately explored by an agent<d-footnote>This enables a new workflow for environment design, where creative ideas can be quickly tested and iterated upon, dramatically accelerating the pace of research and development.</d-footnote>.</p> <video src="/assets/img/genie2_image.mp4" alt="Genie 2" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> From concept art and drawings to fully interactive environments.</p> </div> <p>To showcase the power of Genie 2, DeepMind introduced <a href="https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/" rel="external nofollow noopener" target="_blank">SIMA</a><d-cite key="raad2024scaling"></d-cite>, a generalist agent capable of following natural language instructions and acting within a wide range of Genie-generated environments. SIMA can be given high-level goals—such as “open the blue door” or “go up the stairs”—and will control an avatar using keyboard and mouse inputs to accomplish these tasks, even in worlds it has never seen before.</p> <video src="/assets/img/sima.mp4" alt="SIMA" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 7.</strong> SIMA can follow natural language instructions in an unseen environment. The environment is generated via a single prompt image using <a href="https://imagen.research.google/" rel="external nofollow noopener" target="_blank">Imagen</a> and turned into a 3D world by Genie 2.</p> </div> <div class="box-warning" title="Remark"> <p>The combination of a powerful environment generator and an agent forms a virtuous cycle: as the environment generator creates new worlds, the agent must adapt and learn, and their progress can be used to further refine both the agent and the environment generation process.</p> </div> <h2 id="exploration-is-the-future">Exploration is the future</h2> <p>One way to understand the urgency of re-centering exploration in AI is through the lens of the emerging <a href="https://blog.minch.co/2022/11/15/software-squared.html" rel="external nofollow noopener" target="_blank">Software²</a> paradigm. While traditional deep learning (<a href="https://karpathy.medium.com/software-2-0-a64152b37c35" rel="external nofollow noopener" target="_blank">Software 2.0</a>) focuses on learning from vast, static datasets, Software² envisions a new generation of AI systems that actively seek out and generate their own training data. This shift—from passively absorbing curated data to actively exploring and producing new, informative experiences—places exploration at the heart of progress. In this view, the ability of an AI to decide <em>what</em> data to learn from, and to continually expand its own learning environment, becomes a critical driver of generality and innovation. As we move toward more open-ended and self-improving AI, the science of exploration is poised to become the central engine of advancement.</p> <p><img src="/assets/img/generalized_exp.png" alt="Software²" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 8.</strong> Software² rests on a form of <em>generalized exploration</em> for active data collection. Unlike existing notions of exploration in RL and SL (where it takes the form of active learning), generalized exploration seeks the most informative samples from the <strong>full data space</strong>.</p> </div> <p>Closely related is the vision articulated in <a href="https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf" rel="external nofollow noopener" target="_blank">The Era of Experience</a>, which argues that the next leap in AI will come not from scaling up static data, but from enabling agents to learn through rich, interactive experiences. In this new era, AI systems will continually generate, seek out, and learn from novel experiences—mirroring the way humans and animals learn by engaging with the world. Exploration, therefore, is not just a technical detail, but the foundation of a new paradigm where experience itself becomes the primary driver of intelligence.</p> <p><img src="/assets/img/era_of_exp.png" alt="Era of Experience" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 9.</strong> We are currently transitioning from the Era of Data to the Era of Experience.</p> </div> <h2 id="takeaways">Takeaways</h2> <p>Intelligent exploration lies at the heart of discovery, creativity, and adaptation—across science, innovation, and AI. We have just seen that breakthroughs rarely come from following a single, well-trodden path. Instead, they emerge from venturing into the unknown, embracing diversity, and allowing for serendipity and surprise. As AI evolves, the most capable and resilient systems will be those that do more than optimize known patterns—they will actively seek the adjacent possible, generate novel experiences, and expand the frontiers of knowledge. To build truly general and self-improving systems, we must elevate exploration to a first-class principle in AI design. The future belongs to those who explore. Let us design AI that does the same.</p> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (Jul 2025). The Science of Intelligent Exploration. Posterior Update. https://richardcsuwandi.github.io/blog/2025/exploration-in-ai/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2025explorationai</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"The Science of Intelligent Exploration"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Posterior Update"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"Jul"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2025/exploration-in-ai/"</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-07-23-exploration-in-ai.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"richardcsuwandi/richardcsuwandi.github.io","data-repo-id":"R_kgDOL_rwfw","data-category":"General","data-category-id":"DIC_kwDOL_rwf84CigRl","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <style>footer{background-color:#fff!important;border-top:1px solid #fff!important}[data-theme="dark"] footer{background-color:#1a1a1a!important;border-top:1px solid #1a1a1a!important;color:#1a1a1a!important}.back-to-top{background-color:#6c757d!important;border-color:#6c757d!important;color:white!important}.back-to-top:hover{background-color:#5a6268!important;border-color:#545b62!important;color:white!important}[data-theme="dark"] .back-to-top{background-color:#6c757d!important;border-color:#6c757d!important;color:white!important}[data-theme="dark"] .back-to-top:hover{background-color:#5a6268!important;border-color:#545b62!important;color:white!important}</style> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Richard Cornelius Suwandi. Last updated: October 01, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SYSQ70M95W"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SYSQ70M95W");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>