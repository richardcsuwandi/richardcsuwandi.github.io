<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Dream Machines | Richard Cornelius Suwandi </title> <meta name="author" content="Richard Cornelius Suwandi"> <meta name="description" content="How AI is learning to simulate our physical world"> <meta name="keywords" content="academic, personal, phd, richard cornelius suwandi, cuhk, cuhksz, machine learning, artificial intelligence, data science, bayesian optimization"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="/assets/css/blog.css?299c94cb50b62039c064ffe297befe4e"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&amp;display=swap" rel="stylesheet"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700;900&amp;family=Open+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_dark.PNG?14ecb2d6d14ba0d3c8d0e1f2823062e2"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://richardcsuwandi.github.io/blog/2025/dream-machines/"> <script src="/assets/js/theme.js?6ba506522a0f931c10c11ff001dde98f"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.center{display:block;margin-left:auto;margin-right:auto}.framed{border:1px var(--global-text-color) dashed!important;padding:20px}d-article{overflow-x:visible}.underline{text-decoration:underline}.todo{display:block;margin:12px 0;font-style:italic;color:red}.todo:before{content:"TODO: ";font-weight:bold;font-style:normal}summary{color:steelblue;font-weight:bold}summary-math{text-align:center;color:black}[data-theme="dark"] summary-math{text-align:center;color:white}details[open]{--bg:#e2edfc;color:black;border-radius:15px;padding-left:8px;background:var(--bg);outline:.5rem solid var(--bg);margin:0 0 2rem 0;font-size:80%;line-height:1.4}[data-theme="dark"] details[open]{--bg:#112f4a;color:white;border-radius:15px;padding-left:8px;background:var(--bg);outline:.5rem solid var(--bg);margin:0 0 2rem 0;font-size:80%}.box-note,.box-warning,.box-error,.box-important,.box-example{padding:15px 15px 15px 10px;margin:20px 20px 20px 5px;border:1px solid #f9f9f9;border-left-width:5px;border-radius:5px 3px 3px 5px;position:relative}.box-note[title]::before,.box-warning[title]::before,.box-error[title]::before,.box-important[title]::before,.box-example[title]::before{content:attr(title);display:block;font-weight:bold;font-size:1.1em;margin-bottom:8px;padding-bottom:5px;border-bottom:1px solid rgba(0,0,0,0.1)}d-article .box-note{background-color:#f9f9f9;border-left-color:#9db2d8}d-article .box-note[title]::before{color:#000}d-article .box-warning{background-color:#f9f9f9;border-left-color:#f8de92}d-article .box-warning[title]::before{color:#000}d-article .box-error{background-color:#f9f9f9;border-left-color:#ddb4be}d-article .box-error[title]::before{color:#000}d-article .box-important{background-color:#f9f9f9;border-left-color:#a8c08a}d-article .box-important[title]::before{color:#000}d-article .box-example{background-color:#f9f9f9;border-left-color:#aaa}d-article .box-example[title]::before{color:#000}html[data-theme='dark'] d-article .box-note{background-color:#2f2f2f;border-left-color:#9db2d8}html[data-theme='dark'] d-article .box-note[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-warning{background-color:#2f2f2f;border-left-color:#f8de92}html[data-theme='dark'] d-article .box-warning[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-error{background-color:#2f2f2f;border-left-color:#ddb4be}html[data-theme='dark'] d-article .box-error[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-important{background-color:#2f2f2f;border-left-color:#a8c08a}html[data-theme='dark'] d-article .box-important[title]::before{color:#fff;border-bottom-color:#686868}html[data-theme='dark'] d-article .box-example{background-color:#2f2f2f;border-left-color:#aaa}html[data-theme='dark'] d-article .box-example[title]::before{color:#fff;border-bottom-color:#686868}d-article aside{border:1px solid #aaa;border-radius:4px;padding:.5em .5em 0;font-size:90%}.caption{font-size:80%;line-height:1.2;text-align:left}d-citation-list .references li{margin:0 0 .6rem 0}d-citation-list .references .title{display:inline;margin:0;line-height:1.4;font-weight:600}d-citation-list .references a{white-space:nowrap;margin-left:.5em}d-citation-list .references{line-height:1.5}code{background-color:#f5f5f5;color:#d73027;padding:2px 6px;border-radius:4px;font-family:'SF Mono',Monaco,'Inconsolata','Fira Code','Droid Sans Mono','Source Code Pro',monospace;font-size:.9em;font-weight:500;border:1px solid #e1e1e1}.highlight{background-color:#f8f8f8;border:1px solid #e1e1e1;border-radius:8px;margin:20px 0;box-shadow:0 1px 3px rgba(0,0,0,0.1);position:relative}.highlight pre{background-color:transparent!important;border:none!important;border-radius:0;padding:16px 20px;margin:0!important;overflow-x:auto;box-shadow:none!important}pre:not(.highlight pre){background-color:#f8f8f8;border:1px solid #e1e1e1;border-radius:8px;margin:20px 0;padding:16px 20px;overflow-x:auto;box-shadow:0 1px 3px rgba(0,0,0,0.1);position:relative}pre code{background-color:transparent;color:#2d3748;padding:0;border:0;font-size:.85em;line-height:1.6;font-weight:400;display:block}pre code{color:#2d3748}pre code .token.keyword,pre code .language-python .hljs-keyword{color:#1976d2;font-weight:600}pre code .token.string,pre code .language-python .hljs-string{color:#388e3c}pre code .token.comment,pre code .language-python .hljs-comment{color:#757575;font-style:italic}pre code .token.number,pre code .language-python .hljs-number{color:#d32f2f}
pre code .token.function,pre code .token.class-name,pre code .language-python .hljs-title{color:#7b1fa2;font-weight:600}pre code .token.builtin,pre code .language-python .hljs-built_in{color:#1976d2}pre code{white-space:pre;line-height:1.6}html[data-theme='dark'] code{background-color:#2d3748;color:#fbb6ce;border:1px solid #4a5568}html[data-theme='dark'] .highlight{background-color:#1a202c!important;border:1px solid #2d3748!important;box-shadow:none!important;border-radius:8px}html[data-theme='dark'] pre,html[data-theme='dark'] .highlight pre{background-color:transparent!important;border:none!important;box-shadow:none!important;margin:0!important;padding:16px 20px!important}html[data-theme='dark'] pre:not(.highlight pre){background-color:#1a202c!important;border:1px solid #2d3748!important;border-radius:8px}html[data-theme='dark'] pre code{background-color:transparent!important;color:#e2e8f0!important;border:none!important}html[data-theme='dark'] pre code,html[data-theme='dark'] .highlight code{background-color:transparent!important;color:#e2e8f0!important;border:none!important}html[data-theme='dark'] pre code .token.keyword,html[data-theme='dark'] pre code .language-python .hljs-keyword{color:#81d4fa;font-weight:600}html[data-theme='dark'] pre code .token.string,html[data-theme='dark'] pre code .language-python .hljs-string{color:#a5d6a7}html[data-theme='dark'] pre code .token.comment,html[data-theme='dark'] pre code .language-python .hljs-comment{color:#90a4ae;font-style:italic}html[data-theme='dark'] pre code .token.number,html[data-theme='dark'] pre code .language-python .hljs-number{color:#ffab91}html[data-theme='dark'] pre code .token.function,html[data-theme='dark'] pre code .token.class-name,html[data-theme='dark'] pre code .language-python .hljs-title{color:#ce93d8;font-weight:600}html[data-theme='dark'] pre code .token.builtin,html[data-theme='dark'] pre code .language-python .hljs-built_in{color:#81d4fa}pre::before{content:"Python";position:absolute;top:8px;right:12px;background-color:rgba(0,0,0,0.1);color:#666;padding:2px 8px;border-radius:4px;font-size:.75em;font-weight:500;text-transform:uppercase;letter-spacing:.5px;backdrop-filter:blur(5px)}pre.bibtex::before,pre.language-bibtex::before{content:"BibTeX"!important}html[data-theme='dark'] pre::before{background-color:rgba(255,255,255,0.1);color:#a0aec0}.citation-box{background-color:#f5f5f5;border-radius:8px;padding:16px 20px;margin:20px 0;font-family:'SF Mono',Monaco,'Inconsolata','Fira Mono',monospace;font-size:.9em;line-height:1.6;overflow-x:auto}html[data-theme='dark'] .citation-box{background-color:#2d3748}@media(max-width:768px){pre{margin:15px -10px;border-radius:0;border-left:0;border-right:0}code{font-size:.85em}pre code{font-size:.8em}}</style> <style type="text/css">d-byline{max-width:100%!important;width:100%!important}d-byline .byline{max-width:100%!important;width:100%!important;display:grid!important;grid-template-columns:minmax(150px,1fr) minmax(250px,2fr) minmax(200px,1fr);gap:4rem;align-items:start}d-byline h3{margin-bottom:.5rem!important;font-weight:bold;white-space:nowrap}d-byline .author,d-byline .authors{white-space:nowrap;overflow:visible}d-byline .affiliations,d-byline .affiliation{white-space:nowrap;overflow:visible}d-byline .published{white-space:nowrap}d-byline>div{margin-right:0!important}@media(min-width:1024px){d-contents{position:sticky!important;top:80px!important;z-index:500!important;max-height:calc(100vh - 120px)!important;overflow-y:auto!important;margin-top:0!important}}@media(max-width:1023px){d-contents{position:static!important;top:auto!important;max-height:none!important;overflow-y:visible!important}}d-contents nav>div>a{font-weight:bold!important}d-contents nav ul li a{font-weight:normal!important}h1,h2,h3,h4,h5,h6{scroll-margin-top:100px!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Dream Machines",
            "description": "How AI is learning to simulate our physical world",
            "published": "August 11, 2025",
            "authors": [
              
              {
                "author": "Richard Cornelius Suwandi",
                "authorURL": "https://richardcsuwandi.github.io/",
                "affiliations": [
                  {
                    "name": "The Chinese University of Hong Kong, Shenzhen",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <style>#navbar{padding:.25rem 0!important;min-height:auto!important}#navbar .container{max-width:75%!important;padding:0 2rem!important}@media(max-width:768px){#navbar .container{max-width:95%!important;padding:0 1rem!important}}#navbar .navbar-nav .nav-link{padding:.25rem .75rem!important;margin:0!important}#navbar .navbar-brand.social{margin:0!important;padding:.25rem 0!important}#navbar .navbar-brand.social a{margin:0 .25rem!important}#navbar .toggle-container{margin:0!important;padding:.25rem 0!important;margin-top:-0.5rem!important}@media(max-width:991px){#navbar .toggle-container{margin-top:0!important;display:flex!important;align-items:center!important;height:100%!important}#navbar .toggle-container button{margin:0!important;padding:.5rem!important}}#navbar .navbar-toggler{padding:.25rem 0!important;border:none!important}#navbar .navbar-nav .nav-link{font-weight:normal!important}</style> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%72%69%63%68%61%72%64%73%75%77%61%6E%64%69@%6C%69%6E%6B.%63%75%68%6B.%65%64%75.%63%6E" title="email" class="social-icon" style="margin-right: 15px;"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=28o0CkgAAAAJ" title="Google Scholar" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/richardcsuwandi" title="GitHub" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/richardcsuwandi" title="LinkedIn" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/richardcsuwandi" title="X" class="social-icon" style="margin-right: 15px;" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Dream Machines</h1> <p>How AI is learning to simulate our physical world</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-birth-of-dream-machines">The birth of dream machines</a> </div> <ul> <li> <a href="#what-is-a-world-model">What is a "world model"?</a> </li> <li> <a href="#how-to-build-a-world-model">How to build a world model?</a> </li> <li> <a href="#learning-inside-dreams">Learning inside dreams</a> </li> </ul> <div> <a href="#a-whole-new-world">A whole new world</a> </div> <ul> <li> <a href="#how-genie-works">How Genie works</a> </li> <li> <a href="#beyond-2d-worlds">Beyond 2D worlds</a> </li> <li> <a href="#interactive-3d-worlds">Interactive 3D worlds</a> </li> </ul> <div> <a href="#the-future-of-dream-machines">The future of dream machines</a> </div> <div> <a href="#waking-up-to-a-new-reality">Waking up to a new reality</a> </div> </nav> </d-contents> <p>A young girl sits, not in front of a screen, but within a world of her own making. With a thought, she conjures a cyberpunk metropolis—a sprawling cityscape alive with neon lights and towering skyscrapers. The air is thick with the scent of rain as crowds of people navigate elevated walkways under umbrellas, their reflections shimmering on wet surfaces below. She slips into the body of a luminous <a href="https://en.wikipedia.org/wiki/Koi" rel="external nofollow noopener" target="_blank">koi</a>, diving through this immersive world from an aquatic perspective. The city comes alive around her, its neon glow reflecting off her scales as she swims past towering buildings and floating advertisements. She is not just playing a game; she is living in a dream—a world that responds to her every whim, a world that learns and grows with her. This is not a scene from a distant science fiction novel. This is the future that “dream machines” like <a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/" rel="external nofollow noopener" target="_blank">Genie 3</a> are beginning to build, one pixel at a time.</p> <video src="/assets/img/genie3_koi.mp4" alt="Genie 3" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> A sample world generated by Genie 3. Clip from @apples_jimmy and @MattMcGill_ on X.</p> </div> <p>These models aren’t just tools for creating games. They are engines of experience, simulators of reality, and perhaps, the key to unlocking the next stage of artificial general intelligence (AGI). But what does it mean when the line between our dreams and our digital realities begins to blur? In this post, we will explore how foundation world models like Genie are reshaping our digital world and where they might take us next.</p> <h2 id="the-birth-of-dream-machines">The birth of dream machines</h2> <p>For years, AI has dazzled us with its creative abilities, from <a href="https://www.theatlantic.com/technology/archive/2022/12/chatgpt-ai-writing-college-student-essays/672371/" rel="external nofollow noopener" target="_blank">writing eloquent stories</a> and <a href="https://www.midjourney.com/explore?tab=video_top" rel="external nofollow noopener" target="_blank">generating stunning artwork</a> to <a href="https://deepmind.google/models/veo/" rel="external nofollow noopener" target="_blank">producing convincing video</a>. But now, with models like Genie, we are witnessing a new kind of breakthrough. Rather than simply creating content to be observed, these models generate <em>worlds</em> that can be explored and shaped in real time. For example, we can now generate a 3D world from a single image, and even interact with it in real time. This shift marks the beginning of what NVIDIA’s Jensen Huang envisioned—a future where <a href="https://www.youtube.com/watch?v=Y2F8yisiS6E" rel="external nofollow noopener" target="_blank">every single pixel will be generated, not rendered</a>.</p> <p>The path to interactive world generation began with a crucial realization: the most sophisticated video generation models were inadvertently learning to simulate reality. When OpenAI unveiled <a href="https://openai.com/index/video-generation-models-as-world-simulators/" rel="external nofollow noopener" target="_blank">Sora</a><d-cite key="videoworldsimulators2024"></d-cite> in early 2024, they explicitly positioned it not just as a video generator, but as a “world simulator”<d-footnote>During that time, OpenAI claimed that scaling video generation models is a promising path towards building general purpose simulators of the physical world.</d-footnote>. What made Sora remarkable wasn’t just its visual fidelity, but its apparent understanding of physical laws. Objects moved with convincing momentum, liquids flowed naturally, and complex interactions emerged without explicit programming. The model had learned these behaviors by observing millions of hours of video, internalizing patterns of how the world works at a level that went far beyond surface appearances.</p> <video src="/assets/img/sora_demo.mp4" alt="Sora" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> A video generated by Sora using the prompt: “Photorealistic closeup video of two pirate ships battling each other as they sail inside a cup of coffee.”</p> </div> <p>Google’s <a href="https://deepmind.google/models/veo/" rel="external nofollow noopener" target="_blank">Veo 3</a><d-cite key="veo3"></d-cite> pushed these capabilities further, offering unprecedented creative control through reference images, camera movement specifications, and synchronized audio generation. The result was a new genre of AI-generated content, including entirely novel forms like AI <a href="https://en.wikipedia.org/wiki/ASMR" rel="external nofollow noopener" target="_blank">ASMR</a> videos that pushed the boundaries of synthetic media.</p> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/PQr5TRbheAU" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <p>Yet for all their sophistication, these systems shared a fundamental limitation that highlighted the next frontier. You could watch their generated worlds, but you couldn’t inhabit them<d-footnote>While AI models like Sora and Veo can generate stunning, immersive scenes, they lack the interactivity to let users freely explore or alter the environment in real time.</d-footnote>. This gap between observation and interaction represents one of the most significant challenges in AI today: how do we move from systems that generate convincing simulations to systems that generate inhabitable realities? The answer lies in understanding the so-called “world models”—AI systems that don’t just generate plausible content, but maintain consistent internal representations of how worlds work.</p> <h3 id="what-is-a-world-model">What is a “world model”?</h3> <p>Before we dive deeper, let’s clarify what we mean by a “world model.”</p> <div class="box-note" title="Definition"> <p>A world model is a system that can simulate the dynamics of an environment.</p> </div> <p>In other words, it is a model that is able to predict how actions change states and how the environment evolves over time. Perhaps the best way to understand world models is to consider how humans operate. As <a href="https://en.wikipedia.org/wiki/Jay_Wright_Forrester" rel="external nofollow noopener" target="_blank">Jay Wright Forrester</a>, a pioneer of systems dynamics, observed:</p> <blockquote> <p>“The image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system.” <d-cite key="forrester1971counterintuitive"></d-cite></p> </blockquote> <p>To understand this better, consider the following intuitive example from <d-cite key="ha2018world"></d-cite>:</p> <div class="box-warning" title="Example"> <p>Imagine you’re playing a baseball game. You have mere milliseconds to decide how to swing—less time than it takes for visual signals to travel from eyes to brain. Yet professional players consistently make contact. How? Their brains have developed predictive models that can anticipate where and when the ball will arrive, allowing for subconscious, reflexive responses based on internal simulations of the ball’s trajectory.</p> </div> <p>AI world models also use similar principles to simulate our physical world. They learn the “rules” not through explicit programming, but by <em>observing countless examples of how things behave</em>. For instance, a world model might discover that water flows downward and around obstacles, objects cast shadows that change with lighting, and characters maintain consistent appearances from different angles.</p> <h3 id="how-to-build-a-world-model">How to build a world model?</h3> <p>The classic <a href="https://worldmodels.github.io/" rel="external nofollow noopener" target="_blank">world model</a><d-cite key="ha2018world"></d-cite>, proposed by <a href="https://x.com/hardmaru" rel="external nofollow noopener" target="_blank">David Ha</a> and <a href="https://x.com/SchmidhuberAI" rel="external nofollow noopener" target="_blank">Jürgen Schmidhuber</a>, consists of three key components that work together to create and navigate simulated realities:</p> <div class="box-example" title="Vision (V)"> <p>The role of the V component is to take high-dimensional observations and encodes them into compact, meaningful representations<d-footnote>This is similar to how our brains process visual information, where we can recognize objects even when they are partially occluded or in different lighting conditions.</d-footnote>.</p> </div> <div class="box-example" title="Memory (M)"> <p>The role of the M component is to learn temporal patterns and predicts future states based on past experience<d-footnote>This is similar to how our brains works, where we can speculate future events based on what we have seen in the past.</d-footnote>.</p> </div> <div class="box-example" title="Controller (C)"> <p>The role of the C is to map the current compressed state and predicted future to select actions <d-footnote>This is similar to how our brains make decisions, where we can plan our actions based on our current state and predicted future state.</d-footnote>.</p> </div> <p><img src="/assets/img/world_model_overview.svg" alt="World Model Overview" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> Overview of a world model architecture showing the interaction between Vision (V), Memory (M), and Controller (C) components <d-cite key="ha2018world"></d-cite>.</p> </div> <h3 id="learning-inside-dreams">Learning inside dreams</h3> <p>Perhaps the most remarkable capability of world models is to “learn inside dreams”. Instead of learning in the real world, an agent can learn to perform tasks entirely within the simulated environment generated by its own world model. The process works like this:</p> <ol> <li>The world model observes the real environment and learns its dynamics.</li> <li>The controller trains by taking actions in this learned simulation, experiencing consequences and rewards without ever touching the actual environment.</li> <li>The trained policy transfers back to reality.</li> </ol> <p>This approach offers several advantages:</p> <ul> <li> <p><strong>Accelerated learning</strong>: Time moves at computational speed rather than physical speed. For example, a robotic system can experience years of practice in days of compute time, potentially learning from more diverse scenarios than would be possible in a lifetime of real-world operation.</p> </li> <li> <p><strong>Rare event training</strong>: Edge cases that might occur once in millions of real-world interactions can be generated on demand in simulation. For example, a rescue robot can train for disaster scenarios that haven’t happened yet.</p> </li> <li> <p><strong>Safety and ethics</strong>: Dangerous scenarios—car crashes, medical emergencies, military conflicts—can be simulated without real-world consequences. For example, an autonomous vehicle can experience thousands of potential accidents in simulation, learning to avoid them without endangering anyone.</p> </li> </ul> <p>While traditional simulators rely on people manually programming how things move and interact, including rare or complex situations, world models learn these behaviors by analyzing real-world data, which allows them to capture details that humans might overlook or find too hard to describe.</p> <h2 id="a-whole-new-world">A whole new world</h2> <p>Building on the foundational insights from world models research, DeepMind’s <a href="https://sites.google.com/view/genie-2024/home" rel="external nofollow noopener" target="_blank">Genie</a><d-cite key="bruce2024genie"></d-cite> represents a significant leap forward. While the original world models work focused on learning compressed representations for efficient control in constrained environments, Genie scales this vision to create photorealistic, explorable worlds that respond to human input in real-time.</p> <h3 id="how-genie-works">How Genie works</h3> <p>Unlike traditional game engines that rely on hand-coded physics and pre-designed assets, or video models that generate fixed sequences, Genie learns to create controllable environments entirely from observing unlabeled internet videos<d-footnote>Genie was trained on over 200,000 hours of publicly available gaming footage.</d-footnote>, without being explicitly taught anything about the environments. Genie consists of 3 main components that work together to enable interactive world generation:</p> <div class="box-note" title="Spatiotemporal video tokenizer"> <p>Converts raw video frames into compressed discrete tokens that capture both spatial and temporal patterns. Rather than processing each frame independently, this component uses a novel spatiotemporal approach that understands how visual elements change over time. It compresses 16×16 pixel patches across multiple frames into discrete tokens<d-footnote> This compression is crucial—it reduces the computational burden while preserving the essential dynamics needed for interactive control.</d-footnote>, learning to represent not just what objects look like, but how they move and change.</p> </div> <div class="box-warning" title="Latent action model"> <p>Discovers and learns a discrete action space entirely from observing video transitions, without any action labels. This component observes pairs of consecutive video frames and learns to infer what “action” must have occurred to cause the transition from frame A to frame B.</p> </div> <div class="box-error" title="Autoregressive dynamics model"> <p>Generates the next frame tokens given the current state and a chosen latent action<d-footnote>This component uses a sophisticated autoregressive architecture based on MaskGIT, which generates video tokens in parallel rather than sequentially.</d-footnote>. When a user selects an action, the dynamics model predicts how the world should change, generating new video tokens that maintain visual and physical consistency with the previous frame.</p> </div> <p><img src="/assets/img/genie_architecture.png" alt="Genie Architecture" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> Genie takes in $T$ frames of video as input, tokenizes them into discrete tokens $\mathbf{z}$ via the video tokenizer, and infers the latent actions $\tilde{\mathbf{a}}$ between each frame with the latent action model. Both are then passed to the dynamics model to generate predictions for the next $T$ frames in an iterative manner.</p> </div> <p>What makes Genie remarkable is how these components learn to work together without explicit supervision. The system watches millions of video transitions and automatically discovers that certain types of changes occur repeatedly—characters moving in different directions, jumping, interacting with objects. It learns to represent these as <strong>discrete latent actions</strong>. Simultaneously, the dynamics model learns to predict what happens when each type of action is taken in different contexts. It develops an understanding of physics, object interactions, and environmental consistency. All three components are trained together, creating a feedback loop where better action recognition improves dynamics prediction, and better dynamics prediction enables more precise action discovery.</p> <h3 id="beyond-2d-worlds">Beyond 2D worlds</h3> <p>The original Genie’s transformation of 2D sprite-based games into interactive, explorable worlds was just the beginning. By late 2024, DeepMind had set its sights on a far more ambitious target: scaling these insights to create fully three-dimensional, photorealistic worlds that could rival modern game engines in visual quality while surpassing them in creative flexibility. Just eight months after the original Genie captured the world’s imagination with its 2D interactive environments, DeepMind unveiled <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/" rel="external nofollow noopener" target="_blank">Genie 2</a><d-cite key="parkerholder2024genie2"></d-cite>—a foundation world model that represents one of the most significant advances in AI-generated interactive content to date. Where Genie transformed simple 2D videos into playable experiences, Genie 2 creates rich, three-dimensional worlds from nothing more than a single prompt image.</p> <p><img src="/assets/img/genie2.png" alt="Genie 2" class="l-body rounded z-depth-1 center" width="100%"></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> Overview of the diffusion world model used in Genie 2.</p> </div> <p>While the original Genie operated on discrete video tokens in 2D space, Genie 2 employs an <strong>autoregressive latent diffusion model</strong><d-cite key="rombach2022high"></d-cite> trained on a massive dataset of 3D game videos. This hybrid approach combines the sequential prediction capabilities of autoregressive models with the high-quality generation of diffusion models. The system processes video through a sophisticated <strong>autoencoder</strong><d-cite key="kingma2013auto"></d-cite> that maps high-resolution 3D scenes into a compressed latent space. Within this space, a large <strong>transformer</strong><d-cite key="dosovitskiy2020image"></d-cite> dynamics model—similar in structure to large language models but adapted for spatial-temporal prediction—learns to generate coherent sequences of 3D environments. The use of <strong>classifier-free guidance</strong><d-cite key="ho2022classifier"></d-cite> during inference allows for precise control over action execution, ensuring that user inputs translate reliably into desired environmental changes.</p> <p>One of Genie 2’s most impressive capabilities is its ability to transform <strong>real-world photographs</strong> into interactive 3D environments. Show it a picture of a forest path, and it generates a navigable woodland where grass sways in the wind and leaves rustle overhead. Provide an image of a rushing river, and it creates a dynamic aquatic environment with flowing water and realistic fluid dynamics. This capability suggests that Genie 2 has developed sophisticated <em>scene understanding</em> that goes beyond simple pattern matching. The model appears to infer the three-dimensional structure of scenes, the likely physics governing environmental elements, and the potential interaction affordances—all from a single static image.</p> <video src="/assets/img/genie2_demo.mp4" alt="Genie 2" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> An environment concept by Max Cant transformed into a 3D world by Genie 2.</p> </div> <h3 id="interactive-3d-worlds">Interactive 3D worlds</h3> <p>More recently, DeepMind released <a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/" rel="external nofollow noopener" target="_blank">Genie 3</a><d-cite key="genie3"></d-cite>, which represents the next evolution in interactive world generation. While Genie 2 demonstrated the ability to create 3D environments from single images, Genie 3 transforms these capabilities into truly <strong>real-time, high-fidelity interactive experiences</strong> that approach the quality and responsiveness of modern game engines.</p> <p>Perhaps Genie 3’s most impressive advancement is its <strong>visual memory</strong> that remembers objects, textures, and even text for up to a minute. Turn away from a scene and look back—the world remains exactly as you left it, with objects in their previous positions and environmental details intact. This consistency enables longer storytelling sessions, complex navigation tasks, and meaningful interaction with persistent world elements. It’s the difference between a fleeting dream and a stable reality you can truly inhabit.</p> <video src="/assets/img/genie3_consistency.mp4" alt="Genie 3 consistency" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 7.</strong> A demonstration of Genie 3’s visual memory, where the world remains consistent even when the camera is turned away.</p> </div> <p>Genie 3 also introduces <strong>promptable world events</strong>, where you can instantly transform the world (e.g., change the weather, add a character, or trigger an event) using natural language. These changes integrate seamlessly into the ongoing experience without breaking immersion or requiring scene resets. This capability also enables the generation of “what if” scenarios that can be learned by agents to handle unforeseen events.</p> <video src="/assets/img/genie3_prompt.mp4" alt="Genie 3 prompt" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 8.</strong> With Genie 3, we can use natural language promps like “spawn a brown bear” to trigger events in the world.</p> </div> <p>The progression from Genie 1 to Genie 3 is mind-blowing considering that the timeframe was only about 1.5 years! Here’s a table comparing the features of the three generations:</p> <table> <thead> <tr> <th>Feature</th> <th>Genie 1</th> <th>Genie 2</th> <th>Genie 3</th> </tr> </thead> <tbody> <tr> <td><strong>Resolution</strong></td> <td>Low (2D sprites)</td> <td>360p</td> <td>720p</td> </tr> <tr> <td><strong>Control</strong></td> <td>Basic 2D actions</td> <td>Limited keyboard/mouse actions</td> <td>Navigation + promptable world events</td> </tr> <tr> <td><strong>Interaction latency</strong></td> <td>Not real-time</td> <td>Not real-time</td> <td>Real-time</td> </tr> <tr> <td><strong>Interaction horizon</strong></td> <td>Few seconds</td> <td>10–20 seconds</td> <td>Multiple minutes</td> </tr> <tr> <td><strong>Visual memory</strong></td> <td>Minimal consistency</td> <td>Minimal, scenes changed quickly</td> <td>Remembers objects and details for ~1 minute</td> </tr> <tr> <td><strong>Scene consistency</strong></td> <td>2D sprite coherence</td> <td>Frequent visual shifts in 3D</td> <td>Stable, believable 3D environments</td> </tr> </tbody> </table> <h3 id="waking-up-to-a-new-reality">Waking up to a new reality</h3> <p>We stand at the threshold of a transformative era—one where the ancient human dream of creation becomes as accessible as natural language. The dream machines like Genie represent more than technological achievement. They herald a fundamental shift in how we conceive of digital creation, learning, and experience. Imagine:</p> <ul> <li> <em>AI that learns like life evolved</em>—trained in vast, dynamic worlds simulating billions of real-world experiences<d-footnote>This what Jeff Clune usually refers to as "Darwinian complete" search spaces, an environment that can generate any possible learning environment. </d-footnote>, accelerating the path to true AGI through immersive, adaptive environments.</li> <li> <em>Creativity without limits</em>—filmmakers, artists, and storytellers bringing entire worlds to life from a single sentence, turning imagination into interactive reality in real time.</li> <li> <em>Education redefined</em>—students stepping into history, medicine, or engineering challenges through lifelike simulations, mastering skills in safe, personalized, and endlessly adaptable virtual environments.</li> </ul> <p>Yet perhaps the most exciting part is what we <em>cannot yet</em> imagine. We are likely only glimpsing the surface<d-footnote>As Tim Rocktäschel tweeted, "we have only scratched the surface of what can be done with prompting and post-training of foundational world models."</d-footnote> of what becomes possible when anyone can conjure interactive worlds from imagination alone. Though the challenges ahead are substantial and real<d-footnote>As DeepMind suggests, the computational costs and accessibility barriers are not the only challenges. There are also ethical concerns about authenticity and potential misuse.</d-footnote>, history suggests a familiar pattern, “the most transformative technologies initially seem impossible, then inevitable”. Dream machines appear to be following this well-trodden path, moving rapidly from research curiosity to practical capability. The question is not whether this future will arrive, but how we will shape it as it emerges. The dream machines are awakening, offering unprecedented creative possibilities while challenging fundamental assumptions about reality, creativity, and human-AI collaboration. As we stand at this inflection point, we have the opportunity—and responsibility—to guide this technology toward applications that amplify human creativity, accelerate learning, and expand the boundaries of what we can experience and achieve together.</p> <blockquote> <p>If you could dream up any world and bring it to life, what would you create?</p> </blockquote> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (Aug 2025). The Dream Machines. Posterior Update. https://richardcsuwandi.github.io/blog/2025/dream-machines/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2025dream</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"The Dream Machines"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Posterior Update"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"Aug"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2025/dream-machines/"</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-08-08-dream-machines.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"richardcsuwandi/richardcsuwandi.github.io","data-repo-id":"R_kgDOL_rwfw","data-category":"General","data-category-id":"DIC_kwDOL_rwf84CigRl","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <style>footer{background-color:#fff!important;border-top:1px solid #fff!important}[data-theme="dark"] footer{background-color:#1a1a1a!important;border-top:1px solid #1a1a1a!important;color:#1a1a1a!important}.back-to-top{background-color:#6c757d!important;border-color:#6c757d!important;color:white!important}.back-to-top:hover{background-color:#5a6268!important;border-color:#545b62!important;color:white!important}[data-theme="dark"] .back-to-top{background-color:#6c757d!important;border-color:#6c757d!important;color:white!important}[data-theme="dark"] .back-to-top:hover{background-color:#5a6268!important;border-color:#545b62!important;color:white!important}</style> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Richard Cornelius Suwandi. Last updated: December 24, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SYSQ70M95W"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SYSQ70M95W");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>