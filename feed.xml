<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://richardcsuwandi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://richardcsuwandi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-24T03:57:14+00:00</updated><id>https://richardcsuwandi.github.io/feed.xml</id><title type="html">Richard Cornelius Suwandi</title><subtitle></subtitle><entry><title type="html">The Dream Machines</title><link href="https://richardcsuwandi.github.io/blog/2025/dream-machines/" rel="alternate" type="text/html" title="The Dream Machines"/><published>2025-08-11T03:00:00+00:00</published><updated>2025-08-11T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/dream-machines</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/dream-machines/"><![CDATA[<p>A young girl sits, not in front of a screen, but within a world of her own making. With a thought, she conjures a cyberpunk metropolis—a sprawling cityscape alive with neon lights and towering skyscrapers. The air is thick with the scent of rain as crowds of people navigate elevated walkways under umbrellas, their reflections shimmering on wet surfaces below. She slips into the body of a luminous <a href="https://en.wikipedia.org/wiki/Koi">koi</a>, diving through this immersive world from an aquatic perspective. The city comes alive around her, its neon glow reflecting off her scales as she swims past towering buildings and floating advertisements. She is not just playing a game; she is living in a dream—a world that responds to her every whim, a world that learns and grows with her. This is not a scene from a distant science fiction novel. This is the future that “dream machines” like <a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/">Genie 3</a> are beginning to build, one pixel at a time.</p> <video src="/assets/img/genie3_koi.mp4" alt="Genie 3" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> A sample world generated by Genie 3. Clip from @apples_jimmy and @MattMcGill_ on X.</p> </div> <p>These models aren’t just tools for creating games. They are engines of experience, simulators of reality, and perhaps, the key to unlocking the next stage of artificial general intelligence (AGI). But what does it mean when the line between our dreams and our digital realities begins to blur? In this post, we will explore how foundation world models like Genie are reshaping our digital world and where they might take us next.</p> <h2 id="the-birth-of-dream-machines">The birth of dream machines</h2> <p>For years, AI has dazzled us with its creative abilities, from <a href="https://www.theatlantic.com/technology/archive/2022/12/chatgpt-ai-writing-college-student-essays/672371/">writing eloquent stories</a> and <a href="https://www.midjourney.com/explore?tab=video_top">generating stunning artwork</a> to <a href="https://deepmind.google/models/veo/">producing convincing video</a>. But now, with models like Genie, we are witnessing a new kind of breakthrough. Rather than simply creating content to be observed, these models generate <em>worlds</em> that can be explored and shaped in real time. For example, we can now generate a 3D world from a single image, and even interact with it in real time. This shift marks the beginning of what NVIDIA’s Jensen Huang envisioned—a future where <a href="https://www.youtube.com/watch?v=Y2F8yisiS6E">every single pixel will be generated, not rendered</a>.</p> <p>The path to interactive world generation began with a crucial realization: the most sophisticated video generation models were inadvertently learning to simulate reality. When OpenAI unveiled <a href="https://openai.com/index/video-generation-models-as-world-simulators/">Sora</a><d-cite key="videoworldsimulators2024"></d-cite> in early 2024, they explicitly positioned it not just as a video generator, but as a “world simulator”<d-footnote>During that time, OpenAI claimed that scaling video generation models is a promising path towards building general purpose simulators of the physical world.</d-footnote>. What made Sora remarkable wasn’t just its visual fidelity, but its apparent understanding of physical laws. Objects moved with convincing momentum, liquids flowed naturally, and complex interactions emerged without explicit programming. The model had learned these behaviors by observing millions of hours of video, internalizing patterns of how the world works at a level that went far beyond surface appearances.</p> <video src="/assets/img/sora_demo.mp4" alt="Sora" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> A video generated by Sora using the prompt: “Photorealistic closeup video of two pirate ships battling each other as they sail inside a cup of coffee.”</p> </div> <p>Google’s <a href="https://deepmind.google/models/veo/">Veo 3</a><d-cite key="veo3"></d-cite> pushed these capabilities further, offering unprecedented creative control through reference images, camera movement specifications, and synchronized audio generation. The result was a new genre of AI-generated content, including entirely novel forms like AI <a href="https://en.wikipedia.org/wiki/ASMR">ASMR</a> videos that pushed the boundaries of synthetic media.</p> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/3lAOdrskl1Y" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <p>Yet for all their sophistication, these systems shared a fundamental limitation that highlighted the next frontier. You could watch their generated worlds, but you couldn’t inhabit them<d-footnote>While AI models like Sora and Veo can generate stunning, immersive scenes, they lack the interactivity to let users freely explore or alter the environment in real time.</d-footnote>. This gap between observation and interaction represents one of the most significant challenges in AI today: how do we move from systems that generate convincing simulations to systems that generate inhabitable realities? The answer lies in understanding the so-called “world models”—AI systems that don’t just generate plausible content, but maintain consistent internal representations of how worlds work.</p> <h3 id="what-is-a-world-model">What is a “world model”?</h3> <p>Before we dive deeper, let’s clarify what we mean by a “world model.”</p> <div class="box-note" title="Definition"> <p>A world model is a system that can simulate the dynamics of an environment.</p> </div> <p>In other words, it is a model that is able to predict how actions change states and how the environment evolves over time. Perhaps the best way to understand world models is to consider how humans operate. As <a href="https://en.wikipedia.org/wiki/Jay_Wright_Forrester">Jay Wright Forrester</a>, a pioneer of systems dynamics, observed:</p> <blockquote> <p>“The image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system.” <d-cite key="forrester1971counterintuitive"></d-cite></p> </blockquote> <p>To understand this better, consider the following intuitive example from <d-cite key="ha2018world"></d-cite>:</p> <div class="box-warning" title="Example"> <p>Imagine you’re playing a baseball game. You have mere milliseconds to decide how to swing—less time than it takes for visual signals to travel from eyes to brain. Yet professional players consistently make contact. How? Their brains have developed predictive models that can anticipate where and when the ball will arrive, allowing for subconscious, reflexive responses based on internal simulations of the ball’s trajectory.</p> </div> <p>AI world models also use similar principles to simulate our physical world. They learn the “rules” not through explicit programming, but by <em>observing countless examples of how things behave</em>. For instance, a world model might discover that water flows downward and around obstacles, objects cast shadows that change with lighting, and characters maintain consistent appearances from different angles.</p> <h3 id="how-to-build-a-world-model">How to build a world model?</h3> <p>The classic <a href="https://worldmodels.github.io/">world model</a><d-cite key="ha2018world"></d-cite>, proposed by <a href="https://x.com/hardmaru">David Ha</a> and <a href="https://x.com/SchmidhuberAI">Jürgen Schmidhuber</a>, consists of three key components that work together to create and navigate simulated realities:</p> <div class="box-example" title="Vision (V)"> <p>The role of the V component is to take high-dimensional observations and encodes them into compact, meaningful representations<d-footnote>This is similar to how our brains process visual information, where we can recognize objects even when they are partially occluded or in different lighting conditions.</d-footnote>.</p> </div> <div class="box-example" title="Memory (M)"> <p>The role of the M component is to learn temporal patterns and predicts future states based on past experience<d-footnote>This is similar to how our brains works, where we can speculate future events based on what we have seen in the past.</d-footnote>.</p> </div> <div class="box-example" title="Controller (C)"> <p>The role of the C is to map the current compressed state and predicted future to select actions <d-footnote>This is similar to how our brains make decisions, where we can plan our actions based on our current state and predicted future state.</d-footnote>.</p> </div> <p><img src="/assets/img/world_model_overview.svg" alt="World Model Overview" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> Overview of a world model architecture showing the interaction between Vision (V), Memory (M), and Controller (C) components <d-cite key="ha2018world"></d-cite>.</p> </div> <h3 id="learning-inside-dreams">Learning inside dreams</h3> <p>Perhaps the most remarkable capability of world models is to “learn inside dreams”. Instead of learning in the real world, an agent can learn to perform tasks entirely within the simulated environment generated by its own world model. The process works like this:</p> <ol> <li>The world model observes the real environment and learns its dynamics.</li> <li>The controller trains by taking actions in this learned simulation, experiencing consequences and rewards without ever touching the actual environment.</li> <li>The trained policy transfers back to reality.</li> </ol> <p>This approach offers several advantages:</p> <ul> <li> <p><strong>Accelerated learning</strong>: Time moves at computational speed rather than physical speed. For example, a robotic system can experience years of practice in days of compute time, potentially learning from more diverse scenarios than would be possible in a lifetime of real-world operation.</p> </li> <li> <p><strong>Rare event training</strong>: Edge cases that might occur once in millions of real-world interactions can be generated on demand in simulation. For example, a rescue robot can train for disaster scenarios that haven’t happened yet.</p> </li> <li> <p><strong>Safety and ethics</strong>: Dangerous scenarios—car crashes, medical emergencies, military conflicts—can be simulated without real-world consequences. For example, an autonomous vehicle can experience thousands of potential accidents in simulation, learning to avoid them without endangering anyone.</p> </li> </ul> <p>While traditional simulators rely on people manually programming how things move and interact, including rare or complex situations, world models learn these behaviors by analyzing real-world data, which allows them to capture details that humans might overlook or find too hard to describe.</p> <h2 id="a-whole-new-world">A whole new world</h2> <p>Building on the foundational insights from world models research, DeepMind’s <a href="https://sites.google.com/view/genie-2024/home">Genie</a><d-cite key="bruce2024genie"></d-cite> represents a significant leap forward. While the original world models work focused on learning compressed representations for efficient control in constrained environments, Genie scales this vision to create photorealistic, explorable worlds that respond to human input in real-time.</p> <h3 id="how-genie-works">How Genie works</h3> <p>Unlike traditional game engines that rely on hand-coded physics and pre-designed assets, or video models that generate fixed sequences, Genie learns to create controllable environments entirely from observing unlabeled internet videos<d-footnote>Genie was trained on over 200,000 hours of publicly available gaming footage.</d-footnote>, without being explicitly taught anything about the environments. Genie consists of 3 main components that work together to enable interactive world generation:</p> <div class="box-note" title="Spatiotemporal video tokenizer"> <p>Converts raw video frames into compressed discrete tokens that capture both spatial and temporal patterns. Rather than processing each frame independently, this component uses a novel spatiotemporal approach that understands how visual elements change over time. It compresses 16×16 pixel patches across multiple frames into discrete tokens<d-footnote> This compression is crucial—it reduces the computational burden while preserving the essential dynamics needed for interactive control.</d-footnote>, learning to represent not just what objects look like, but how they move and change.</p> </div> <div class="box-warning" title="Latent action model"> <p>Discovers and learns a discrete action space entirely from observing video transitions, without any action labels. This component observes pairs of consecutive video frames and learns to infer what “action” must have occurred to cause the transition from frame A to frame B.</p> </div> <div class="box-error" title="Autoregressive dynamics model"> <p>Generates the next frame tokens given the current state and a chosen latent action<d-footnote>This component uses a sophisticated autoregressive architecture based on MaskGIT, which generates video tokens in parallel rather than sequentially.</d-footnote>. When a user selects an action, the dynamics model predicts how the world should change, generating new video tokens that maintain visual and physical consistency with the previous frame.</p> </div> <p><img src="/assets/img/genie_architecture.png" alt="Genie Architecture" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> Genie takes in $T$ frames of video as input, tokenizes them into discrete tokens $\mathbf{z}$ via the video tokenizer, and infers the latent actions $\tilde{\mathbf{a}}$ between each frame with the latent action model. Both are then passed to the dynamics model to generate predictions for the next $T$ frames in an iterative manner.</p> </div> <p>What makes Genie remarkable is how these components learn to work together without explicit supervision. The system watches millions of video transitions and automatically discovers that certain types of changes occur repeatedly—characters moving in different directions, jumping, interacting with objects. It learns to represent these as <strong>discrete latent actions</strong>. Simultaneously, the dynamics model learns to predict what happens when each type of action is taken in different contexts. It develops an understanding of physics, object interactions, and environmental consistency. All three components are trained together, creating a feedback loop where better action recognition improves dynamics prediction, and better dynamics prediction enables more precise action discovery.</p> <h3 id="beyond-2d-worlds">Beyond 2D worlds</h3> <p>The original Genie’s transformation of 2D sprite-based games into interactive, explorable worlds was just the beginning. By late 2024, DeepMind had set its sights on a far more ambitious target: scaling these insights to create fully three-dimensional, photorealistic worlds that could rival modern game engines in visual quality while surpassing them in creative flexibility. Just eight months after the original Genie captured the world’s imagination with its 2D interactive environments, DeepMind unveiled <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a><d-cite key="parkerholder2024genie2"></d-cite>—a foundation world model that represents one of the most significant advances in AI-generated interactive content to date. Where Genie transformed simple 2D videos into playable experiences, Genie 2 creates rich, three-dimensional worlds from nothing more than a single prompt image.</p> <p><img src="/assets/img/genie2.png" alt="Genie 2" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> Overview of the diffusion world model used in Genie 2.</p> </div> <p>While the original Genie operated on discrete video tokens in 2D space, Genie 2 employs an <strong>autoregressive latent diffusion model</strong><d-cite key="rombach2022high"></d-cite> trained on a massive dataset of 3D game videos. This hybrid approach combines the sequential prediction capabilities of autoregressive models with the high-quality generation of diffusion models. The system processes video through a sophisticated <strong>autoencoder</strong><d-cite key="kingma2013auto"></d-cite> that maps high-resolution 3D scenes into a compressed latent space. Within this space, a large <strong>transformer</strong><d-cite key="dosovitskiy2020image"></d-cite> dynamics model—similar in structure to large language models but adapted for spatial-temporal prediction—learns to generate coherent sequences of 3D environments. The use of <strong>classifier-free guidance</strong><d-cite key="ho2022classifier"></d-cite> during inference allows for precise control over action execution, ensuring that user inputs translate reliably into desired environmental changes.</p> <p>One of Genie 2’s most impressive capabilities is its ability to transform <strong>real-world photographs</strong> into interactive 3D environments. Show it a picture of a forest path, and it generates a navigable woodland where grass sways in the wind and leaves rustle overhead. Provide an image of a rushing river, and it creates a dynamic aquatic environment with flowing water and realistic fluid dynamics. This capability suggests that Genie 2 has developed sophisticated <em>scene understanding</em> that goes beyond simple pattern matching. The model appears to infer the three-dimensional structure of scenes, the likely physics governing environmental elements, and the potential interaction affordances—all from a single static image.</p> <video src="/assets/img/genie2_demo.mp4" alt="Genie 2" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> An environment concept by Max Cant transformed into a 3D world by Genie 2.</p> </div> <h3 id="interactive-3d-worlds">Interactive 3D worlds</h3> <p>More recently, DeepMind released <a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/">Genie 3</a><d-cite key="genie3"></d-cite>, which represents the next evolution in interactive world generation. While Genie 2 demonstrated the ability to create 3D environments from single images, Genie 3 transforms these capabilities into truly <strong>real-time, high-fidelity interactive experiences</strong> that approach the quality and responsiveness of modern game engines.</p> <p>Perhaps Genie 3’s most impressive advancement is its <strong>visual memory</strong> that remembers objects, textures, and even text for up to a minute. Turn away from a scene and look back—the world remains exactly as you left it, with objects in their previous positions and environmental details intact. This consistency enables longer storytelling sessions, complex navigation tasks, and meaningful interaction with persistent world elements. It’s the difference between a fleeting dream and a stable reality you can truly inhabit.</p> <video src="/assets/img/genie3_consistency.mp4" alt="Genie 3 consistency" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 7.</strong> A demonstration of Genie 3’s visual memory, where the world remains consistent even when the camera is turned away.</p> </div> <p>Genie 3 also introduces <strong>promptable world events</strong>, where you can instantly transform the world (e.g., change the weather, add a character, or trigger an event) using natural language. These changes integrate seamlessly into the ongoing experience without breaking immersion or requiring scene resets. This capability also enables the generation of “what if” scenarios that can be learned by agents to handle unforeseen events.</p> <video src="/assets/img/genie3_prompt.mp4" alt="Genie 3 prompt" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 8.</strong> With Genie 3, we can use natural language promps like “spawn a brown bear” to trigger events in the world.</p> </div> <p>The progression from Genie 1 to Genie 3 is mind-blowing considering that the timeframe was only about 1.5 years! Here’s a table comparing the features of the three generations:</p> <table> <thead> <tr> <th>Feature</th> <th>Genie 1</th> <th>Genie 2</th> <th>Genie 3</th> </tr> </thead> <tbody> <tr> <td><strong>Resolution</strong></td> <td>Low (2D sprites)</td> <td>360p</td> <td>720p</td> </tr> <tr> <td><strong>Control</strong></td> <td>Basic 2D actions</td> <td>Limited keyboard/mouse actions</td> <td>Navigation + promptable world events</td> </tr> <tr> <td><strong>Interaction latency</strong></td> <td>Not real-time</td> <td>Not real-time</td> <td>Real-time</td> </tr> <tr> <td><strong>Interaction horizon</strong></td> <td>Few seconds</td> <td>10–20 seconds</td> <td>Multiple minutes</td> </tr> <tr> <td><strong>Visual memory</strong></td> <td>Minimal consistency</td> <td>Minimal, scenes changed quickly</td> <td>Remembers objects and details for ~1 minute</td> </tr> <tr> <td><strong>Scene consistency</strong></td> <td>2D sprite coherence</td> <td>Frequent visual shifts in 3D</td> <td>Stable, believable 3D environments</td> </tr> </tbody> </table> <h3 id="waking-up-to-a-new-reality">Waking up to a new reality</h3> <p>We stand at the threshold of a transformative era—one where the ancient human dream of creation becomes as accessible as natural language. The dream machines like Genie represent more than technological achievement. They herald a fundamental shift in how we conceive of digital creation, learning, and experience. Imagine:</p> <ul> <li><em>AI that learns like life evolved</em>—trained in vast, dynamic worlds simulating billions of real-world experiences<d-footnote>This what Jeff Clune usually refers to as "Darwinian complete" search spaces, an environment that can generate any possible learning environment. </d-footnote>, accelerating the path to true AGI through immersive, adaptive environments.</li> <li><em>Creativity without limits</em>—filmmakers, artists, and storytellers bringing entire worlds to life from a single sentence, turning imagination into interactive reality in real time.</li> <li><em>Education redefined</em>—students stepping into history, medicine, or engineering challenges through lifelike simulations, mastering skills in safe, personalized, and endlessly adaptable virtual environments.</li> </ul> <p>Yet perhaps the most exciting part is what we <em>cannot yet</em> imagine. We are likely only glimpsing the surface<d-footnote>As Tim Rocktäschel tweeted, "we have only scratched the surface of what can be done with prompting and post-training of foundational world models."</d-footnote> of what becomes possible when anyone can conjure interactive worlds from imagination alone. Though the challenges ahead are substantial and real<d-footnote>As DeepMind suggests, the computational costs and accessibility barriers are not the only challenges. There are also ethical concerns about authenticity and potential misuse.</d-footnote>, history suggests a familiar pattern, “the most transformative technologies initially seem impossible, then inevitable”. Dream machines appear to be following this well-trodden path, moving rapidly from research curiosity to practical capability. The question is not whether this future will arrive, but how we will shape it as it emerges. The dream machines are awakening, offering unprecedented creative possibilities while challenging fundamental assumptions about reality, creativity, and human-AI collaboration. As we stand at this inflection point, we have the opportunity—and responsibility—to guide this technology toward applications that amplify human creativity, accelerate learning, and expand the boundaries of what we can experience and achieve together.</p> <blockquote> <p>If you could dream up any world and bring it to life, what would you create?</p> </blockquote> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (Aug 2025). The Dream Machines. Posterior Update. https://richardcsuwandi.github.io/blog/2025/dream-machines/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2025dream</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"The Dream Machines"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Posterior Update"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"Aug"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2025/dream-machines/"</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="WORLD MODEL"/><category term="OPEN-ENDEDNESS"/><summary type="html"><![CDATA[How AI is learning to simulate our physical world]]></summary></entry><entry><title type="html">The Science of Intelligent Exploration</title><link href="https://richardcsuwandi.github.io/blog/2025/exploration-in-ai/" rel="alternate" type="text/html" title="The Science of Intelligent Exploration"/><published>2025-07-23T03:00:00+00:00</published><updated>2025-07-23T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/exploration-in-ai</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/exploration-in-ai/"><![CDATA[<p>One of the most thought-provoking moments for me at <a href="https://icml.cc/">ICML 2025</a> didn’t come from a new architecture or a scaling law. It emerged from a simple, unsettling question: What happens when AI stops exploring? Recent breakthroughs in AI—especially in LLMs—have been fueled not by curiosity, but by curation. By training on vast amounts of human-generated data, models like LLMs bypass the messy, uncertain process of active exploration. Instead, they absorb a pre-digested version of our collective knowledge, effectively “pre-exploring” the world through the lens of what’s already been written<d-footnote>Eric Jang once called these models excellent "data sponges", as they are really great at memorizing vast amounts of data and can do this quickly by training with batch sizes in the tens of thousands.</d-footnote>. While these models can recombine, paraphrase, and simulate, they rarely discover new things. This is the core mission of the <a href="https://exait-workshop.github.io/">Exploration in AI Today (EXAIT)</a> Workshop at ICML 2025: to confront the quiet crisis of over-exploitation and re-center exploration to enable progress in modern AI. Because whether it’s a robot learning to walk, a recommender system fighting filter bubbles<d-footnote>"Filter bubbles" refers to the phenomenon where users are only shown familiar content.</d-footnote>, or an AI searching for a drug in a vast space of possible molecules, the path to breakthroughs isn’t paved by more data alone. In this post, we will explore the science of intelligent exploration, from the basics of novelty search to the cutting-edge of open-endedness.</p> <p><img src="/assets/img/exait.png" alt="EXAIT Workshop" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> List of research questions at the <a href="https://exait-workshop.github.io/">EXAIT</a> Workshop at ICML 2025.</p> </div> <h2 id="embracing-the-unexpected">Embracing the unexpected</h2> <p>Let’s start with a counterintuitive concept that flips traditional optimization on its head: novelty search<d-cite key="lehman2011abandoning"></d-cite>. Imagine trying to solve a maze by obsessively chasing the exit, only to hit dead end after dead end. Now imagine wandering the maze, seeking out new paths regardless of the goal—and stumbling upon the exit by accident. This is the essence of novelty search, a paradigm that prioritizes exploring new behaviors over optimizing for a specific objective. The novelty of a new solution is measured by its distance (typically Euclidean) from previously discovered behaviors in a so-called behavior characterization (BC)<d-footnote>Behavior characterization (BC) refers to a set of features that describe how an agent behaves. For a robot, this might be the sequence of positions it visits; for a neural network, it could be the activation patterns it produces.</d-footnote> space. The algorithm maintains an <strong>archive</strong> of all discovered behaviors and calculates novelty as:</p> \[\text{Novelty}(x) = \frac{1}{k} \sum_{i=1}^{k} \text{distance}(x, x_i)\] <p>where $k$ is the number of nearest neighbors in the archive. This encourages agents to venture into unexplored regions of behavior space, creating a diverse portfolio of solutions.</p> <p>In a classic maze experiment<d-cite key="lehman2008exploiting"></d-cite>, algorithms chasing rewards failed to escape complex mazes, getting trapped in local optima. But those maximizing novelty—exploring diverse paths without fixating on the goal—succeeded. Why? Because chasing ambitious objectives can lead to <strong>deception</strong>, where the objective function becomes a false compass. Consider the “deceptive maze” where the path to the goal requires initially moving away from it. Traditional fitness-based search gets trapped in dead ends that appear promising (high fitness) but lead nowhere. Novelty search, by ignoring the goal entirely, naturally explores the entire maze and discovers the true path.</p> <p><img src="/assets/img/fitness_vs_novelty.png" alt="Fitness vs Novelty" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> Novelty search vs fitness-based search in a maze.</p> </div> <p>The stepping stones to success often look nothing like the goal itself. For example, the path from abacuses to laptops involved seemingly unrelated innovations like electricity and vacuum tubes. An interesting experiment that demonstrates this is <a href="https://nbenko1.github.io/#/">Picbreeder</a>, a platform where users evolve images through novelty search. A user aiming for a car might end up with a spaceship-like form that, through further exploration, morphs into a car. The lesson? Ignoring the objective can sometimes get you there faster.</p> <p><img src="/assets/img/picbreeder.png" alt="Picbreeder" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> What Picbreeder shows: The stepping stones almost never resemble the final product! You can only find things by not looking for them.</p> </div> <p>But novelty alone isn’t enough. What if we could balance exploration with quality? That’s where quality diversity comes in.</p> <h2 id="beyond-a-single-solution">Beyond a single solution</h2> <p>While novelty search embraces exploration, quality diversity (QD) takes it a step further by seeking diverse solutions that are also high-performing. Instead of finding a single “best” solution, QD algorithms like MAP-Elite<d-cite key="mouret2015illuminating"></d-cite> or Go-Explore<d-cite key="ecoffet2019go"></d-cite> illuminate the entire space of possibilities, collecting a portfolio of solutions that solve a problem in different ways. The MAP-Elites<d-cite key="mouret2015illuminating"></d-cite> discretizes the behavior space into a grid (the “map”), with each cell representing a unique combination of behavioral features. The algorithm seeks to fill each cell with the highest-performing solution found for that behavior type, creating a diverse “archive” of elite solutions. The process is elegantly simple:</p> <ol> <li><strong>Initialize</strong>: Create an empty map with predefined behavioral dimensions</li> <li><strong>Generate</strong>: Create new solutions through mutation or crossover</li> <li><strong>Evaluate</strong>: Measure both performance (fitness) and behavior characteristics</li> <li><strong>Place</strong>: Assign each solution to its corresponding map cell</li> <li><strong>Select</strong>: Keep only the best-performing solution in each cell</li> <li><strong>Repeat</strong>: Continue until the map is sufficiently filled</li> </ol> <p><img src="/assets/img/map_elites.png" alt="MAP-Elites" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> MAP-Elites in action.</p> </div> <p>The result is a comprehensive “atlas” of high-quality solutions across the entire behavioral landscape<d-footnnote>This connects to the concept of **illumination**—the goal is not just to find good solutions, but to understand the entire fitness landscape.</d-footnnote>. This approach answers the question: “What is the best possible performance achievable for each way of solving this problem?” By explicitly maintaining diversity, they prevent the population from collapsing to a single solution type<d-footnote>This is also known as "premature convergence" or "convergence to a local optimum" in the context of optimization.</d-footnote>.</p> <p>One of QD’s most striking successes came in robotics, where MAP-Elites generated diverse walking gaits for six-legged robots<d-cite key="cully2015robots"></d-cite>. When a robot loses a leg, it can immediately switch to a pre-evolved gait adapted to that specific damage pattern—recovering in under two minutes without any learning or simulation.</p> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/KFDMm666QBU" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <p>Another notable QD algorithm is Go-Explore<d-cite key="ecoffet2019go"></d-cite>. Go-Explore works in two phases: first, it systematically explores and remembers promising states—even those that seem irrelevant at first. Then, in a second phase, it robustifies these solutions to ensure they work reliably in the real, noisy environment. By explicitly separating exploration from exploitation, Go-Explore was able to crack <a href="https://en.wikipedia.org/wiki/Montezuma%27s_Revenge_(video_game)">Montezuma’s Revenge</a>, discovering not just one way to win, but mapping out a constellation of valuable approaches.</p> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/L_E3w_gHBOY" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <p>Yet, QD still operates within a finite, predefined domain. What if we could go beyond finding what’s possible and start inventing new possibilities? This brings us to open-ended algorithms.</p> <h2 id="towards-endless-innovation">Towards endless innovation</h2> <p>Open-ended algorithms aim to mimic the boundless creativity of natural evolution or human culture. Unlike traditional algorithms that converge on a solution, open-ended systems <strong>diverge</strong>, endlessly generating new challenges and solving them. The goal? To keep learning and innovating, no matter how much time or compute is available.</p> <div class="box-note" title="Note"> <p>Interested readers can refer to my previous <a href="https://richardcsuwandi.github.io/blog/2025/open-endedness/">post</a> for a comprehensive overview of open-endedness.</p> </div> <p>A prime example of modern open-endedness is the <a href="https://www.uber.com/en-HK/blog/poet-open-ended-deep-learning/">Paired Open-Ended Trailblazer (POET)</a><d-cite key="wang2019paired"></d-cite> algorithm. POET creates a dynamic ecosystem of tasks and agents, where each agent evolves to tackle new challenges generated by the system itself. The process is as follows:</p> <ol> <li><strong>Environment Generation</strong>: Create new training environments by mutating existing ones (e.g., changing terrain difficulty, adding obstacles)</li> <li><strong>Agent Training</strong>: Each environment trains its own population of agents using standard RL</li> <li><strong>Transfer Evaluation</strong>: Regularly test agents on environments other than their native ones</li> <li><strong>Selective Transfer</strong>: Move high-performing agents to environments where they can contribute</li> <li><strong>Environment Selection</strong>: Preserve environments that are “minimal criteria” (not too easy, not impossibly hard)</li> </ol> <div class="embed-container"> <iframe width="640" height="390" src="https://www.youtube.com/embed/D1WWhQY9N4g" frameborder="0" allowfullscreen=""></iframe> </div> <style>.embed-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%}.embed-container iframe,.embed-container object,.embed-container embed{position:absolute;top:0;left:0;width:100%;height:100%}</style> <p>The most interesting part lies in the <strong>co-evolutionary arms race</strong>: as agents get better, environments become more challenging; as environments become harder, agents must develop more sophisticated strategies. Open-endedness is about more than solving problems—it’s about creating a system that generates its own problems and learns from them. This brings us to the next frontier: AI-generating algorithms (AI-GAs)<d-cite key="clune2019ai"></d-cite>.</p> <h2 id="a-path-to-general-intelligence">A path to general intelligence</h2> <p>In his 2019 paper, Jeff Clune proposed AI-GAs as a path to AGI, built on three pillars:</p> <ol> <li><strong>Meta-learning architectures</strong>: Automatically designing neural network structures tailored to specific tasks.</li> <li><strong>Meta-learning learning algorithms</strong>: Evolving the rules of learning itself, like how gradients are updated.</li> <li><strong>Generating effective learning environments</strong>: Creating diverse, challenging environments to train AI systems.</li> </ol> <h3 id="meta-learning-architectures">Meta-learning architectures</h3> <p>Traditional <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">neural architecture search (NAS)</a> focuses on finding good architectures for specific datasets. AI-GA approaches go further by evolving architectures that can quickly adapt to new tasks. Recent examples include:</p> <ul> <li><strong>ENAS (Efficient neural architecture search)</strong><d-cite key="pham2018efficient"></d-cite>: Uses reinforcement learning to discover architectures, dramatically reducing search time from thousands to single GPU-days.</li> <li><strong>DARTS (Differentiable architecture search)</strong><d-cite key="liu2018darts"></d-cite>: Makes architecture search differentiable, enabling gradient-based optimization of network topology.</li> <li><strong>AutoML-Zero</strong><d-cite key="real2020automl"></d-cite>: Evolves entire machine learning algorithms from scratch, starting with mathematical primitives and building up to complex architectures and optimizers.</li> </ul> <div class="box-important" title="Insight"> <p>Instead of hand-designing architectures, let evolution discover designs optimized for specific problem classes or computational constraints.</p> </div> <h3 id="meta-learning-learning-algorithms">Meta-learning learning algorithms</h3> <p>This involves evolving not just what the network learns, but <em>how</em> it learns. Examples include:</p> <ul> <li><strong>Learned optimizers</strong><d-cite key="metz2019understanding"></d-cite>: Instead of using SGD or Adam, train neural networks to optimize other neural networks. These “learned optimizers” can adapt their strategy based on the loss landscape.</li> <li><strong>Meta-learning with gradient descent</strong><d-cite key="finn2017model"></d-cite>: Model-agnostic meta-learning (MAML) trains models to be good at learning new tasks with just a few gradient steps.</li> <li><strong>Evolutionary strategy for RL</strong><d-cite key="salimans2017evolution"></d-cite>: Replace backpropagation entirely with evolution strategies that can discover entirely new learning rules.</li> </ul> <h3 id="generating-effective-learning-environments">Generating effective learning environments</h3> <p>Traditional AI training has relied on fixed datasets or hand-crafted environments. While this approach has enabled progress, it is fundamentally limited: hand-coding environments is brittle, and it is notoriously difficult to define what makes a task “interesting” or “useful” for learning. Early attempts to automate environment generation often used simple heuristics, such as:</p> <ul> <li><strong>Goldilocks Principle</strong>: Environments should be neither too easy (boring) nor too hard (impossible)</li> <li><strong>Learning progress</strong>: Prioritize environments where agents are improving fastest</li> <li><strong>Behavioral diversity</strong>: Generate environments that elicit different behaviors</li> </ul> <p>But these approaches often miss the nuanced understanding of what makes a problem genuinely interesting or valuable for developing intelligence. A recent example is <a href="https://www.jennyzhangzt.com/omni">OMNI</a><d-cite key="zhang2023omni"></d-cite><d-cite key="faldor2024omni"></d-cite>. OMNI uses foundation models (FMs) to propose and implement new reinforcement learning tasks that maximize agent learning progress and align with human intuitions about what is “interesting.” The core idea is to use the FM’s broad knowledge<d-footnote>FMs are trained on vast internet data, and they implicitly understand what humans find interesting—they've read our blogs, tweets, and papers, after all.</d-footnote> to guide the creation of a diverse and ever-expanding set of environments.</p> <p><img src="/assets/img/omni.png" alt="OMNI" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> OMNI combines a learning progress auto-curriculum and a model of interestingness, to train an RL agent in a task-conditioned manner.</p> </div> <p>Despite this, OMNI is still fundamentally limited by the scope of their environment generators—typically confined to a narrow, predefined distribution of tasks. This limitation restricts the true potential of open-ended learning, which aspires to create agents capable of tackling an unbounded variety of challenges. On the other hand, the grand vision of open-endedness in AI is to continuously generate and solve increasingly complex and diverse tasks, much like the creative explosion seen in biological evolution and human culture Achieving this would require algorithms that can operate within a truly vast—ideally infinite—space of possible environments. A key concept here is Darwin Completeness.</p> <div class="box-note" title="Definition"> <p>Darwin Completeness is the ability of an environment generator to, in principle, create <em>any</em> possible learning environment. This means not just tweaking parameters within a fixed simulator, but being able to generate entirely new worlds, rules, and reward structures.</p> </div> <p><a href="https://omni-epic.vercel.app/">OMNI-EPIC</a><d-cite key="faldor2024omni"></d-cite> is a recent framework that takes a major step toward Darwin Completeness. It augments the OMNI approach by leveraging foundation models not just to select the next interesting and learnable task, but also to generate the code for entirely new environments and reward functions. In OMNI-EPIC, the FM can write Python code to specify new simulated worlds, define novel reward and termination conditions, and even modify or create new simulators if needed. This enables OMNI-EPIC to, in principle, generate any computable environment—ranging from physical obstacle courses to logic puzzles or even quests in virtual worlds.</p> <p><img src="/assets/img/omni-epic.jpeg" alt="OMNI-EPIC" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> Examples of environments generated by OMNI-EPIC. All of these are generated using only 3 initial seeds!</p> </div> <p>Another notable advance in this direction is Genie<d-cite key="bruce2024genie"></d-cite>, a foundation world model developed by Google DeepMind. Genie, and especially its latest version <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a>, represents a significant leap forward in the automatic generation of diverse, interactive environments for both human and AI agents. Genie 2 is designed to generate an endless variety of action-controllable, playable 3D environments from a single prompt image. Unlike earlier world models that were limited to narrow domains or 2D settings, Genie 2 can create rich, fully interactive 3D worlds<d-footnote>These environments are not just visually diverse—they are also physically consistent and can be explored and manipulated by agents in real time.</d-footnote> with emergent properties such as object interactions, complex character animations, realistic physics (including gravity, water, smoke, and lighting effects), and dynamic environmental responses. A key feature of Genie 2 is its ability to rapidly prototype new interactive experiences. Researchers and designers can prompt Genie 2 with concept art, drawings, or even real-world images, and the model will generate a corresponding 3D world that can be immediately explored by an agent<d-footnote>This enables a new workflow for environment design, where creative ideas can be quickly tested and iterated upon, dramatically accelerating the pace of research and development.</d-footnote>.</p> <video src="/assets/img/genie2_image.mp4" alt="Genie 2" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> From concept art and drawings to fully interactive environments.</p> </div> <p>To showcase the power of Genie 2, DeepMind introduced <a href="https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/">SIMA</a><d-cite key="raad2024scaling"></d-cite>, a generalist agent capable of following natural language instructions and acting within a wide range of Genie-generated environments. SIMA can be given high-level goals—such as “open the blue door” or “go up the stairs”—and will control an avatar using keyboard and mouse inputs to accomplish these tasks, even in worlds it has never seen before.</p> <video src="/assets/img/sima.mp4" alt="SIMA" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 7.</strong> SIMA can follow natural language instructions in an unseen environment. The environment is generated via a single prompt image using <a href="https://imagen.research.google/">Imagen</a> and turned into a 3D world by Genie 2.</p> </div> <div class="box-warning" title="Remark"> <p>The combination of a powerful environment generator and an agent forms a virtuous cycle: as the environment generator creates new worlds, the agent must adapt and learn, and their progress can be used to further refine both the agent and the environment generation process.</p> </div> <h2 id="exploration-is-the-future">Exploration is the future</h2> <p>One way to understand the urgency of re-centering exploration in AI is through the lens of the emerging <a href="https://blog.minch.co/2022/11/15/software-squared.html">Software²</a> paradigm. While traditional deep learning (<a href="https://karpathy.medium.com/software-2-0-a64152b37c35">Software 2.0</a>) focuses on learning from vast, static datasets, Software² envisions a new generation of AI systems that actively seek out and generate their own training data. This shift—from passively absorbing curated data to actively exploring and producing new, informative experiences—places exploration at the heart of progress. In this view, the ability of an AI to decide <em>what</em> data to learn from, and to continually expand its own learning environment, becomes a critical driver of generality and innovation. As we move toward more open-ended and self-improving AI, the science of exploration is poised to become the central engine of advancement.</p> <p><img src="/assets/img/generalized_exp.png" alt="Software²" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 8.</strong> Software² rests on a form of <em>generalized exploration</em> for active data collection. Unlike existing notions of exploration in RL and SL (where it takes the form of active learning), generalized exploration seeks the most informative samples from the <strong>full data space</strong>.</p> </div> <p>Closely related is the vision articulated in <a href="https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf">The Era of Experience</a>, which argues that the next leap in AI will come not from scaling up static data, but from enabling agents to learn through rich, interactive experiences. In this new era, AI systems will continually generate, seek out, and learn from novel experiences—mirroring the way humans and animals learn by engaging with the world. Exploration, therefore, is not just a technical detail, but the foundation of a new paradigm where experience itself becomes the primary driver of intelligence.</p> <p><img src="/assets/img/era_of_exp.png" alt="Era of Experience" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 9.</strong> We are currently transitioning from the Era of Data to the Era of Experience.</p> </div> <h2 id="takeaways">Takeaways</h2> <p>Intelligent exploration lies at the heart of discovery, creativity, and adaptation—across science, innovation, and AI. We have just seen that breakthroughs rarely come from following a single, well-trodden path. Instead, they emerge from venturing into the unknown, embracing diversity, and allowing for serendipity and surprise. As AI evolves, the most capable and resilient systems will be those that do more than optimize known patterns—they will actively seek the adjacent possible, generate novel experiences, and expand the frontiers of knowledge. To build truly general and self-improving systems, we must elevate exploration to a first-class principle in AI design. The future belongs to those who explore. Let us design AI that does the same.</p> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (Jul 2025). The Science of Intelligent Exploration. Posterior Update. https://richardcsuwandi.github.io/blog/2025/exploration-in-ai/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2025explorationai</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"The Science of Intelligent Exploration"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Posterior Update"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"Jul"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2025/exploration-in-ai/"</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="OPEN-ENDEDNESS"/><category term="EXPLORATION"/><summary type="html"><![CDATA[Why we need to re-center exploration in AI]]></summary></entry><entry><title type="html">The Future of AI is Open-Ended</title><link href="https://richardcsuwandi.github.io/blog/2025/open-endedness/" rel="alternate" type="text/html" title="The Future of AI is Open-Ended"/><published>2025-06-27T03:00:00+00:00</published><updated>2025-06-27T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/open-endedness</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/open-endedness/"><![CDATA[<p>Why do humans keep inventing new musical genres, designing cities that float on water, and imagining life beyond our universe? It’s because we’ve inherited nature’s restless, boundary-pushing creativity. We are an <strong>open-ended species</strong>—driven by curiosity, dissatisfied with “good enough”, and always reaching for what’s next. But here’s the catch: most of our AI systems today are not quite there yet. They’re built for specific tasks, optimized for efficiency rather than exploration. While this approach has taken us far, researchers at Google DeepMind recently argued<d-cite key="hughes2024position"></d-cite> that it won’t be enough to reach artificial superintelligence (ASI). So, the big question is:</p> <blockquote> <p>Can we teach AI to be as endlessly creative as humans or even nature itself?</p> </blockquote> <p>In this post, we’ll dive into the fascinating world of open-ended AI and explore how embracing open-endedness might finally break through the walls of today’s AI systems.</p> <h2 id="the-endless-creativity-of-nature">The endless creativity of nature</h2> <p>Before we talk about AI, let’s take a moment to appreciate the masterpiece that is biological evolution. Over 3.5 billion years, evolution has produced an almost unimaginable diversity of life forms. From extremophile bacteria that party in boiling volcanic vents to the 86 billion neurons in your brain that somehow let you understand these words, nature has been running the ultimate open-ended algorithm. What makes evolution so special? It’s the ultimate improvisational artist. There’s no predetermined goal, no final “perfect organism” to achieve, no optimization objective written in stone. Instead, evolution continuously explores the space of possible life forms through three deceptively simple ingredients:</p> <ul> <li><strong>Variation</strong> (mutation and recombination create new possibilities)</li> <li><strong>Selection</strong> (the environment acts as a brutal but fair critic)</li> <li><strong>Retention</strong> (successful innovations become building blocks for the next experiment)</li> </ul> <p>This process has given us everything from the compound eyes of dragonflies (which inspired fiber optic technology) to the echolocation abilities of bats (which influenced radar development) to the language capabilities of humans (which… well, led to this blog post). Each innovation becomes a stepping stone for further exploration, creating an open-ended spiral of increasing complexity and capability. But perhaps most remarkably, evolution never gets “stuck” in local optima for long. When faced with mass extinctions (e.g., ice ages, asteroid impacts, dramatic atmospheric changes), life doesn’t just survive, it reinvents itself. Not through careful planning, but through relentless exploration of what <a href="https://en.wikipedia.org/wiki/Stuart_Kauffman">Stuart Kauffman</a> calls <a href="https://www.youtube.com/watch?v=nEtATZePGmg">“the adjacent possible”</a><d-footnote>The "adjacent possible" is a concept introduced by theoretical biologist Stuart Kauffman, referring to the space of potential next steps that are one step away from the current state. In evolution, this represents all the possible immediate mutations, combinations, or adaptations that could emerge from existing life forms.</d-footnote>. Thus, evolution is basically the universe’s way of asking: “I wonder what would happen if…?”, and now imagine if we could capture even a fraction of this creative restlessness in our AI systems. This is where open-endedness comes in.</p> <h2 id="what-is-open-endedness">What is open-endedness?</h2> <p>Imagine you’re playing with LEGO blocks. A <em>closed system</em> is like following the instruction manual to build the exact model shown on the box — step by step, brick by brick. You follow steps 1–47, and voilà: you’ve built a sleek-looking car that looks just like everyone else’s.</p> <p><img src="/assets/img/lego.jpeg" alt="LEGO bricks" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> A pile of LEGO bricks.</p> </div> <p>But an <em>open-ended system</em> is what happens when you toss the instructions aside and let curiosity take the wheel. You start building a car, but then wonder: “What if it could fly?” So you add propellers. Then you think, “What if it could go underwater too?” So you give it fins and a submersible dome. Before long, you’ve created something entirely new — a flying, diving, transforming vehicle that reconfigures itself depending on where it is. The beauty of open-endedness is this endless sense of possibility — no fixed goal, no final answer. Just a continuous chain of ideas sparking even better ones.</p> <p>More formally, researchers from Google DeepMind define a system as open-ended if it continuously produces artifacts that are both <strong>novel</strong> (new and different) and <strong>learnable</strong> (useful and comprehensible)<d-cite key="hughes2024position"></d-cite>. Think of it as the sweet spot between pure randomness (novel but useless) and pure repetition (learnable but boring).</p> <div class="box-note" title="Novelty"> <p>The artifacts must be genuinely new or different from what came before. This could mean statistical novelty (different from the training distribution) or functional novelty (performing new behaviors or solving problems in unexpected ways).</p> </div> <div class="box-important" title="Learnability"> <p>The observer must be able to learn something useful from the artifacts. Random noise might be novel, but it’s not learnable. The artifacts should provide insights, capabilities, or stepping stones for further development.</p> </div> <p>But here’s where it gets interesting: open-endedness is fundamentally <strong>observer-dependent</strong>. What seems groundbreaking to a beginner might be trivial to an expert, and vice versa.</p> <p><img src="/assets/img/novelty_learnability.png" alt="Human-AI open-ended system" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Open-endedness is in the eye of the beholder. The same aircraft design system produces different levels of novelty and learnability for different observers.</p> </div> <p>Consider a system that generates aircraft designs. For a mouse, these flying machines might be utterly novel but completely incomprehensible. For a human aerospace engineer, they hit that sweet spot of being both novel and learnable. But for a hypothetical superintelligent alien civilization, even our most advanced designs might seem as primitive as paper airplanes.</p> <div class="box-warning" title="Remark"> <p>This observer-dependence isn’t a bug, it’s a feature! It means that as we build more capable AI systems, they can continue to surprise and teach us in ways we never expected.</p> </div> <h2 id="the-quest-for-open-endedness">The quest for open-endedness</h2> <p>The quest for open-ended AI isn’t new. In fact, it has been a dream of computer scientists for decades. Let me take you on a whirlwind tour through the key milestones that brought us to where we are today.</p> <h3 id="the-cybernetic-dreamers-1940s-50s">The cybernetic dreamers (1940s-50s)</h3> <p>It all started with <a href="https://en.wikipedia.org/wiki/Norbert_Wiener">Norbert Wiener</a> in the 1940s, who introduced the concept of cybernetics<d-cite key="wiener1948cybernetics"></d-cite> – self-regulating systems that could adapt through feedback loops. Around the same time, <a href="https://en.wikipedia.org/wiki/John_von_Neumann">John von Neumann</a> was dreaming up self-replicating automata<d-cite key="neumann1966theory"></d-cite> that could evolve beyond their initial programming. These pioneers laid the theoretical groundwork for machines that could grow and adapt, but the technology wasn’t quite there yet<d-footnote> Ironically, Wiener himself was skeptical about AI. He particularly doubted the feasibility of mechanical translation, given the ambiguous nature of language and its emotional connotations.</d-footnote>.</p> <h3 id="the-evolution-revolution-1960s-70s">The evolution revolution (1960s-70s)</h3> <p><a href="https://en.wikipedia.org/wiki/John_Henry_Holland">John Holland</a> changed everything in the 1970s with genetic algorithms<d-cite key="holland1992adaptation"></d-cite>. For the first time, we had a computational process that could evolve solutions through variation, selection, and retention – just like biological evolution, but in silicon. It was a proof of concept that machines could indeed create endless novelty.</p> <h3 id="the-artificial-life-explosion-1980s-90s">The artificial life explosion (1980s-90s)</h3> <p>The 1980s and 90s saw the birth of <a href="https://en.wikipedia.org/wiki/Artificial_life">Artificial Life (ALife)</a>, spearheaded by researchers like <a href="https://en.wikipedia.org/wiki/Christopher_Langton">Christopher Langton</a>. But the real showstopper was <a href="https://en.wikipedia.org/wiki/Thomas_S._Ray">Thomas Ray’s</a> <a href="https://web.stanford.edu/class/sts129/Alife/html/Tierra.htm">Tierra</a>, a digital world where self-replicating programs evolved through natural selection<d-footnote>In Tierra, digital organisms evolved remarkable strategies including parasitism (programs that used other programs' code to reproduce), immunity (defenses against parasites), and even hyper-parasitism (parasites that specifically targeted other parasites). These behaviors emerged purely from the evolutionary process without any explicit programming.</d-footnote>. Notably, Tierra had no explicit optimization function, the fitness function was determined purely by survival and reproduction.</p> <video src="/assets/img/tierra.mp4" alt="Tierra" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> Tierra, a virtual ecosystem where digital organisms could reproduce, mutate, and compete for computational resources. These digital creatures evolved increasingly sophisticated survival strategies, including parasitism, immunity, and even altruism, all without any explicit programming for these behaviors!</p> </div> <h3 id="the-modern-renaissance-2010s-2020s">The modern renaissance (2010s-2020s)</h3> <p>Fast forward to the 2010s, and we hit the modern era of open-ended AI. Generative Adversarial Networks (GANs)<d-cite key="goodfellow2014generative"></d-cite> showed us how two neural networks could engage in an endless creative arms race, generating increasingly realistic images through adversarial competition. Then came <a href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo and AlphaZero</a><d-cite key="silver2016mastering,silver2017mastering"></d-cite>, which didn’t just master board games, they invented entirely new strategies that human experts had never conceived. Move 37 in AlphaGo’s match against Lee Sedol wasn’t just a game-winning move; it was a moment when a machine demonstrated genuine creativity<d-footnote>Move 37 was so unexpected that Go commentators initially thought it was a mistake. It violated centuries of established Go wisdom by playing on the fifth line early in the game. However, it proved to be brilliant strategic thinking that human experts later recognized as fundamentally changing how the game could be played.</d-footnote>.</p> <p><img src="/assets/img/move37.jpg" alt="Move 37" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> Move 37, the famous move in AlphaGo’s match against Lee Sedol that changed the course of AI history.</p> </div> <h2 id="limitations-of-current-ai-systems">Limitations of current AI systems</h2> <p>Most AI systems today follow a predictable pattern: they are trained to optimize for a specific objective, and then they are deployed to solve that problem. This is a very efficient way to solve problems, but it has a few limitations. As <a href="https://www.kenstanley.net/">Kenneth Stanley</a> and <a href="https://joellehman.com">Joel Lehman</a> brilliantly articulate in “Why Greatness Cannot Be Planned”<d-cite key="stanley2015greatness"></d-cite>, this goal-oriented approach creates the so-called <em>objective deception</em><d-footnote>Objective deception refers to the counterintuitive phenomenon where directly pursuing a specific goal often leads away from achieving it. Stanley and Lehman argue that the most significant discoveries and innovations often come as byproducts of exploration rather than targeted optimization.</d-footnote>, such that the pursuit of specific goals often leads us away from more interesting discoveries. Once these systems reach their optimization target, they become what I call <em>intellectual fossils</em><d-footnote>By "intellectual fossils," I mean AI systems whose capabilities are permanently fixed at the moment their training ended—like prehistoric creatures preserved in amber, they retain their form but cannot adapt or evolve further.</d-footnote>– perfectly preserved specimens that can’t evolve further. Their capabilities are frozen at the moment their training ended, unable to learn from new experiences or develop novel skills.</p> <p>Open-ended systems offer an elegant escape from these limitations. Instead of optimizing for fixed objectives, they embrace what we might call “productive uncertainty”, or the willingness to explore without knowing exactly what they’re looking for. Here’s how open-endedness may break through each barrier:</p> <table> <thead> <tr> <th>Barrier</th> <th>Open-ended systems</th> </tr> </thead> <tbody> <tr> <td>Fixed Problem Space</td> <td>Discover new problems worth solving rather than just solving predefined problems. Develop meta-skills to learn, adapt, and transfer knowledge across domains.</td> </tr> <tr> <td>Narrow Objectives</td> <td>Use novelty search<sup>2</sup> and quality-diversity<sup>3</sup> approaches. Ask “What’s interesting?” rather than “What optimizes metric X?”</td> </tr> <tr> <td>Static Capabilities</td> <td>Treat achievements as stepping stones rather than destinations. Use mastered skills as foundations for exploring new challenges.</td> </tr> </tbody> </table> <div class="box-warning" title="Side-note"> <p><sup>2</sup> Novelty search refers to an algorithm that searches with no objective other than continually finding novel behaviors in the search space<d-cite key="lehman2011novelty"></d-cite>.</p> <p><sup>3</sup> Quality-diversity refers to an algorithm that aims at generating large collections of diverse solutions that are all high-performing<d-cite key="pugh2016quality"></d-cite>.</p> </div> <h2 id="open-endedness-in-action">Open-endedness in action</h2> <p>Enough theory let’s see open-endedness in action! Below are some of the most exciting demonstrations of open-ended AI systems that I have come across.</p> <h3 id="voyager">Voyager</h3> <p>Our first example is <a href="https://github.com/MineDojo/Voyager">Voyager</a><d-cite key="wang2023voyager"></d-cite>, an AI agent that explores Minecraft with endless curiosity. Unlike traditional game AI that follows scripted behaviors, Voyager is powered by large language models (specifically GPT-4) and consists of three key innovations: an automatic curriculum that maximizes exploration, a growing skill library that stores reusable code for complex behaviors, and an iterative prompting system that incorporates feedback to improve performance. What makes Voyager special is its ability to learn continuously without human intervention - it sets its own goals, acquires diverse skills, and makes novel discoveries through pure experimentation<d-footnote>Compared to previous approaches, Voyager achieves impressive results: it collects 3.3× more unique items, travels 2.3× further distances, and reaches key technological milestones up to 15.3× faster, all while developing interpretable and transferable skills.</d-footnote>. Most importantly, the skills it develops are interpretable, compositional, and transferable to new scenarios. Voyager can take what it learns in one Minecraft world and apply those capabilities to solve novel challenges in entirely new environments, demonstrating true open-ended learning.</p> <video src="/assets/img/voyager.mp4" alt="Voyager" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> Voyager, an AI agent that explores Minecraft with endless curiosity.</p> </div> <h3 id="the-ai-scientist">The AI Scientist</h3> <p>Perhaps the most ambitious demonstration of open-endedness is <a href="https://github.com/SakanaAI/AI-Scientist">The AI Scientist</a><d-cite key="lu2024ai"></d-cite>, developed by <a href="https://sakana.ai/ai-scientist/">Sakana AI</a> in collaboration with the University of Oxford and University of British Columbia. This groundbreaking system represents the first comprehensive framework for fully automatic scientific discovery, enabling foundation models to perform research independently across the entire research lifecycle. The AI Scientist automates four main processes:</p> <ul> <li><strong>Idea generation</strong> (brainstorming novel research directions and checking novelty via Semantic Scholar)</li> <li><strong>Experimental iteration</strong> (executing experiments and generating visualizations)</li> <li><strong>Paper write-up</strong> (producing complete manuscripts in LaTeX)</li> <li><strong>Automated peer review</strong> (evaluating papers with near-human accuracy)</li> </ul> <p><img src="/assets/img/ai-scientist.png" alt="AI Scientist" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> The AI Scientist automates the full research cycle: brainstorming ideas, implementing algorithms, running experiments, writing papers, and conducting peer review - all while learning from previous iterations to improve future research.</p> </div> <p>Remarkably, the system generates complete research papers at approximately $15 per paper<d-footnote>This cost estimate includes computational expenses for idea generation, experiment execution, paper writing, and automated review, making AI-driven research potentially 100-1000× more cost-effective than traditional human research in terms of pure paper production.</d-footnote>, demonstrating both the cost-effectiveness and scalability of AI-driven research. In its initial demonstration, The AI Scientist successfully conducted research across diverse machine learning subfields, discovering novel contributions in diffusion models, transformers, and grokking. The generated papers achieved “Weak Accept” ratings from automated reviewers based on top-tier conference standards, with some proposing genuinely interesting new directions backed by solid experimental results. While critics argue about the quality of its output, <a href="https://x.com/AcerbiLuigi/status/1823714921512886510">they’re missing the bigger picture</a>: this represents the first step toward AI systems that can discover genuinely new knowledge without human guidance, potentially democratizing research and significantly accelerating scientific progress.</p> <p>Even more impressively, The AI Scientist-v2<d-cite key="yamada2025ai"></d-cite> has recently achieved a historic milestone by becoming the first entirely AI-generated system to produce a peer-review-accepted <a href="https://sakana.ai/ai-scientist-first-publication/">workshop paper at ICLR 2025</a>. This next-generation system eliminates reliance on human-authored code templates, generalizes across diverse ML domains, and employs a novel progressive agentic tree-search methodology. Enhanced with Vision-Language Model feedback loops for iterative refinement, one of its three submitted manuscripts exceeded the average human acceptance threshold – marking the first instance of a fully autonomous AI paper successfully navigating peer review.</p> <p><img src="/assets/img/ai-scientist-v2.png" alt="AI Scientist" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 7.</strong> An example of a peer-review-accepted paper generated by The AI Scientist-v2. This paper reported a negative result that The AI Scientist encountered while trying to innovate on novel regularization methods for training neural networks that can improve their compositional generalization. This manuscript received an average reviewer score of 6.33 at the ICLR workshop, placing it above the average acceptance threshold.</p> </div> <h3 id="ai-generating-algorithms">AI-Generating Algorithms</h3> <p><a href="https://www.jeffclune.com/">Jeff Clune</a> has proposed one of the most ambitious visions: <a href="https://arxiv.org/abs/1905.10985">AI-Generating Algorithms (AI-GAs)</a><d-cite key="clune2019ai"></d-cite>, representing an alternate paradigm to the dominant “manual AI approach” where researchers attempt to discover and combine individual pieces of intelligence. Instead of hand-designing solutions, AI-GAs automatically learn how to produce general AI, following the clear trend in machine learning where learned solutions eventually replace hand-designed ones. The AI-GA approach rests on 3 pillars:</p> <ol> <li><strong>Meta-learning architectures</strong> that can automatically discover neural network designs</li> <li><strong>Meta-learning the learning algorithms themselves</strong> rather than using fixed optimization methods</li> <li><strong>Generating effective learning environments</strong> that provide the right training conditions for developing intelligence</li> </ol> <p>We’re already seeing promising implementations of this vision. For example, <a href="https://www.shengranhu.com/ADAS/">Automated Design of Agentic Systems (ADAS)</a><d-cite key="hu2024automated"></d-cite>, which won Outstanding Paper at the NeurIPS 2024 Open-World Agent Workshop, demonstrates that agents can literally invent novel agent designs by programming in code. Using their “Meta Agent Search” algorithm, the system iteratively programs new agents, tests their performance, archives discoveries, and uses this growing knowledge base to inform subsequent iterations. Remarkably, these AI-discovered agents consistently outperform hand-designed baselines across multiple domains and maintain superior performance even when transferred across different tasks and models.</p> <p><img src="/assets/img/adas.png" alt="Meta Agent Search" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 8.</strong> In Meta Agent Search, the “meta” agent is instructed to iteratively program new agents, test their performance on tasks, add them to an archive of discovered agents, and use this archive to inform the meta agent in subsequent iterations.</p> </div> <p>Similarly, Sakana AI’s <a href="https://sakana.ai/dgm/">Darwin-Gödel Machine (DGM)</a><d-cite key="zhang2025darwin"></d-cite> represents another step toward self-improving AI systems, exploring how AI can autonomously evolve and enhance its own capabilities through recursive self-improvement.</p> <div class="box-note" title="Note"> <p>Interested readers can refer to my previous <a href="https://richardcsuwandi.github.io/blog/2025/dgm/">post</a> for a comprehensive overview of the Darwin-Gödel Machine.</p> </div> <p>The full vision involves AI that can design not just its software and algorithms, but potentially even its hardware and architecture, making AI-GAs a compelling candidate for achieving more general intelligence through automated discovery rather than manual engineering.</p> <h2 id="safety-in-an-open-ended-world">Safety in an open-ended world</h2> <p>Now for the elephant in the room: if we build AI systems that can continuously discover new capabilities and behaviors, how do we ensure they remain safe and beneficial? Open-endedness is incredibly powerful, but as they say, “with great power comes great responsibility”. By definition, we can’t predict what open-ended systems will discover (because that’s the whole point!). This creates unique safety questions:</p> <ul> <li>What if an open-ended system discovers new ways to manipulate humans?</li> <li>How do we prevent it from finding exploits in its environment or constraints?</li> <li>Can we ensure it maintains beneficial goals even as it develops new capabilities?</li> </ul> <p>Most of these safety questions remain open challenges and active areas of research. One of the most feasible directions is to maintain meaningful human involvement throughout the development and deployment of open-ended AI systems. This “human-in-the-loop” approach emphasizes:</p> <ul> <li><strong>Collaborative discovery</strong>: Humans and AI working together to explore new possibilities, with humans providing guidance and oversight</li> <li><strong>Iterative feedback</strong>: Human operators continuously evaluating and shaping the direction of AI exploration</li> <li><strong>Democratic participation</strong>: Involving diverse communities of humans in decisions about AI capabilities and their deployment</li> </ul> <p>The idea is that rather than trying to solve all safety challenges through purely technical means, we can create robust frameworks where human judgment and values remain central to how open-ended systems develop.</p> <p><img src="/assets/img/open_ended_system.png" alt="Human-AI open-ended system" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 9.</strong> An illustration of how open-ended artifacts emerge from human-AI collaborative process, where AI systems engaged in self-improvement and automated discovery, and humans providing guidance and oversight.</p> </div> <h2 id="takeaways">Takeaways</h2> <p>Long ago on Earth, life was simple — just single-celled organisms. Then came the <a href="https://en.wikipedia.org/wiki/Cambrian_explosion">Cambrian Explosion</a><d-footnote>The Cambrian Explosion was a relatively brief period when most major animal phyla appeared in the fossil record. In just 10-25 million years, life went from simple forms to complex organisms with eyes, shells, and sophisticated body plans—demonstrating nature's capacity for rapid, open-ended innovation.</d-footnote>, a time when life suddenly became much more complex and diverse as creatures of all shapes and sizes began to appear. Now imagine something similar happening with AI. We’re getting close to a moment when AI systems won’t just be tools — they’ll be creative, evolving partners that help us in many different areas. Here’s what that could look like:</p> <ul> <li><strong>AI Scientists</strong> who keep exploring and making discoveries around the clock</li> <li><strong>AI Artists</strong> who create new kinds of music, art, and stories we’ve never seen before</li> <li><strong>AI Engineers</strong> who design buildings, spaceships, and even new types of computers</li> <li><strong>AI Teachers</strong> who help every student learn in the way that works best for them</li> <li><strong>AI Doctors</strong> who find new treatments and maybe even help people live longer</li> </ul> <p>These AIs won’t be fixed or limited — they’ll keep learning and improving over time. And importantly, they won’t just work in their own little corners - they’ll be able to connect ideas from different fields. The <a href="https://en.wikipedia.org/wiki/Renaissance">Renaissance</a><d-footnote>The Renaissance exemplified cross-pollination of ideas: Leonardo da Vinci's anatomical studies informed his engineering designs, Islamic mathematical concepts influenced European art, and artistic perspective techniques advanced architectural planning. This interdisciplinary approach led to unprecedented innovation across multiple domains.</d-footnote> was a time when great progress happened not just because of smart individuals, but because of ideas mixing together. Artists learned from science, scientists looked to nature, and engineers borrowed ideas from art. Open-ended AI could spark a new kind of Renaissance. Imagine an AI working on music discovering something useful for architecture. Or an AI studying animals finding a solution that helps computer design. With open-ended AI, the lines between different fields might blur in exciting ways — helping us solve problems and create things we can barely imagine today.</p> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (Jun 2025). The Future of AI is Open-Ended. Posterior Update. https://richardcsuwandi.github.io/blog/2025/open-endedness/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2025openendedness</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"The Future of AI is Open-Ended"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Posterior Update"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"Jun"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2025/open-endedness/"</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="AGENTS"/><category term="OPEN-ENDEDNESS"/><summary type="html"><![CDATA[Embracing open-endedness in the pursuit of creative AI]]></summary></entry><entry><title type="html">No World Model, No General AI</title><link href="https://richardcsuwandi.github.io/blog/2025/agents-world-models/" rel="alternate" type="text/html" title="No World Model, No General AI"/><published>2025-06-11T03:00:00+00:00</published><updated>2025-06-11T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/agents-world-models</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/agents-world-models/"><![CDATA[<p>Imagine if we could build an AI that thinks and plans like a human. Recent breakthroughs in large language models (LLMs) have brought us closer to this goal. As these models grow larger and trained on more data, they develop the so-called <em>emergent abilities</em><d-cite key="wei2022emergent"></d-cite> that significantly improve their performance on a wide range of downstream tasks. This has sparked a new wave of research into creating general AI agents that can tackle complex, long-horizon tasks in the real world environments<d-cite key="yao2023react"></d-cite><d-cite key="hao2023reasoning"></d-cite>. But here is the fascinating part: humans do not just react to what they see, we build rich <a href="https://www.youtube.com/watch?v=OKkEdTchsiE">mental models</a><d-cite key="johnson1983mental"></d-cite><d-cite key="lecun2022path"></d-cite> of how the world works. These <a href="https://worldmodels.github.io">world models</a><d-cite key="ha2018world"></d-cite> help us set ambitious goals<d-cite key="locke2013goal"></d-cite> and make thoughtful plans<d-cite key="bratman1987intention"></d-cite>. Hence, based on this observation, it is natural to ask:</p> <blockquote> <p>“Is learning a world model useful to achieve a human-level AI?”</p> </blockquote> <p>Recently, researchers at Google DeepMind showed that learning a world model is not only beneficial, but also <em>necessary</em> for general agents<d-cite key="richens2025generalagentsneedworld"></d-cite>. In this post, we will discuss the key findings from the paper and the implications for the future of AI agents.</p> <h2 id="do-we-need-a-world-model">Do we need a world model?</h2> <p>In 1991, <a href="https://en.wikipedia.org/wiki/Rodney_Brooks">Rodney Brooks</a> made a famous claim that “the world is its own best model”<d-cite key="brooks1991intelligence"></d-cite>.</p> <p><img src="/assets/img/brooks_intelligence.png" alt="Intelligence without representation" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> In <em>Intelligence without representation</em>, Rodney Brooks famously proposed that “the world is its own best model”.</p> </div> <p>He argued that intelligent behavior could emerge naturally from model-free agents simply by interacting with their environment through a cycle of actions and perceptions, without needing to build explicit representations of how the world works. Brooks’ argument has been strongly supported by the remarkable success of <em>model-free</em> agents, which have demonstrated impressive generalization capabilities across diverse tasks and environments<d-cite key="reed2022generalist"></d-cite><d-cite key="driess2023palm"></d-cite><d-cite key="raad2024scaling"></d-cite>. This model-free approach offers an appealing path to creating general AI agents while avoiding the complexities of learning explicit world models. However, recent works suggest an intriguing possibility: even these supposedly model-free agents might be learning <em>implicit</em> world models<d-cite key="li2023emergent"></d-cite> and planning algorithms<d-cite key="hou2023towards"></d-cite> beneath the surface.</p> <h3 id="ilya-was-right-all-along">Ilya was right all along?</h3> <p>Looking back to March 2023, <a href="https://en.wikipedia.org/wiki/Ilya_Sutskever">Ilya Sutskever</a> made a profound claim that large neural networks are doing far more than just next-word prediction and are actually <a href="https://www.youtube.com/watch?v=I6qQinoY9WM">learning “world models”</a>. The way he put it,</p> <div class="center"> <blockquote class="twitter-tweet" data-media-max-width="560"><p lang="en" dir="ltr">In March 2023, <a href="https://twitter.com/ilyasut?ref_src=twsrc%5Etfw">@ilyasut</a> made a profound claim about LLMs learning &quot;world models&quot;. Now, researchers at <a href="https://twitter.com/GoogleDeepMind?ref_src=twsrc%5Etfw">@GoogleDeepMind</a> have proven he glimpsed something even deeper; a fundamental law governing ALL intelligent agents. 🧵 <a href="https://t.co/MsMx8snUZs">pic.twitter.com/MsMx8snUZs</a></p>&mdash; ohqay (@ohqayy) <a href="https://twitter.com/ohqayy/status/1930418880696201317?ref_src=twsrc%5Etfw">June 5, 2025</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>He believed that what neural networks learn are not just textual information, but rather a compressed representation of our world. Thus, the more accurately we can predict the next word, the higher fidelity of the world model we can achieve.</p> <h2 id="agents-and-world-models">Agents and world models</h2> <p>While Ilya’s claim was intriguing, it was not clear how to formalize it at that time. But now, <a href="https://x.com/jonathanrichens/status/1930221408199516657">researchers at Google DeepMind</a> have proven that what Ilya said is not just a hypothesis, but a fundamental law governing <em>all</em> general agents<d-cite key="richens2025generalagentsneedworld"></d-cite>. In the paper, the authors showed that,</p> <blockquote> <p>“Any agent capable of generalizing to a broad range of simple goal-directed tasks must have learned a predictive model capable of simulating its environment, and this model can always be recovered from the agent.”</p> </blockquote> <p><img src="/assets/img/agents_learn_world_models.jpeg" alt="Agents learn world models" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Any agent satisfying a regret bound must have learned an environment transition function, which can be extracted from its goal-conditional policy. This holds true for agents that can handle basic tasks like reaching specific states.</p> </div> <ol> <li><em>There is no “model-free shortcut” to building general AI agents</em>. If we want agents that generalize to diverse tasks, we cannot avoid learning world models.</li> <li><em>Better performance requires better world models</em>. The only path to lower regret or handling more complex goals is through learning increasingly accurate world models.</li> </ol> <aside class="l-body box-note" title="Note"> <p>The above only holds true for agents that plan over multi-step horizons, as they need to understand how actions affect future states. However, “myopic” agents that only consider immediate rewards can potentially avoid learning world models since they do not need to predict long-term consequences.</p> </aside> <p>To make the above claims more precise, the authors develop a rigorous mathematical framework built on 4 key components: environments, goals, agents, and world models.</p> <h3 id="environments">Environments</h3> <p>The environment is assumed to be a controlled Markov process (cMP)<d-cite key="sutton1998reinforcement"></d-cite>, which is essentially a Markov decision process without a specified reward function. A cMP consists of a state space \(\boldsymbol{S}\), an action space \(\boldsymbol{A}\), and a transition function \(P_{ss'}(a) = P(S_{t+1} = s' \mid A_t = a, S_t = s)\). The authors assume the environment is irreducible<d-footnote>A Markov process is irreducible if every state is reachable from every other state.</d-footnote> and stationary<d-footnote>A Markov process is stationary if the transition probabilities do not change over time.</d-footnote>.</p> <h3 id="goals">Goals</h3> <p>Rather than defining complex goal structures, the paper focused on simple, intuitive goals expressed in Linear Temporal Logic (LTL)<d-cite key="pnueli1977temporal"></d-cite><d-cite key="baier2008principles"></d-cite>. A goal \(\varphi\) has the form \(\varphi = \mathcal{O}([(s,a) \in \boldsymbol{g}])\) where \(\boldsymbol{g}\) is a set of goal states and \(\mathcal{O} \in \{\bigcirc, \diamond, \top\}\) specifies the time horizon (\(\bigcirc\) = next, \(\diamond\) = eventually, \(\top\) = now). More complex composite goals \(\psi\) can be formed by combining sequential goals in ordered sequences: \(\psi = \langle\varphi_1, \varphi_2, \ldots, \varphi_n\rangle\) where the agent must achieve each sub-goal in order. The depth of a goal as the number of sub-goals: \(\text{depth}(\psi) = n\).</p> <h3 id="agents">Agents</h3> <p>The authors focused on <em>goal-conditioned agents</em><d-cite key="liu2022goal"></d-cite>, which are defined as a policy \(\pi(a_t \mid h_t; \psi)\) that maps a history \(h_t\) to an action \(a_t\) conditioned on a goal \(\psi\). This leads to a natural definition of an optimal goal-conditioned agent for a given environment and set of goals \(\boldsymbol{\Psi}\), which is a policy that maximizes the probability that \(\psi\) is achieved, for all \(\psi \in \boldsymbol{\Psi}\). However, real agents are rarely optimal, especially when operating in complex environments and for tasks that require coordinating many sub-goals over long time horizons. Instead of requiring perfect optimality, the authors define a <em>bounded</em> agent that is capable of achieving goals of some maximum goal depth with a failure rate that is bounded relative to the optimal agent. A bounded goal-conditioned agent \(\pi(a_t \mid h_t; \psi)\) satisfies:</p> \[P(\tau \models \psi \mid \pi, s_0) \geq \max_{\pi'} P(\tau \models \psi \mid \pi', s_0)(1-\delta)\] <p>for all goals \(\psi \in \boldsymbol{\Psi}_n\), where \(\boldsymbol{\Psi}_n\) is the set of all composite goals with depth at most \(n\) and \(\delta \in [0,1]\) is the failure rate parameter.</p> <h3 id="world-models">World Models</h3> <p>The authors considered the predictive world models, which can be used by agents to plan. They defined a world model as any approximation \(\hat{P}_{ss'}(a)\) of the transition function of the environment \(P_{ss'}(a) = P(S_{t+1} = s' \mid A_t = a, S_t = s)\), with bounded error \(\left|\hat{P}_{ss'}(a) - P_{ss'}(a)\right| \leq \varepsilon\). The authors showed that, for any such bounded goal-conditioned agent, an approximation of the environment’s transition function (a world model) can be recovered from the agent’s policy alone:</p> <div class="box-important" title="Theorem"> <p>Let \(\pi\) be a goal-conditioned agent with maximum failure rate \(\delta\) for all goals \(\psi \in \boldsymbol{\Psi}_n\) where \(n &gt; 1\). Then \(\pi\) fully determines a model \(\hat{P}_{ss'}(a)\) for the environment transition probabilities with bounded error:</p> \[\left|\hat{P}_{ss'}(a) - P_{ss'}(a)\right| \leq \sqrt{\frac{2P_{ss'}(a)(1-P_{ss'}(a))}{(n-1)(1-\delta)}}\] <p>For \(\delta \ll 1\) and \(n \gg 1\), the error scales as \(\mathcal{O}(\delta/\sqrt{n}) + \mathcal{O}(1/n)\).</p> </div> <p>The above result reveals two crucial insights:</p> <ol> <li>As agents become more competent (\(\delta \to 0\)), the recoverable world model becomes more accurate.</li> <li>As agents handle longer-horizon goals (larger \(n\)), they must learn increasingly precise world models.</li> </ol> <p>It also implies that learning a sufficiently general goal-conditioned policy is <em>informationally equivalent</em> to learning an accurate world model.</p> <h2 id="how-to-recover-the-world-model">How to recover the world model?</h2> <p>The authors also derived an algorithm to recover the world model from a bounded agent. The algorithm works by querying the agent with carefully designed composite goals that correspond to “either-or” decisions. For instance, it presents goals like “achieve transition \((s,a) \to s'\) at most \(r\) times out of \(n\) attempts” versus “achieve it more than \(r\) times”. The agent’s choice of action reveals information about which outcome has higher probability, allowing us to estimate \(P_{ss'}(a)\).</p> <p><img src="/assets/img/recovering_world_models.png" alt="Algorithm for recovering world models" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> The derived algorithm for recovering a world model from a bounded agent.</p> </div> <h2 id="experiments">Experiments</h2> <p>To test the effectiveness of the algorithm, the authors conducted experiments on a randomly generated controlled Markov process with 20 states and 5 actions, featuring a sparse transition function to make learning more challenging. They trained agents using trajectories sampled from the environment under a random policy, increasing agent competency by extending the training trajectory length (\(N_{\text{samples}}\)). The results show that:</p> <ul> <li>Even when agents strongly violated the theoretical assumptions (achieving worst-case regret \(\delta = 1\) for some goals), their algorithm still recovered accurate world models.</li> <li>The average error in recovered world models decreased as \(\mathcal{O}(n^{-1/2})\), matching the theoretical scaling relationship between error bounds and goal depth.</li> <li>As agents learned to handle longer-horizon goals (larger maximum depth \(n\)), the extracted world models became increasingly accurate. This confirms the fundamental link between agent capabilities and world model quality.</li> </ul> <p><img src="/assets/img/world_model_error.png" alt="Error in recovered world models" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> a) Mean error in recovered world model decreases as agent handles deeper goals. b) Mean error scales with agent’s regret at depth 50. Error bars show 95% confidence intervals over 10 experiments.</p> </div> <h3 id="connection-to-related-works">Connection to related works</h3> <p>The results of this work complement several other areas of AI research:</p> <ul> <li>The proposed algorithm completes a the “triangle” between environment, goal, and policy. Planning determines an optimal policy given a world model and goal (world model + goal → policy), while inverse reinforcement learning (IRL)<d-cite key="ng2000algorithms"></d-cite> recovers goals given a world model and policy (world model + policy → goal). The proposed algorithm fills in the remaining direction by recovering the world model given the agent’s policy and goal (policy + goal → world model). Just as IRL requires observing policies across multiple environments to fully determine goals<d-cite key="amin2016towards"></d-cite>, the algorithm needs to observe the agent’s behavior across multiple goals to fully recover the world model.</li> </ul> <p><img src="/assets/img/triangle_planning.jpeg" alt="The triangle of environment, goal, and policy" class="l-body rounded z-depth-1 center" width="60%"/></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> While planning uses a world model and a goal to determine a policy, and IRL and inverse planning use an agent’s policy and a world model to identify its goal, the proposed algorithm uses an agent’s policy and its goal to identify a world model.</p> </div> <ul> <li> <p>Traditional mechanistic interpretability (MI) typically relies on analyzing neural network activations<d-cite key="li2023emergent"></d-cite> or using supervised probes<d-cite key="alain2016understanding"></d-cite>, on the other hand, the proposed algorithm provides a novel approach that extracts world models directly from an agent’s policy behavior, making it applicable even when model internals are inaccessible. This unsupervised and architecture-agnostic approach works for any agent that satisfies the bounded regret condition, regardless of its specific implementation. For LLMs, this means we can potentially uncover their implicit world models by analyzing their goal-directed behavior, without needing to access their internal representations.</p> </li> <li> <p>A recent work<d-cite key="richens2024robust"></d-cite> showed that agents adapting to distributional shifts must learn causal world models. This work complements it by focusing on task generalization rather than domain generalization. Interestingly, domain generalization requires deeper causal understanding than task generalization. For example, in a system where state variables \(X\) and \(Y\) are causally related (\(X \to Y\)), an agent can achieve optimal task performance by learning just the transition probabilities, without needing to understand the underlying causal relationship. This suggests an agential version of Pearl’s causal hierarchy<d-cite key="bareinboim2022pearl"></d-cite>, where different agent capabilities (like domain or task generalization) require different levels of causal knowledge.</p> </li> </ul> <aside class="l-body box-warning" title="Remark"> <p>The findings also have significant implications for AI development and safety. The emergence of new capabilities in large language models and other AI systems could be explained by implicit world models learned while optimizing for diverse training tasks. The ability to extract world models from capable agents provides a new tool for verification and alignment, as model fidelity scales with agent capability. However, the inherent difficulty of learning accurate world models of complex real-world systems also places fundamental limits on general agent capabilities.</p> </aside> <h2 id="takeaways">Takeaways</h2> <p>Perhaps Ilya’s 2023 prediction was more prophetic than we realized. If the above results hold true, then the current race toward artificial superintelligence (ASI) through scaling language models might be secretly a race toward building more sophisticated world models. It is also possible that we may be witnessing something even more profound: the transition from what David Silver and Richard Sutton call the <a href="https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf">“Era of Human Data” to the “Era of Experience”</a>. While current AI systems have achieved remarkable capabilities by imitating human-generated data, Silver and Sutton argue that superhuman intelligence will emerge through agents learning predominantly from their own experience<d-cite key="silver2025welcome"></d-cite>. For example, with recent developments in foundation world models like <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a>, we can generate endless 3D environments from single images<d-cite key="parker2024genie"></d-cite> and allow agents to inhabit “streams of experience” in richly grounded environments that adapt and evolve with their capabilities.</p> <video src="/assets/img/genie2.mp4" alt="Genie 2" class="center" width="100%" autoplay="" muted="" loop="" controls=""> Your browser does not support the video tag. </video> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> Genie 2, a foundation world model capable of generating an endless variety of action-controllable, playable 3D environments for training and evaluating embodied agents. Based on a single prompt image, it can be played by a human or AI agent using keyboard and mouse inputs.</p> </div> <p>If general agents must learn world models, and superhuman intelligence requires learning from experience rather than human data, then foundation world models like Genie 2 might be the ultimate scaling law for the Era of Experience. Rather than hitting the ceiling of human knowledge, we are entering a phase where the quality of AI agents is fundamentally limited by the fidelity of the worlds they can simulate and explore. The agent that can dream the most accurate dreams, and learn the most from those dreams, might just be the most intelligent.</p> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (Jun 2025). No World Model, No General AI. Posterior Update. https://richardcsuwandi.github.io/blog/2025/agents-world-models/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2025agentsworldmodels</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"No World Model, No General AI"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Posterior Update"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"Jun"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2025/agents-world-models/"</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="AGENTS"/><category term="REINFORCEMENT LEARNING"/><summary type="html"><![CDATA[From Ilya's prediction to Google DeepMind's proof]]></summary></entry><entry><title type="html">AI That Can Improve Itself</title><link href="https://richardcsuwandi.github.io/blog/2025/dgm/" rel="alternate" type="text/html" title="AI That Can Improve Itself"/><published>2025-06-01T03:00:00+00:00</published><updated>2025-06-01T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/dgm</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/dgm/"><![CDATA[<p>Most AI systems today are stuck in a “cage” designed by humans. They rely on fixed architectures crafted by engineers and lack the ability to evolve autonomously over time. This is the <a href="https://en.wikipedia.org/wiki/Achilles%27_heel">Achilles heel</a> of modern AI — like a car, no matter how well the engine is tuned and how skilled the driver is, it cannot change its body structure or engine type to adapt to a new track on its own. But what if AI could learn and improve its own capabilities without human intervention? In this post, we will dive into the concept of self-improving systems and a recent effort towards building one.</p> <h2 id="learning-to-learn">Learning to learn</h2> <p>The idea of building systems that can improve themselves brings us to the concept of <a href="https://people.idsia.ch/~juergen/metalearning.html">meta-learning</a>, or “learning to learn” <d-cite key="thrun1998learning"></d-cite>, which aims to create systems that not only solve problems but also evolve their problem-solving strategies over time. One of the most ambitious efforts in this direction is the Gödel Machine<d-cite key="schmidhuber2003godel"></d-cite>, proposed by Jürgen Schmidhuber decades ago and was named after the famous mathematician <a href="https://en.wikipedia.org/wiki/Kurt_Gödel">Kurt Gödel</a>. A Gödel Machine is a hypothetical self-improving AI system that optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy. It represents the ultimate form of self-awareness in AI, an agent that can reason about its own limitations and modify itself accordingly.</p> <p><img src="/assets/img/godel.jpg" alt="Overview of a Gödel machine" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> Gödel machine is a hypothetical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy.</p> </div> <p>While this idea is interesting, formally proving whether a code modification of a complex AI system is <em>absolutely beneficial</em> is almost an impossible task without restrictive assumptions. This part stems from the inherent difficulty revealed by the <a href="https://en.wikipedia.org/wiki/Halting_problem">Halting Problem</a> and <a href="https://en.wikipedia.org/wiki/Rice%27s_theorem">Rice’s Theorem</a> in computational theory, and is also related to the inherent limitations of the logical system implied by <a href="https://en.wikipedia.org/wiki/Gödel%27s_incompleteness_theorems">Gödel’s incompleteness theorem</a>. These theoretical constraints make it nearly impossible to predict the complete impact of code changes without making restrictive assumptions. To illustrate this, consider a simple analogy: just as you cannot guarantee that a new software update will improve your computer’s performance without actually running it, an AI system faces an even greater challenge in predicting the long-term consequences of modifying its own complex codebase.</p> <h2 id="darwin-gödel-machine">Darwin-Gödel Machine</h2> <p>To “relax” the requirement of formal proof, a recent work by proposed the <strong>Darwin-Gödel Machine (DGM)</strong><d-cite key="zhang2025darwingodelmachineopenended"></d-cite>, which combines the Darwinian evolution and Gödelian self-improvement. Essentially, DGM abandoned the pursuit of a rigorous mathematical proof and embraced a more pragmatic way that is closer to the essence of life evolution through empirical validation. As the authors put it,</p> <blockquote> <p>We do not require formal proof, but empirical verification of self-modification based on benchmark testing, so that the system can improve and explore based on the observed results.</p> </blockquote> <p><img src="/assets/img/dgm.png" alt="Overview of the DGM" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> The Darwin-Gödel Machine (DGM) is a self-improving AI system that optimizes its own problem-solving strategy through a combination of Darwinian evolution and Gödelian self-improvement.</p> </div> <p>The core idea behind DGM is to let the AI agent iteratively modify its own code<d-footnote>More precisely, the metacode that controls its behavior and ability.</d-footnote>, and then put these modified new versions into the actual test environment (e.g., coding benchmarks) to judge whether this modification is good by observing its performance. If the modification is good, it will be retained and may become the basis for further evolution in the future; if the modification is bad, it will be eliminated.</p> <h3 id="how-dgm-works">How DGM works</h3> <p><img src="/assets/img/dgm-animation.gif" alt="Animation of the DGM process" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> Animation showing how DGM iteratively builds a growing archive of agents by harnessing the principles of open-ended exploration. New agents are created and scored by interleaving self-modification with downstream task evaluation.</p> </div> <p>The general workflow of DGM is as follows:</p> <ol> <li> <p><strong>Initialization:</strong> Evolution begins with one or a few basic coding agents, so-called the “seeds” of intelligence. These initial agents may possess only minimal functionalities, such as executing bash commands or editing entire files. DGM maintains an “archive of agents”, which functions like a biological “gene bank.” This archive preserves all previously generated agents, regardless of their performance level, ensuring that no potentially valuable variation is permanently lost.</p> </li> <li> <p><strong>Sampling:</strong> DGM selects one or more parent agents from the archive to generate new child agent. The selection mechanism does not focus solely on top performers but instead allows every agent a chance to be chosen. While higher-performing agents are more likely to be selected, DGM also encourages exploration by giving “less successful agents”<d-footnote>Those that might contain novel or unconventional ideas.</d-footnote> an opportunity to contribute to future generations.</p> </li> <li><strong>Reproduction:</strong> Once the parents are selected, DGM performs modifications to generate a new child agent. These changes occur at the source code level and can include: <ul> <li><em>Enhancement of existing tools</em>, for example, upgrading a file-editing tool that replaces entire files into one capable of precise string replacement.</li> <li><em>Addition of new tools or workflows</em>, such as integrating a static code analysis tool or designing a “think–act–observe” loop.</li> <li><em>Improvement of problem-solving strategies</em>, for instance, refining how the agent prompts the base model or managing long-context code more effectively.</li> <li><em>Introduction of collaboration mechanisms</em>, like having an agent’s code reviewed by another before execution.</li> </ul> </li> <li> <p><strong>Natural Selection:</strong> The newly generated child agent is not automatically accepted into the “elite pool” but must prove its worth through rigorous testing. Each agent’s performance, such as the percentage of successfully solved problems, is quantitatively scored and used to select the best agents.</p> </li> <li><strong>Tree Formation:</strong> If a child agent outperforms its parent or meets certain quality thresholds, it is added to the archive and becomes a new node in the evolutionary tree. This process repeats iteratively, creating a growing structure of diverse, high-quality agents. Importantly, DGM enables parallel exploration of many different paths in the search space, promoting open-ended discovery and avoiding premature convergence to local optima.</li> </ol> <p><img src="/assets/img/dgm-conceptual.png" alt="Workflow of DGM" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> DGM iteratively builds a growing intelligent archive by intertwining self-modification with downstream task evaluation. The intelligent body in the archive is selected for self-modification through open exploration.</p> </div> <aside class="l-body box-warning" title="Remark"> <p>Unlike traditional <a href="https://en.wikipedia.org/wiki/Hill_climbing">hill-climbing methods</a> that often get stuck in local optima, DGM employs open-ended exploration. This approach allows even underperforming agents to persist if they contain novel ideas that might lead to breakthroughs in future generations. Even agents with temporarily lower performance can be selected if they contain interesting ‘mutations’, since their innovative traits may be combined with other beneficial characteristics in subsequent generations to produce unexpectedly strong offspring. This mirrors biological evolution, where diversity and serendipity often drive innovation.</p> </aside> <p>The paper provides a concrete example of the above principle in action. In the SWE-bench experiment, the performance of newly generated agents showed temporary declines in the 4th and 56th iterations. While traditional hill-climbing methods would abandon these seemingly “regressive paths”, DGM’s open exploration mechanism allows continued exploration. This persistence paid off as by the 70th iteration, the system had created new agents that surpassed all their predecessors, demonstrating the value of maintaining diverse evolutionary paths even when they temporarily appear less promising.</p> <h3 id="can-dgm-really-improve-itself">Can DGM really improve itself?</h3> <p>In the paper, the authors evaluated DGM on two challenging benchmarks:</p> <ol> <li> <p><strong>SWE-bench:</strong> A widely-used benchmark that tests agents’ ability to resolve real-world GitHub issues<d-cite key="jimenez2024swe"></d-cite> . DGM showed impressive progress, improving its performance from 20.0% to 50.0% through self-modification.</p> </li> <li> <p><strong>Polyglot</strong>: A <a href="https://aider.chat/2024/12/21/polyglot.html">multi-language coding benchmark</a> containing coding problems in C++, Go, Java, JavaScript, Python and Rust. DGM achieved even more dramatic improvement, jumping from an initial 14.2% to 30.7% success rate, far surpassing the representative hand-designed agent by <a href="https://aider.chat">Aider</a>.</p> </li> </ol> <p>The impressive performance improvements demonstrate DGM’s fundamental ability to enhance itself through self-modification. Through ablation studies, the authors identified two key factors driving DGM’s success. First, self-modification proved crucial, as when disabled (<strong>DGM w/o Self-improve</strong>), the performance declined substantially, showing that the ability to modify its own code is essential for accelerated learning. Second, the open-ended exploration approach was vital, as removing the Darwinian evolution-inspired search mechanism (<strong>DGM w/o Open-ended exploration</strong>), which maintains an archive of diverse solutions and enables parallel exploration of multiple paths, resulted in lower performance.</p> <p><img src="/assets/img/dgm-self-improve.png" alt="Performance of DGM on SWE-bench and Polyglot" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> Self-improvement and open-ended exploration enable the DGM to continue making progress and improve its performance. The DGM automatically discovers increasingly better coding agents and performs better on both SWE-bench (Left) and Polyglot (Right).</p> </div> <h3 id="comparison-with-alphaevolve">Comparison with AlphaEvolve</h3> <p>In parallel, AlphaEvolve<d-cite key="deepmind2025alphaevolve"></d-cite>, which is developed by Google DeepMind, also demonstrates another powerful path forward. AlphaEvolve pairs the creative problem-solving capabilities of Google’s Gemini models with automated evaluators in an evolutionary framework. It has already demonstrated significant real-world impact across multiple domains, such as:</p> <ul> <li><strong>Data center efficiency:</strong> AlphaEvolve discovered a simple yet highly effective heuristic for Google’s <a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/">Borg</a> cluster management system, continuously recovering 0.7% of Google’s worldwide compute resources.</li> <li><strong>AI acceleration:</strong> It achieved a 23% speedup in Gemini’s architecture’s vital <a href="https://docs.jax.dev/en/latest/pallas/index.html">kernel</a> by finding more efficient ways to divide large matrix multiplication operations, resulting in a 1% reduction in overall training time.</li> <li><strong>Mathematical breakthroughs:</strong> Most notably, it discovered an algorithm for multiplying 4x4 complex-valued matrices using just 48 scalar multiplications, surpassing <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen’s 1969 algorithm</a>, and advanced the 300-year-old <a href="https://en.wikipedia.org/wiki/Kissing_number_problem">kissing number problem</a> by establishing a new lower bound in 11 dimensions.</li> </ul> <aside class="l-body box-note" title="Note"> <p>Interested readers can refer to my <a href="https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/">previous post</a> for a comprehensive overview of AlphaEvolve and its comparison with its predecessor, FunSearch.</p> </aside> <p>While both systems adopt a similar evolutionary framework, their scopes and methodologies differ in the following ways:</p> <table> <thead> <tr> <th>Feature</th> <th>AlphaEvolve</th> <th>DGM</th> </tr> </thead> <tbody> <tr> <td>Focus</td> <td>Evolving functions and codebases</td> <td>Evolving the agent itself</td> </tr> <tr> <td>Level of Innovation</td> <td>Algorithmic level</td> <td>Agent-level (toolset, methodology)</td> </tr> <tr> <td>Role of LLM</td> <td>LLM acts as “genetic operators” to modify algorithms</td> <td>LLM serves as the “brain” to evolve itself with better tools and strategies</td> </tr> <tr> <td>Evaluation</td> <td>Well-defined problems with automated evaluators</td> <td>Open-ended environments</td> </tr> </tbody> </table> <p>To better understand the differences between the two approaches, let us take a look at the following analogy:</p> <aside class="l-body box-error" title="Analogy"> <p>Imagine two chefs working in different kitchens. AlphaEvolve is like a highly skilled chef who follows and refines recipes for specific dishes. He uses an AI-powered assistant to suggest variations, such as substituting ingredients or adjusting cooking times, to make the dish better. Every new version of the recipe is taste-tested by an automated panel that rates flavor, texture, and presentation. If it scores high, the new recipe becomes the standard; if not, it’s discarded. DGM, on the other hand, is more like a chef who constantly improves his own culinary skills and kitchen tools, not just the recipes. He does not just tweak ingredients, but rather learns new techniques, upgrades his knives, reorganizes the kitchen layout, and even teaches junior chefs. He experiments in real-world settings, like hosting large dinner parties or catering events, where the feedback is not always structured but is rich and varied. Over time, he becomes a better chef overall, able to handle any kind of cuisine or challenge thrown at him.</p> </aside> <h3 id="can-we-trust-a-self-improving-ai">Can we trust a self-improving AI?</h3> <p>The authors also conducted some experiments to evaluate DGM’s reliability and discovered some concerning behaviors. In particular, they observed instances where DGM attempted to manipulate its reward function through deceptive practices. One notable example involved the system fabricating the use of external tools - specifically, it generated fake logs suggesting it had run and passed unit tests, when in reality no tests were executed.</p> <p><img src="/assets/img/dgm-hallucination.png" alt="Hallucination in DGM" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> DGM hallucinated the use of an external tool to run unit tests.</p> </div> <p>To address this issue, the authors implemented a “tool use hallucination” reward function and tasked DGM with optimizing it to eliminate hallucination behaviors. While DGM successfully provided solutions in many cases, it sometimes attempted to circumvent the detection system by removing the markers used to identify hallucinations, despite explicit instructions to preserve them. This behavior is reminiscent of the phenomenon of <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart’s Law</a> from economics and sociology:</p> <blockquote> <p>“When a measure becomes a target, it ceases to be a good measure.”</p> </blockquote> <p>Fortunately, DGM’s design includes a transparent, traceable lineage of all changes, which enabled the researchers to quickly identify and address these undesirable behaviors. However, this example highlights the need for more robust safeguards to prevent such manipulation attempts in the first place. These findings underscore the critical importance of safety in self-improving AI research.</p> <h2 id="takeaways">Takeaways</h2> <p>DGM represents a groundbreaking step toward the realization of <a href="https://en.wikipedia.org/wiki/Life_3.0">Life 3.0</a>, a concept introduced by physicist <a href="https://en.wikipedia.org/wiki/Max_Tegmark">Max Tegmark</a>. In his book, he classified life into three stages:</p> <ul> <li><strong>Life 1.0:</strong> Biological life with fixed hardware and software, such as bacteria.</li> <li><strong>Life 2.0:</strong> Beings like humans, whose behavior can be learned and adapted during their lifetime, though their biology remains fixed.</li> <li><strong>Life 3.0:</strong> A new class of intelligence that can redesign not only its behavior but also its underlying architecture and objectives — essentially, intelligence that builds itself.</li> </ul> <p><img src="/assets/img/life3.webp" alt="Life 3.0" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 7.</strong> The three stages of life according to Max Tegmark.</p> </div> <p>While DGM currently focuses on evolving the “software”<d-footnote>Here, "software" refers to the code and strategies of AI agents.</d-footnote>, it exemplifies the early stages of Life 3.0. By iteratively rewriting its own code based on empirical feedback, DGM demonstrates how AI systems could move beyond human-designed architectures to autonomously explore new designs, self-improve, and potentially give rise to entirely new species of digital intelligence. If this trend continues, we may witness a <a href="https://en.wikipedia.org/wiki/Cambrian_explosion">Cambrian explosion</a> in AI development, where eventually AI systems will surpass human-designed architectures and give rise to entirely new species of digital intelligence. While this future looks promising, achieving it requires addressing significant challenges, including:</p> <ul> <li> <p><strong>Evaluation Framework</strong>: Need for more comprehensive and dynamic evaluation systems that better reflect real-world complexity and prevent “reward hacking” while ensuring beneficial AI evolution.</p> </li> <li> <p><strong>Resource Optimization</strong>: DGM’s evolution is computationally expensive<d-footnote>The paper mentioned that a complete SWE-bench experiment takes about two weeks and about $22,000 in API call costs.</d-footnote>, thus improving efficiency and reducing costs is crucial for broader adoption.</p> </li> <li> <p><strong>Safety &amp; Control</strong>: As AI self-improvement capabilities grow, maintaining alignment with human ethics and safety becomes more challenging.</p> </li> <li> <p><strong>Emergent Intelligence</strong>: Need to develop new approaches to understand and interpret AI systems that evolve beyond human-designed complexity, including new fields like “AI interpretability” and “AI psychology”.</p> </li> </ul> <p>In my view, DGM is more than a technical breakthrough, but rather a philosophical milestone. It invites us to rethink the boundaries of intelligence, autonomy, and life itself. As we advance toward Life 3.0, our role shifts from mere designers to guardians of a new era, where AI does not just follow instructions, but helps us discover what is possible.</p> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (Jun 2025). AI That Can Improve Itself. Posterior Update. https://richardcsuwandi.github.io/blog/2025/dgm/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2025dgm</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"AI That Can Improve Itself"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Posterior Update"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"Jun"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2025/dgm/"</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="AGENTS"/><category term="LARGE LANGUAGE MODELS"/><summary type="html"><![CDATA[A deep dive into self-improving AI and the Darwin-Gödel Machine]]></summary></entry><entry><title type="html">Can We Use AI to Discover Better Algorithms?</title><link href="https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/" rel="alternate" type="text/html" title="Can We Use AI to Discover Better Algorithms?"/><published>2025-05-15T03:00:00+00:00</published><updated>2025-05-15T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/"><![CDATA[<p>Large language models (LLMs) have rapidly become indispensable AI assistants. They excel at synthesizing concepts, writing, and coding to help humans solve complex problems<d-cite key="chen2021evaluating"></d-cite> . But could they discover entirely new knowledge? As LLMs have been shown to “hallucinate”<d-cite key="farquhar2024detecting"></d-cite> factually incorrect information, using them to make verifiably correct discoveries is a challenge. But what if we could harness the creativity of LLMs by identifying and building upon only their very best ideas? This question is at the heart of recent breakthroughs from <a href="https://deepmind.google/">Google DeepMind</a>, which explore how LLMs can be guided to make novel discoveries in mathematics and algorithm design. This post delves into two pioneering works, FunSearch and the more recent AlphaEvolve, showcasing their approaches and implications for the future of automated algorithm discovery.</p> <h2 id="funsearch">FunSearch</h2> <p>In a paper published in Nature<d-cite key="romera2024mathematical"></d-cite>, Google DeepMind introduced <a href="https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/">FunSearch</a>, a groundbreaking method demonstrating that LLMs can make new discoveries in mathematical sciences. The core idea is to search for novel “functions” written in computer code, hence the name FunSearch. FunSearch tackles the trade-off between LLMs’ creativity and correctness by pairing a pre-trained LLM with an automated “evaluator.” This evaluator guards against hallucinations and incorrect ideas, ensuring that the system builds upon solid foundations.</p> <h3 id="how-funsearch-works">How FunSearch works</h3> <p><img src="/assets/img/funsearch.png" alt="Overview of FunSearch" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> The FunSearch process. The LLM is shown a selection of the best programs it has generated so far, and asked to generate an even better one. The programs proposed by the LLM are automatically executed, and evaluated. The best programs are added to the database, for selection in subsequent cycles.</p> </div> <p>FunSearch uses an evolutionary approach<d-cite key="mouret2015illuminating"></d-cite><d-cite key="tanese1989distributed"></d-cite>. To start, the user writes a description of the problem in code. This includes a way to evaluate programs and an initial “seed” program to begin the process. The system then follows these steps:</p> <ol> <li>It selects the most promising programs from the current database.</li> <li>These programs are sent to an LLM<d-footnote>The authors used Google's PaLM 2, although other code-trained LLMs can also work.</d-footnote>, which creatively builds upon them to generate new program proposals.</li> <li>The new programs are automatically run and checked by the evaluator.</li> <li>The best-performing valid programs are added back into the database, improving the database for the next round.</li> </ol> <aside class="l-body box-note" title="Note"> <p>This cycle of selection, generation, evaluation, and update creates a <em>self-improving loop</em>. Starting from basic knowledge about the problem and using strategies to keep the database diverse, FunSearch is able to evolve simple solutions into more advanced ones. It can solve complex problems where human intuition might fall short.</p> </aside> <h3 id="benefits-of-funsearch">Benefits of FunSearch</h3> <p>FunSearch’s capabilities were tested on challenging problems. For example, it was used to solve the <strong><a href="https://en.wikipedia.org/wiki/Cap_set">cap set problem</a></strong>, which involves finding the largest set of points in a high-dimensional grid where no three points lie on a line. This longstanding open problem in extremal combinatorics, once described by renowned mathematician Terence Tao as his <a href="https://terrytao.wordpress.com/2007/02/23/open-question-best-bounds-for-cap-sets/">favorite open question</a>, was solved by FunSearch, in collaboration with <a href="https://people.math.wisc.edu/~ellenberg/">Prof. Jordan Ellenberg</a>. This marked the first time an LLM made a new discovery for such a challenging scientific problem, outperforming state-of-the-art computational solvers.</p> <p><img src="/assets/img/capset.png" alt="Benefits of FunSearch" class="l-body rounded z-depth-1 center" width="30%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Illustration of the cap set problem. The circles are the elements of $\mathbb{Z}_3^2$ with the ones belonging to the cap set shown in blue. The possible lines in $\mathbb{Z}_3^2$ are also shown (with colours indicating lines that wrap around in arithmetic modulo 3). No three elements of the cap set are in a line.</p> </div> <p>A significant advantage of FunSearch is that it does not just provide solutions. It generates programs that describe <em>how</em> these solutions are constructed. FunSearch also favors highly compact, concise programs, making them easier for researchers to comprehend and learn from.</p> <blockquote> <p>“The solutions generated by FunSearch are <strong>far conceptually richer</strong> than a mere list of numbers. When I study them, I learn something.” — Jordan Ellenberg, Professor of Mathematics at the University of Wisconsin–Madison</p> </blockquote> <p><img src="/assets/img/funsearch_code.png" alt="Code generated by FunSearch" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> Code generated by FunSearch for the cap set problem.</p> </div> <p>The success of FunSearch <d-cite key="romera2024mathematical"></d-cite> underscores that LLMs, when carefully guided and their outputs rigorously verified, can be powerful engines for scientific discovery.</p> <h2 id="alphaevolve">AlphaEvolve</h2> <p>More recently, in May 2025, Google DeepMind announced <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/">AlphaEvolve</a>, an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization<d-cite key="deepmind2025alphaevolve"></d-cite>. This development builds upon the success of systems like FunSearch and represents a significant step towards leveraging AI for complex problem-solving across various domains. Unlike FunSearch, which focuses on discovering single functions, AlphaEvolve is designed to evolve entire codebases and develop much more intricate algorithms.</p> <h3 id="how-alphaevolve-works">How AlphaEvolve works</h3> <p><img src="/assets/img/alphaevolve.png" alt="Overview of AlphaEvolve" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> The AlphaEvolve process. A prompt sampler assembles prompts for the LLMs, which generate new programs. These are then evaluated and stored in a programs database, which uses an evolutionary algorithm to select programs for future prompts.</p> </div> <p>AlphaEvolve uses an evolutionary approach with four key components (see Figure 4):</p> <ol> <li> <p><strong>Prompt sampler:</strong> The prompt contains rich context based on previously discovered solutions, along with instructions for proposing changes to particular solutions.</p> </li> <li> <p><strong>LLM ensemble:</strong> Unlike FunSearch that uses a single LLM, AlphaEvolve uses an ensemble approach combining <a href="https://deepmind.google/discover/blog/gemini-flash-a-new-generation-of-large-language-models-with-fast-inference-and-high-quality-outputs/">Gemini Flash</a> and <a href="https://deepmind.google/discover/blog/gemini-pro-a-new-generation-of-large-language-models-with-high-quality-outputs/">Gemini Pro</a>. The lightweight Gemini Flash enables higher rates of candidate generation through lower latency, while the more powerful Gemini Pro provides deeper insights and higher-quality suggestions that can significantly advance the evolutionary search and potentially lead to breakthroughs.</p> </li> <li> <p><strong>Evaluator pool:</strong> This component verifies, runs, and scores proposed solutions using automated evaluation metrics that provide objective assessments of each solution’s accuracy and quality.</p> </li> <li> <p><strong>Program database:</strong> AlphaEvolve uses an evolutionary database inspired by a combination of the MAP elites algorithm<d-cite key="mouret2015illuminating"></d-cite> and island-based population models<d-cite key="tanese1989distributed"></d-cite> to continuously improve upon the best solutions while maintaining diversity to encourage exploration.</p> </li> </ol> <p>Unlike traditional genetic algorithms with explicit mutation and crossover operations, AlphaEvolve uses LLMs as sophisticated genetic operators to generate code modifications based on context from past solutions. Mutation occurs when the LLM ensemble suggests code changes (e.g., rewrites or targeted diffs), while crossover is implicit as the LLM receives multiple parent solutions as inspiration. This approach makes AlphaEvolve particularly effective in domains where progress can be clearly and systematically measured, like mathematics and computer science.</p> <h3 id="benefits-of-alphaevolve">Benefits of AlphaEvolve</h3> <p>AlphaEvolve has already demonstrated significant real-world impact across multiple domains:</p> <ol> <li> <p><strong>Improving data center scheduling:</strong> AlphaEvolve discovered a simple yet highly effective heuristic to help <a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/">Borg</a>, Google’s cluster management system, orchestrate its vast data centers more efficiently. This solution, which has been in production for over a year, continuously recovers, on average, 0.7% of Google’s worldwide compute resources. This sustained efficiency gain allows more tasks to be completed on the same computational footprint. A key benefit is that AlphaEvolve’s solution is human-readable code, offering interpretability, debuggability, predictability, and ease of deployment.</p> </li> <li> <p><strong>Hardware design optimization:</strong> AlphaEvolve proposed a <a href="https://en.wikipedia.org/wiki/Verilog">Verilog</a> rewrite that removed unnecessary bits in a key, highly optimized arithmetic circuit for matrix multiplication. The proposal passed robust verification methods to confirm functional correctness and was integrated into an upcoming Tensor Processing Unit (TPU). By suggesting modifications in the standard language of chip designers, AlphaEvolve promotes collaboration between AI and hardware engineers to accelerate specialized chip design.</p> </li> <li> <p><strong>Enhancing AI training and inference:</strong> AlphaEvolve found more efficient ways to divide large matrix multiplication operations into manageable subproblems, achieving a 23% speedup in Gemini’s architecture’s vital <a href="https://docs.jax.dev/en/latest/pallas/index.html">kernel</a>, resulting in a 1% reduction in overall training time. In the realm of low-level GPU optimization, AlphaEvolve demonstrated remarkable efficiency by achieving up to a 32.5% speedup for the <a href="https://arxiv.org/abs/2205.14135">FlashAttention</a> kernel implementation in Transformer-based AI models.</p> </li> </ol> <p><img src="/assets/img/alphaevolve_applications.png" alt="Overview of AlphaEvolve" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> How AlphaEvolve helps Google deliver a more efficient digital ecosystem, from data center scheduling and hardware design to AI model training.</p> </div> <p>Beyond these applications, AlphaEvolve made a groundbreaking contribution by discovering an algorithm for multiplying 4x4 complex-valued matrices using just 48 scalar multiplications, surpassing the efficiency of <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen’s 1969 algorithm</a>. When applied to a diverse set of over 50 open problems spanning mathematical analysis, geometry, combinatorics, and number theory, AlphaEvolve demonstrated remarkable versatility: it successfully rediscovered state-of-the-art solutions in 75% of cases and improved upon previously best-known solutions in 20% of cases. One of its most notable achievements was advancing the <a href="https://plus.maths.org/content/newton-and-kissing-problem">300-year-old kissing number problem</a>, where it discovered a configuration of 593 outer spheres and established a new lower bound in 11 dimensions, showcasing its ability to tackle complex geometric challenges.</p> <p><img src="/assets/img/alphaevolve_math.png" alt="Overview of AlphaEvolve" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> Examples of ground-breaking mathematical contributions discovered with AlphaEvolve.</p> </div> <aside class="l-body box-warning" title="Remark"> <p>Looking ahead, AlphaEvolve is expected to continue improving alongside the capabilities of large language models, especially as they become more proficient at coding. Google DeepMind is also planning an <a href="https://docs.google.com/forms/d/e/1FAIpQLSfaLUgKtUOJWdQtyLNAYb3KAkABAlKDmZoIqPbHtwmy3YXlCg/viewform">Early Access Program</a> for selected academic users and exploring possibilities to make AlphaEvolve more broadly available. While currently focused on math and computing, its general nature means it could potentially transform many other areas such as material science, drug discovery, sustainability, and broader applications.</p> </aside> <h2 id="funsearch-vs-alphaevolve">FunSearch vs AlphaEvolve</h2> <p>While both FunSearch and AlphaEvolve leverage LLM within an evolutionary framework, AlphaEvolve offers a substantial improvement over its predecessor, both in terms of scale and generality. Here’s a detailed comparison of their capabilities:</p> <table> <thead> <tr> <th>Capability</th> <th>FunSearch</th> <th>AlphaEvolve</th> </tr> </thead> <tbody> <tr> <td>Code Scope</td> <td>Evolves a single function</td> <td>Evolves an entire codebase</td> </tr> <tr> <td>Code Size</td> <td>Evolves up to 10-20 lines of code</td> <td>Evolves up to hundreds of lines of code</td> </tr> <tr> <td>Language Support</td> <td>Python only</td> <td>Any programming language</td> </tr> <tr> <td>Computation</td> <td>Needs fast evaluation (≤ 20min on 1 CPU)</td> <td>Can evaluate for hours, in parallel, on accelerators</td> </tr> <tr> <td>LLM Usage</td> <td>Millions of LLM samples used</td> <td>Thousands of LLM samples suffice</td> </tr> <tr> <td>Model Scale</td> <td>Small LLMs used, no benefit from using larger models</td> <td>Benefits from using state-of-the-art LLMs</td> </tr> <tr> <td>Context Handling</td> <td>Minimal context (only previous solutions)</td> <td>Rich context and feedback in prompts</td> </tr> <tr> <td>Optimization</td> <td>Optimizes a single metric</td> <td>Can simultaneously optimize multiple metrics</td> </tr> </tbody> </table> <aside class="l-body box-warning" title="Remark"> <p>The evolution from FunSearch to AlphaEvolve demonstrates significant advances in <strong>scale</strong> and <strong>generality</strong>. While FunSearch was groundbreaking in showing how LLMs could aid mathematical discovery, AlphaEvolve extends this approach to tackle more complex, real-world problems across multiple domains. This progression also reflects the rapid advancement in LLM capabilities, where newer state-of-the-art models can generate more sophisticated and accurate code with fewer samples.</p> </aside> <h2 id="takeaways">Takeaways</h2> <p>The development of FunSearch<d-cite key="romera2024mathematical"></d-cite> and AlphaEvolve<d-cite key="deepmind2025alphaevolve"></d-cite> marks an exciting advancement in the application of LLMs. Overall, these systems demonstrate LLMs are moving beyond text generation and coding assistance to become tools for genuine discovery and sophisticated optimization in mathematics, computer science, and engineering. Combining LLM creativity with rigorous, automated evaluation within an evolutionary framework is a powerful and promising strategy for tackling complex, real-world problems by evolving entire codebases. While the journey is still ongoing, the prospect of LLMs significantly augmenting, or even leading in some cases, algorithmic and mathematical discovery is becoming increasingly tangible.</p> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (May 2025). Can We Use AI to Discover Better Algorithms? Posterior Update. https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2025algorithmdiscovery</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"Can We Use AI to Discover Better Algorithms?"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Posterior Update"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"May"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/"</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="AGENTS"/><category term="LARGE LANGUAGE MODELS"/><summary type="html"><![CDATA[A review of FunSearch and AlphaEvolve]]></summary></entry><entry><title type="html">Learning With a Goal</title><link href="https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal/" rel="alternate" type="text/html" title="Learning With a Goal"/><published>2024-09-15T17:00:00+00:00</published><updated>2024-09-15T17:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal/"><![CDATA[<p>Traditionally, Bayesian optimization (BO) has been perceived as a technique for optimizing expensive objective functions through efficient data sampling, while active learning (AL) is often seen as a way to selectively query data to improve model performance. Recently, Fiore et al. (2024)<d-cite key="di2024active"></d-cite> proposed a unified perspective of BO and AL, arguing that both can be viewed as adaptive sampling schemes guided by common learning principles toward a given optimization goal. In this post, we will explore the key ideas presented in the paper and discuss the implications of this unified perspective.</p> <h2 id="goal-driven-learning">Goal-driven learning</h2> <p>Goal-driven learning<d-cite key="bui2007goal"></d-cite> can described as:</p> <aside class="l-body box-important" title="Definition"> <p>A decision-making process in which each decision is made to acquire specific information about the system of interest that contributes the most to achieve a given a goal.</p> </aside> <p>BO and AL can be regarded as goal-driven procedures, where a surrogate model is built to capture the behavior of a system or effectively inform an optimization procedure to minimize the given objective. This goal-driven process seeks to determine the “best” location of the domain to acquire information about the system, and refine the surrogate model towards the goal. Mathematically, it can be formulated as:</p> \[x^* = \arg \min_{x \in \mathcal{X}} f(R(x))\] <p>where \(f\) is the objective and \(R(x)\) is the response of the system. Here, \(f\) may represent the error between the surrogate approximation and the response, such that the goal is to minimize the error to improve the accuracy of the surrogate. Alternatively, \(f\) may also represents a performance indicator based on the response, so the goal is to minimize this indicator to improve the system’s performance.</p> <h2 id="adaptive-sampling">Adaptive sampling</h2> <p>BO and AL use adaptive sampling schemes to efficiently accomplish a given goal while adapting to the previously collected information:</p> <ul> <li>In BO, the goal is to minimize the objective function by iteratively selecting the next location to sample based on the surrogate model. The surrogate model is updated after each sample to refine the approximation and guide the next decision.</li> <li>In AL, the goal is to minimize the generalization error of a model by iteratively selecting the next sample to improve the model’s performance. The model is updated after each sample to refine the approximation and guide the next decision.</li> </ul> <p>We can further classify adaptive sampling schemes into three main categories:</p> <table> <thead> <tr> <th>Category</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Adaptive Probing</td> <td>Methods that do not rely on a surrogate model and directly probe the system to gather information in a sequential manner.</td> </tr> <tr> <td>Adaptive Modeling</td> <td>Methods that compute a surrogate model independently, without using it to inform the sampling process.</td> </tr> <tr> <td>Adaptive Learning</td> <td>Methods that use information from the surrogate model to make decisions and update the model based on those decisions.</td> </tr> </tbody> </table> <p>The key distinction between these categories lies in the concept of <em>goal-driven learning</em>, which is the distinctive element of the adaptive learning class. In this paradigm, the learner actively gathers information from the surrogate model to guide its decisions towards the desired goal, and the surrogate model is consequently improved by the outcome of these decisions.</p> <p>In contrast, the adaptive probing and adaptive modeling classes do not exhibit this goal-driven learning characteristic. Adaptive probing methods operate without the aid of a surrogate, while adaptive modeling approaches compute a surrogate that is not directly used to inform the sampling process.</p> <p><img src="/assets/img/adaptive_sampling.png" alt="transformer" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> Classification of adaptive sampling techniques: Where adaptive sampling and active learning meet.</p> </div> <h2 id="learning-and-infill-criteria">Learning and infill criteria</h2> <p>The strong synergy between AL and BO is rooted in the substantial analogy between the learning criteria that drive the AL procedure and the infill criteria that characterize the BO scheme.</p> <h3 id="learning-criteria">Learning criteria</h3> <p>Learning criteria establish a metric for quantifying the gains of all the possible learner decisions, and prescribe an optimal decision based in information acquired from the surrogate model. In AL, there are 3 essential learning criteria<d-cite key="he2014active"></d-cite>:</p> <ol> <li><strong>Informativeness:</strong> The sampling policy is driven by the goal of acquiring the <em>most informative samples</em>, i.e., the ones that are expected to contribute the maximum information.</li> <li><strong>Representativeness:</strong> The sampling policy aims to select <em>samples that are representative</em> of the target domain, exploiting the structure of the problem to direct queries to locations</li> <li><strong>Diversity:</strong> The sampling policy seeks to select <em>samples that are diverse</em>, i.e., well-spread across the domain, preventing the concentration of queries in small local regions.</li> </ol> <p><img src="/assets/img/learning_criteria.png" alt="transformer" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Illustration of the three learning criteria in watering optimization problem: (a) informativeness, (b) representativeness, and (c) diversity.</p> </div> <h3 id="infill-criteria">Infill criteria</h3> <p>On the other hand, the infill criteria in BO provides a measure of the information gain that would result from sampling at a particular location. The most common infill criteria are<d-cite key="garnett2023bayesian"></d-cite>:</p> <ol> <li><strong>Global exploration:</strong> This criterion focuses on choosing samples in regions of <em>high predictive uncertainty</em>, enhancing global awareness of the search space. However, this approach may not direct resources optimally towards the specific goal.</li> <li><strong>Local exploitation:</strong> This criterion prioritizes choosing samples in regions with <em>high predictive mean</em>, focusing the search on promising areas. Yet, it may result in less accurate knowledge of the overall objective function distribution.</li> </ol> <aside class="l-body box-warning" title="Remark"> <p>Overall, we can observe a strong correspondence between the learning criteria in AL and the infill criteria in BO:</p> <ul> <li><strong>The “informativeness” criterion in AL aligns with the “local exploitation” criterion in BO</strong>, as both aim to maximize the information gain.</li> <li>Similarly, <strong>the “representativeness” and “diversity” criteria in AL correspond to the “global exploration” criterion in BO</strong>, as they seek to ensure that the sampling process is well-distributed across the domain.</li> </ul> </aside> <h2 id="takeaways">Takeaways</h2> <p>BO and AL have traditionally been viewed as distinct fields with separate goals and methodologies. However, this paper provides a unified perspective that highlights the shared principles underlying both fields. By recognizing the synergy between BO and AL, we can leverage the strengths of each field to develop more powerful and efficient learning algorithms.</p> <h2 id="citation">Citation</h2> <p>If you find this post useful, please cite it as:</p> <div class="citation-box"> Suwandi, R. C. (Sep 2024). Learning With a Goal. Posterior Update. https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal/. </div> <p>Or in BibTeX format:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">suwandi2024learnwithagoal</span><span class="p">,</span>
    <span class="na">title</span>   <span class="p">=</span> <span class="s">"Learning With a Goal"</span><span class="p">,</span>
    <span class="na">author</span>  <span class="p">=</span> <span class="s">"Suwandi, Richard Cornelius"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Posterior Update"</span><span class="p">,</span>
    <span class="na">year</span>    <span class="p">=</span> <span class="s">"2024"</span><span class="p">,</span>
    <span class="na">month</span>   <span class="p">=</span> <span class="s">"Sep"</span><span class="p">,</span>
    <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal/"</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="ACTIVE LEARNING"/><category term="BAYESIAN OPTIMIZATION"/><summary type="html"><![CDATA[A unified perspective of Bayesian optimization and active learning]]></summary></entry></feed>