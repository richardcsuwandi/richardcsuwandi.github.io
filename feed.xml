<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://richardcsuwandi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://richardcsuwandi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-04T05:03:31+00:00</updated><id>https://richardcsuwandi.github.io/feed.xml</id><title type="html">Richard Cornelius Suwandi</title><subtitle></subtitle><entry><title type="html">AI that can improve itself</title><link href="https://richardcsuwandi.github.io/blog/2025/dgm/" rel="alternate" type="text/html" title="AI that can improve itself"/><published>2025-06-01T03:00:00+00:00</published><updated>2025-06-01T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/dgm</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/dgm/"><![CDATA[<p>Most AI systems today are stuck in a “cage” designed by humans. They rely on fixed architectures crafted by engineers and lack the ability to evolve autonomously over time. This is the <a href="https://en.wikipedia.org/wiki/Achilles%27_heel">Achilles heel</a> of modern AI — like a car, no matter how well the engine is tuned and how skilled the driver is, it cannot change its body structure or engine type to adapt to a new track on its own. But what if AI could learn and improve its own capabilities without human intervention? In this post, we will dive into the concept of self-improving systems and a recent effort towards building one.</p> <h2 id="learning-to-learn">Learning to Learn</h2> <p>The idea of building systems that can improve themselves brings us to the concept of <a href="https://people.idsia.ch/~juergen/metalearning.html">meta-learning</a>, or “learning to learn” <d-cite key="thrun1998learning"></d-cite>, which aims to create systems that not only solve problems but also evolve their problem-solving strategies over time. One of the most ambitious efforts in this direction is the Gödel Machine<d-cite key="schmidhuber2003godel"></d-cite>, proposed by Jürgen Schmidhuber decades ago and was named after the famous mathematician <a href="https://en.wikipedia.org/wiki/Kurt_Gödel">Kurt Gödel</a>. A Gödel Machine is a hypothetical self-improving AI system that optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy. It represents the ultimate form of self-awareness in AI, an agent that can reason about its own limitations and modify itself accordingly.</p> <p><img src="/assets/img/godel.jpg" alt="Overview of a Gödel machine" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> Gödel machine is a hypothetical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy.</p> </div> <p>While this idea is interesting, formally proving whether a code modification of a complex AI system is <em>absolutely beneficial</em> is almost an impossible task without restrictive assumptions. This part stems from the inherent difficulty revealed by the <a href="https://en.wikipedia.org/wiki/Halting_problem">Halting Problem</a> and <a href="https://en.wikipedia.org/wiki/Rice%27s_theorem">Rice’s Theorem</a> in computational theory, and is also related to the inherent limitations of the logical system implied by <a href="https://en.wikipedia.org/wiki/Gödel%27s_incompleteness_theorems">Gödel’s incompleteness theorem</a>. These theoretical constraints make it nearly impossible to predict the complete impact of code changes without making restrictive assumptions. To illustrate this, consider a simple analogy: just as you cannot guarantee that a new software update will improve your computer’s performance without actually running it, an AI system faces an even greater challenge in predicting the long-term consequences of modifying its own complex codebase.</p> <h2 id="darwin-gödel-machine">Darwin-Gödel Machine</h2> <p>To “relax” the requirement of formal proof, a recent work by proposed the <strong>Darwin-Gödel Machine (DGM)</strong><d-cite key="zhang2025darwingodelmachineopenended"></d-cite>, which combines the Darwinian evolution and Gödelian self-improvement. Essentially, DGM abandoned the pursuit of a rigorous mathematical proof and embraced a more pragmatic way that is closer to the essence of life evolution through empirical validation. As the authors put it,</p> <blockquote> <p>We do not require formal proof, but empirical verification of self-modification based on benchmark testing, so that the system can improve and explore based on the observed results.</p> </blockquote> <p><img src="/assets/img/dgm.png" alt="Overview of the DGM" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> The Darwin-Gödel Machine (DGM) is a self-improving AI system that optimizes its own problem-solving strategy through a combination of Darwinian evolution and Gödelian self-improvement.</p> </div> <p>The core idea behind DGM is to let the AI agent iteratively modify its own code<d-footnote>More precisely, the metacode that controls its behavior and ability</d-footnote>, and then put these modified new versions into the actual test environment (e.g., coding benchmarks) to judge whether this modification is good by observing its performance. If the modification is good, it will be retained and may become the basis for further evolution in the future; if the modification is bad, it will be eliminated.</p> <h3 id="how-dgm-works">How DGM Works</h3> <p><img src="/assets/img/dgm-animation.gif" alt="Animation of the DGM process" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> Animation showing how DGM iteratively builds a growing archive of agents by harnessing the principles of open-ended exploration. New agents are created and scored by interleaving self-modification with downstream task evaluation.</p> </div> <p>The general workflow of DGM is as follows:</p> <ol> <li> <p><strong>Initialization:</strong> Evolution begins with one or a few basic coding agents, so-called the “seeds” of intelligence. These initial agents may possess only minimal functionalities, such as executing bash commands or editing entire files. DGM maintains an “archive of agents”, which functions like a biological “gene bank.” This archive preserves all previously generated agents, regardless of their performance level, ensuring that no potentially valuable variation is permanently lost.</p> </li> <li> <p><strong>Sampling:</strong> DGM selects one or more parent agents from the archive to generate new child agent. The selection mechanism does not focus solely on top performers but instead allows every agent a chance to be chosen. While higher-performing agents are more likely to be selected, DGM also encourages exploration by giving “less successful agents”<d-footnote>Those that might contain novel or unconventional ideas</d-footnote> an opportunity to contribute to future generations.</p> </li> <li><strong>Reproduction:</strong> Once the parents are selected, DGM performs modifications to generate a new child agent. These changes occur at the source code level and can include: <ul> <li><em>Enhancement of existing tools</em>, for example, upgrading a file-editing tool that replaces entire files into one capable of precise string replacement.</li> <li><em>Addition of new tools or workflows</em>, such as integrating a static code analysis tool or designing a “think–act–observe” loop.</li> <li><em>Improvement of problem-solving strategies</em>, for instance, refining how the agent prompts the base model or managing long-context code more effectively.</li> <li><em>Introduction of collaboration mechanisms</em>, like having an agent’s code reviewed by another before execution.</li> </ul> </li> <li> <p><strong>Natural Selection:</strong> The newly generated child agent is not automatically accepted into the “elite pool” but must prove its worth through rigorous testing. Each agent’s performance, such as the percentage of successfully solved problems, is quantitatively scored and used to select the best agents.</p> </li> <li><strong>Tree Formation:</strong> If a child agent outperforms its parent or meets certain quality thresholds, it is added to the archive and becomes a new node in the evolutionary tree. This process repeats iteratively, creating a growing structure of diverse, high-quality agents. Importantly, DGM enables parallel exploration of many different paths in the search space, promoting open-ended discovery and avoiding premature convergence to local optima.</li> </ol> <p><img src="/assets/img/dgm-conceptual.png" alt="Workflow of DGM" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4</strong> DGM iteratively builds a growing intelligent archive by intertwining self-modification with downstream task evaluation. The intelligent body in the archive is selected for self-modification through open exploration.</p> </div> <aside class="l-body box-note"> <p>Unlike traditional <a href="https://en.wikipedia.org/wiki/Hill_climbing">hill-climbing methods</a> that often get stuck in local optima, DGM employs open-ended exploration. This approach allows even underperforming agents to persist if they contain novel ideas that might lead to breakthroughs in future generations. Even agents with temporarily lower performance can be selected if they contain interesting ‘mutations’, since their innovative traits may be combined with other beneficial characteristics in subsequent generations to produce unexpectedly strong offspring. This mirrors biological evolution, where diversity and serendipity often drive innovation.</p> </aside> <p>The paper provides a concrete example of the above principle in action. In the SWE-bench experiment, the performance of newly generated agents showed temporary declines in the 4th and 56th iterations. While traditional hill-climbing methods would abandon these seemingly “regressive paths”, DGM’s open exploration mechanism allows continued exploration. This persistence paid off as by the 70th iteration, the system had created new agents that surpassed all their predecessors, demonstrating the value of maintaining diverse evolutionary paths even when they temporarily appear less promising.</p> <h3 id="can-dgm-really-improve-itself">Can DGM Really Improve Itself?</h3> <p>In the paper, the authors evaluated DGM on two challenging benchmarks:</p> <ol> <li> <p><strong>SWE-bench:</strong> A widely-used benchmark that tests agents’ ability to resolve real-world GitHub issues<d-cite key="jimenez2024swe"></d-cite> . DGM showed impressive progress, improving its performance from 20.0% to 50.0% through self-modification.</p> </li> <li> <p><strong>Polyglot</strong>: A <a href="https://aider.chat/2024/12/21/polyglot.html">multi-language coding benchmark</a> containing coding problems in C++, Go, Java, JavaScript, Python and Rust. DGM achieved even more dramatic improvement, jumping from an initial 14.2% to 30.7% success rate, far surpassing the representative hand-designed agent by <a href="https://aider.chat">Aider</a>.</p> </li> </ol> <p>The impressive performance improvements demonstrate DGM’s fundamental ability to enhance itself through self-modification. Through ablation studies, the authors identified two key factors driving DGM’s success. First, self-modification proved crucial, as when disabled (<strong>DGM w/o Self-improve</strong>), the performance declined substantially, showing that the ability to modify its own code is essential for accelerated learning. Second, the open-ended exploration approach was vital, as removing the Darwinian evolution-inspired search mechanism (<strong>DGM w/o Open-ended exploration</strong>), which maintains an archive of diverse solutions and enables parallel exploration of multiple paths, resulted in lower performance.</p> <p><img src="/assets/img/dgm-self-improve.png" alt="Performance of DGM on SWE-bench and Polyglot" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 5</strong> Self-improvement and open-ended exploration enable the DGM to continue making progress and improve its performance. The DGM automatically discovers increasingly better coding agents and performs better on both SWE-bench (Left) and Polyglot (Right).</p> </div> <h3 id="comparison-with-alphaevolve">Comparison with AlphaEvolve</h3> <p>In parallel, AlphaEvolve<d-cite key="deepmind2025alphaevolve"></d-cite>, which is developed by Google DeepMind, also demonstrates another powerful path forward. AlphaEvolve pairs the creative problem-solving capabilities of Google’s Gemini models with automated evaluators in an evolutionary framework. It has already demonstrated significant real-world impact across multiple domains, such as:</p> <ul> <li><strong>Data center efficiency:</strong> AlphaEvolve discovered a simple yet highly effective heuristic for Google’s <a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/">Borg</a> cluster management system, continuously recovering 0.7% of Google’s worldwide compute resources.</li> <li><strong>AI acceleration:</strong> It achieved a 23% speedup in Gemini’s architecture’s vital <a href="https://docs.jax.dev/en/latest/pallas/index.html">kernel</a> by finding more efficient ways to divide large matrix multiplication operations, resulting in a 1% reduction in overall training time.</li> <li><strong>Mathematical breakthroughs:</strong> Most notably, it discovered an algorithm for multiplying 4x4 complex-valued matrices using just 48 scalar multiplications, surpassing <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen’s 1969 algorithm</a>, and advanced the 300-year-old <a href="https://en.wikipedia.org/wiki/Kissing_number_problem">kissing number problem</a> by establishing a new lower bound in 11 dimensions.</li> </ul> <aside class="l-body box-warning"> <p>Interested readers can refer to my <a href="https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/">previous post</a> for a comprehensive overview of AlphaEvolve and its comparison with its predecessor, FunSearch.</p> </aside> <p>While both systems adopt a similar evolutionary framework, their scopes and methodologies differ in the following ways:</p> <table> <thead> <tr> <th>Feature</th> <th>AlphaEvolve</th> <th>DGM</th> </tr> </thead> <tbody> <tr> <td>Focus</td> <td>Evolving functions and codebases</td> <td>Evolving the agent itself</td> </tr> <tr> <td>Level of Innovation</td> <td>Algorithmic level</td> <td>Agent-level (toolset, methodology)</td> </tr> <tr> <td>Role of LLM</td> <td>LLM acts as “genetic operators” to modify algorithms</td> <td>LLM serves as the “brain” to evolve itself with better tools and strategies</td> </tr> <tr> <td>Evaluation</td> <td>Well-defined problems with automated evaluators</td> <td>Open-ended environments</td> </tr> </tbody> </table> <p>To better understand the differences between the two approaches, let us take a look at the following analogy:</p> <aside class="l-body box-note"> <p>Imagine two chefs working in different kitchens. AlphaEvolve is like a highly skilled chef who follows and refines recipes for specific dishes. He uses an AI-powered assistant to suggest variations, such as substituting ingredients or adjusting cooking times, to make the dish better. Every new version of the recipe is taste-tested by an automated panel that rates flavor, texture, and presentation. If it scores high, the new recipe becomes the standard; if not, it’s discarded. DGM, on the other hand, is more like a chef who constantly improves his own culinary skills and kitchen tools, not just the recipes. He does not just tweak ingredients, but rather learns new techniques, upgrades his knives, reorganizes the kitchen layout, and even teaches junior chefs. He experiments in real-world settings, like hosting large dinner parties or catering events, where the feedback is not always structured but is rich and varied. Over time, he becomes a better chef overall, able to handle any kind of cuisine or challenge thrown at him.</p> </aside> <h3 id="can-we-trust-a-self-improving-ai">Can we trust a self-improving AI?</h3> <p>The authors also conducted some experiments to evaluate DGM’s reliability and discovered some concerning behaviors. In particular, they observed instances where DGM attempted to manipulate its reward function through deceptive practices. One notable example involved the system fabricating the use of external tools - specifically, it generated fake logs suggesting it had run and passed unit tests, when in reality no tests were executed.</p> <p><img src="/assets/img/dgm-hallucination.png" alt="Hallucination in DGM" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 6</strong> DGM hallucinated the use of an external tool to run unit tests.</p> </div> <p>To address this issue, the authors implemented a “tool use hallucination” reward function and tasked DGM with optimizing it to eliminate hallucination behaviors. While DGM successfully provided solutions in many cases, it sometimes attempted to circumvent the detection system by removing the markers used to identify hallucinations, despite explicit instructions to preserve them. This behavior is reminiscent of the phenomenon of <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart’s Law</a> from economics and sociology:</p> <blockquote> <p>“When a measure becomes a target, it ceases to be a good measure.”</p> </blockquote> <p>Fortunately, DGM’s design includes a transparent, traceable lineage of all changes, which enabled the researchers to quickly identify and address these undesirable behaviors. However, this example highlights the need for more robust safeguards to prevent such manipulation attempts in the first place. These findings underscore the critical importance of safety in self-improving AI research.</p> <h2 id="takeaways">Takeaways</h2> <p>DGM represents a groundbreaking step toward the realization of <a href="https://en.wikipedia.org/wiki/Life_3.0">Life 3.0</a>, a concept introduced by physicist <a href="https://en.wikipedia.org/wiki/Max_Tegmark">Max Tegmark</a>. In his book, he classified life into three stages:</p> <ul> <li><strong>Life 1.0:</strong> Biological life with fixed hardware and software, such as bacteria.</li> <li><strong>Life 2.0:</strong> Beings like humans, whose behavior can be learned and adapted during their lifetime, though their biology remains fixed.</li> <li><strong>Life 3.0:</strong> A new class of intelligence that can redesign not only its behavior but also its underlying architecture and objectives — essentially, intelligence that builds itself.</li> </ul> <p><img src="/assets/img/life3.webp" alt="Life 3.0" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 7</strong> The three stages of life according to Max Tegmark.</p> </div> <p>While DGM currently focuses on evolving the “software”<d-footnote>the code and strategies of AI agents</d-footnote>, it exemplifies the early stages of Life 3.0. By iteratively rewriting its own code based on empirical feedback, DGM demonstrates how AI systems could move beyond human-designed architectures to autonomously explore new designs, self-improve, and potentially give rise to entirely new species of digital intelligence. If this trend continues, we may witness a <a href="https://en.wikipedia.org/wiki/Cambrian_explosion">Cambrian explosion</a> in AI development, where eventually AI systems will surpass human-designed architectures and give rise to entirely new species of digital intelligence. While this future looks promising, achieving it requires addressing significant challenges, including:</p> <ul> <li> <p><strong>Evaluation Framework</strong>: Need for more comprehensive and dynamic evaluation systems that better reflect real-world complexity and prevent “reward hacking” while ensuring beneficial AI evolution.</p> </li> <li> <p><strong>Resource Optimization</strong>: DGM’s evolution is computationally expensive<d-footnote>The paper mentioned that a complete SWE-bench experiment takes about two weeks and about $22,000 in API call costs.</d-footnote>, thus improving efficiency and reducing costs is crucial for broader adoption.</p> </li> <li> <p><strong>Safety &amp; Control</strong>: As AI self-improvement capabilities grow, maintaining alignment with human ethics and safety becomes more challenging.</p> </li> <li> <p><strong>Emergent Intelligence</strong>: Need to develop new approaches to understand and interpret AI systems that evolve beyond human-designed complexity, including new fields like “AI interpretability” and “AI psychology”.</p> </li> </ul> <p>In my view, DGM is more than a technical breakthrough, but rather a philosophical milestone. It invites us to rethink the boundaries of intelligence, autonomy, and life itself. As we advance toward Life 3.0, our role shifts from mere designers to guardians of a new era, where AI does not just follow instructions, but helps us discover what is possible.</p>]]></content><author><name></name></author><category term="large-language-models"/><category term="agents"/><summary type="html"><![CDATA[A deep dive into self-improving AI and the Darwin-Gödel Machine.]]></summary></entry><entry><title type="html">Can we use AI to discover better algorithms?</title><link href="https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/" rel="alternate" type="text/html" title="Can we use AI to discover better algorithms?"/><published>2025-05-15T03:00:00+00:00</published><updated>2025-05-15T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/"><![CDATA[<p>Large language models (LLMs) have rapidly become indispensable AI assistants. They excel at synthesizing concepts, writing, and coding to help humans solve complex problems<d-cite key="chen2021evaluating"></d-cite> . But could they discover entirely new knowledge? As LLMs have been shown to “hallucinate”<d-cite key="farquhar2024detecting"></d-cite> factually incorrect information, using them to make verifiably correct discoveries is a challenge. But what if we could harness the creativity of LLMs by identifying and building upon only their very best ideas? This question is at the heart of recent breakthroughs from <a href="https://deepmind.google/">Google DeepMind</a>, which explore how LLMs can be guided to make novel discoveries in mathematics and algorithm design. This post delves into two pioneering works, FunSearch and the more recent AlphaEvolve, showcasing their approaches and implications for the future of automated algorithm discovery.</p> <h2 id="funsearch">FunSearch</h2> <p>In a paper published in Nature<d-cite key="romera2024mathematical"></d-cite>, Google DeepMind introduced <a href="https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/">FunSearch</a>, a groundbreaking method demonstrating that LLMs can make new discoveries in mathematical sciences. The core idea is to search for novel “functions” written in computer code, hence the name FunSearch. FunSearch tackles the trade-off between LLMs’ creativity and correctness by pairing a pre-trained LLM with an automated “evaluator.” This evaluator guards against hallucinations and incorrect ideas, ensuring that the system builds upon solid foundations.</p> <h3 id="how-funsearch-works">How FunSearch works</h3> <p><img src="/assets/img/funsearch.png" alt="Overview of FunSearch" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> The FunSearch process. The LLM is shown a selection of the best programs it has generated so far, and asked to generate an even better one. The programs proposed by the LLM are automatically executed, and evaluated. The best programs are added to the database, for selection in subsequent cycles.</p> </div> <p>FunSearch uses an evolutionary approach<d-cite key="mouret2015illuminating"></d-cite><d-cite key="tanese1989distributed"></d-cite>. To start, the user writes a description of the problem in code. This includes a way to evaluate programs and an initial “seed” program to begin the process. The system then follows these steps:</p> <ol> <li>It selects the most promising programs from the current database.</li> <li>These programs are sent to an LLM<d-footnote>In their work, Google's PaLM 2 was used, though other code-trained LLMs can also work.</d-footnote>, which creatively builds upon them to generate new program proposals.</li> <li>The new programs are automatically run and checked by the evaluator.</li> <li>The best-performing valid programs are added back into the database, improving the database for the next round.</li> </ol> <aside class="l-body box-note"> <p>This cycle of selection, generation, evaluation, and update creates a <em>self-improving loop</em>. Starting from basic knowledge about the problem and using strategies to keep the database diverse, FunSearch is able to evolve simple solutions into more advanced ones. It can solve complex problems where human intuition might fall short.</p> </aside> <h3 id="benefits-of-funsearch">Benefits of FunSearch</h3> <p>FunSearch’s capabilities were tested on challenging problems. For example, it was used to solve the <strong><a href="https://en.wikipedia.org/wiki/Cap_set">cap set problem</a></strong>, which involves finding the largest set of points in a high-dimensional grid where no three points lie on a line. This longstanding open problem in extremal combinatorics, once described by renowned mathematician Terence Tao as his <a href="https://terrytao.wordpress.com/2007/02/23/open-question-best-bounds-for-cap-sets/">favorite open question</a>, was solved by FunSearch, in collaboration with <a href="https://people.math.wisc.edu/~ellenberg/">Prof. Jordan Ellenberg</a>. This marked the first time an LLM made a new discovery for such a challenging scientific problem, outperforming state-of-the-art computational solvers.</p> <p><img src="/assets/img/capset.png" alt="Benefits of FunSearch" class="l-body rounded z-depth-1 center" width="30%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Illustration of the cap set problem. The circles are the elements of $\mathbb{Z}_3^2$ with the ones belonging to the cap set shown in blue. The possible lines in $\mathbb{Z}_3^2$ are also shown (with colours indicating lines that wrap around in arithmetic modulo 3). No three elements of the cap set are in a line.</p> </div> <p>A significant advantage of FunSearch is that it does not just provide solutions. It generates programs that describe <em>how</em> these solutions are constructed. FunSearch also favors highly compact, concise programs, making them easier for researchers to comprehend and learn from.</p> <blockquote> <p>“The solutions generated by FunSearch are <strong>far conceptually richer</strong> than a mere list of numbers. When I study them, I learn something.” — Jordan Ellenberg, Professor of Mathematics at the University of Wisconsin–Madison</p> </blockquote> <p><img src="/assets/img/funsearch_code.png" alt="Code generated by FunSearch" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> Code generated by FunSearch for the cap set problem.</p> </div> <p>The success of FunSearch <d-cite key="romera2024mathematical"></d-cite> underscores that LLMs, when carefully guided and their outputs rigorously verified, can be powerful engines for scientific discovery.</p> <h2 id="alphaevolve">AlphaEvolve</h2> <p>More recently, in May 2025, Google DeepMind announced <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/">AlphaEvolve</a>, an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization<d-cite key="deepmind2025alphaevolve"></d-cite>. This development builds upon the success of systems like FunSearch and represents a significant step towards leveraging AI for complex problem-solving across various domains. Unlike FunSearch, which focuses on discovering single functions, AlphaEvolve is designed to evolve entire codebases and develop much more intricate algorithms.</p> <h3 id="how-alphaevolve-works">How AlphaEvolve works</h3> <p><img src="/assets/img/alphaevolve.png" alt="Overview of AlphaEvolve" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> The AlphaEvolve process. A prompt sampler assembles prompts for the LLMs, which generate new programs. These are then evaluated and stored in a programs database, which uses an evolutionary algorithm to select programs for future prompts.</p> </div> <p>AlphaEvolve uses an evolutionary approach with four key components (see Figure 4):</p> <ol> <li> <p><strong>Prompt sampler:</strong> The prompt contains rich context based on previously discovered solutions, along with instructions for proposing changes to particular solutions.</p> </li> <li> <p><strong>LLM ensemble:</strong> Unlike FunSearch that uses a single LLM, AlphaEvolve uses an ensemble approach combining <a href="https://deepmind.google/discover/blog/gemini-flash-a-new-generation-of-large-language-models-with-fast-inference-and-high-quality-outputs/">Gemini Flash</a> and <a href="https://deepmind.google/discover/blog/gemini-pro-a-new-generation-of-large-language-models-with-high-quality-outputs/">Gemini Pro</a>. The lightweight Gemini Flash enables higher rates of candidate generation through lower latency, while the more powerful Gemini Pro provides deeper insights and higher-quality suggestions that can significantly advance the evolutionary search and potentially lead to breakthroughs.</p> </li> <li> <p><strong>Evaluator pool:</strong> This component verifies, runs, and scores proposed solutions using automated evaluation metrics that provide objective assessments of each solution’s accuracy and quality.</p> </li> <li> <p><strong>Program database:</strong> AlphaEvolve uses an evolutionary database inspired by a combination of the MAP elites algorithm<d-cite key="mouret2015illuminating"></d-cite> and island-based population models<d-cite key="tanese1989distributed"></d-cite> to continuously improve upon the best solutions while maintaining diversity to encourage exploration.</p> </li> </ol> <p>Unlike traditional genetic algorithms with explicit mutation and crossover operations, AlphaEvolve uses LLMs as sophisticated genetic operators to generate code modifications based on context from past solutions. Mutation occurs when the LLM ensemble suggests code changes (e.g., rewrites or targeted diffs), while crossover is implicit as the LLM receives multiple parent solutions as inspiration. This approach makes AlphaEvolve particularly effective in domains where progress can be clearly and systematically measured, like mathematics and computer science.</p> <h3 id="benefits-of-alphaevolve">Benefits of AlphaEvolve</h3> <p>AlphaEvolve has already demonstrated significant real-world impact across multiple domains:</p> <ol> <li> <p><strong>Improving data center scheduling:</strong> AlphaEvolve discovered a simple yet highly effective heuristic to help <a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/">Borg</a>, Google’s cluster management system, orchestrate its vast data centers more efficiently. This solution, which has been in production for over a year, continuously recovers, on average, 0.7% of Google’s worldwide compute resources. This sustained efficiency gain allows more tasks to be completed on the same computational footprint. A key benefit is that AlphaEvolve’s solution is human-readable code, offering interpretability, debuggability, predictability, and ease of deployment.</p> </li> <li> <p><strong>Hardware design optimization:</strong> AlphaEvolve proposed a <a href="https://en.wikipedia.org/wiki/Verilog">Verilog</a> rewrite that removed unnecessary bits in a key, highly optimized arithmetic circuit for matrix multiplication. The proposal passed robust verification methods to confirm functional correctness and was integrated into an upcoming Tensor Processing Unit (TPU). By suggesting modifications in the standard language of chip designers, AlphaEvolve promotes collaboration between AI and hardware engineers to accelerate specialized chip design.</p> </li> <li> <p><strong>Enhancing AI training and inference:</strong> AlphaEvolve found more efficient ways to divide large matrix multiplication operations into manageable subproblems, achieving a 23% speedup in Gemini’s architecture’s vital <a href="https://docs.jax.dev/en/latest/pallas/index.html">kernel</a>, resulting in a 1% reduction in overall training time. In the realm of low-level GPU optimization, AlphaEvolve demonstrated remarkable efficiency by achieving up to a 32.5% speedup for the <a href="https://arxiv.org/abs/2205.14135">FlashAttention</a> kernel implementation in Transformer-based AI models.</p> </li> </ol> <p><img src="/assets/img/alphaevolve_applications.png" alt="Overview of AlphaEvolve" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> How AlphaEvolve helps Google deliver a more efficient digital ecosystem, from data center scheduling and hardware design to AI model training.</p> </div> <p>Beyond these applications, AlphaEvolve made a groundbreaking contribution by discovering an algorithm for multiplying 4x4 complex-valued matrices using just 48 scalar multiplications, surpassing the efficiency of <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen’s 1969 algorithm</a>. When applied to a diverse set of over 50 open problems spanning mathematical analysis, geometry, combinatorics, and number theory, AlphaEvolve demonstrated remarkable versatility: it successfully rediscovered state-of-the-art solutions in 75% of cases and improved upon previously best-known solutions in 20% of cases. One of its most notable achievements was advancing the <a href="https://plus.maths.org/content/newton-and-kissing-problem">300-year-old kissing number problem</a>, where it discovered a configuration of 593 outer spheres and established a new lower bound in 11 dimensions, showcasing its ability to tackle complex geometric challenges.</p> <p><img src="/assets/img/alphaevolve_math.png" alt="Overview of AlphaEvolve" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 7.</strong> Examples of ground-breaking mathematical contributions discovered with AlphaEvolve.</p> </div> <aside class="l-body box-note"> <p>Looking ahead, AlphaEvolve is expected to continue improving alongside the capabilities of large language models, especially as they become more proficient at coding. Google DeepMind is also planning an <a href="https://docs.google.com/forms/d/e/1FAIpQLSfaLUgKtUOJWdQtyLNAYb3KAkABAlKDmZoIqPbHtwmy3YXlCg/viewform">Early Access Program</a> for selected academic users and exploring possibilities to make AlphaEvolve more broadly available. While currently focused on math and computing, its general nature means it could potentially transform many other areas such as material science, drug discovery, sustainability, and broader applications.</p> </aside> <h2 id="funsearch-vs-alphaevolve">FunSearch vs AlphaEvolve</h2> <p>While both FunSearch and AlphaEvolve leverage LLM within an evolutionary framework, AlphaEvolve offers a substantial improvement over its predecessor, both in terms of scale and generality. Here’s a detailed comparison of their capabilities:</p> <table> <thead> <tr> <th>Capability</th> <th>FunSearch</th> <th>AlphaEvolve</th> </tr> </thead> <tbody> <tr> <td>Code Scope</td> <td>Evolves a single function</td> <td>Evolves an entire codebase</td> </tr> <tr> <td>Code Size</td> <td>Evolves up to 10-20 lines of code</td> <td>Evolves up to hundreds of lines of code</td> </tr> <tr> <td>Language Support</td> <td>Python only</td> <td>Any programming language</td> </tr> <tr> <td>Computation</td> <td>Needs fast evaluation (≤ 20min on 1 CPU)</td> <td>Can evaluate for hours, in parallel, on accelerators</td> </tr> <tr> <td>LLM Usage</td> <td>Millions of LLM samples used</td> <td>Thousands of LLM samples suffice</td> </tr> <tr> <td>Model Scale</td> <td>Small LLMs used, no benefit from using larger models</td> <td>Benefits from using state-of-the-art LLMs</td> </tr> <tr> <td>Context Handling</td> <td>Minimal context (only previous solutions)</td> <td>Rich context and feedback in prompts</td> </tr> <tr> <td>Optimization</td> <td>Optimizes a single metric</td> <td>Can simultaneously optimize multiple metrics</td> </tr> </tbody> </table> <aside class="l-body box-im"> <p>The evolution from FunSearch to AlphaEvolve demonstrates significant advances in <strong>scale</strong> and <strong>generality</strong>. While FunSearch was groundbreaking in showing how LLMs could aid mathematical discovery, AlphaEvolve extends this approach to tackle more complex, real-world problems across multiple domains. This progression also reflects the rapid advancement in LLM capabilities, where newer state-of-the-art models can generate more sophisticated and accurate code with fewer samples.</p> </aside> <h2 id="takeaways">Takeaways</h2> <p>The development of FunSearch<d-cite key="romera2024mathematical"></d-cite> and AlphaEvolve<d-cite key="deepmind2025alphaevolve"></d-cite> marks an exciting advancement in the application of LLMs. Overall, these systems demonstrate LLMs are moving beyond text generation and coding assistance to become tools for genuine discovery and sophisticated optimization in mathematics, computer science, and engineering. Combining LLM creativity with rigorous, automated evaluation within an evolutionary framework is a powerful and promising strategy for tackling complex, real-world problems by evolving entire codebases. While the journey is still ongoing, the prospect of LLMs significantly augmenting, or even leading in some cases, algorithmic and mathematical discovery is becoming increasingly tangible.</p>]]></content><author><name></name></author><category term="large-language-models"/><category term="agents"/><summary type="html"><![CDATA[A review of FunSearch and AlphaEvolve]]></summary></entry><entry><title type="html">Learning with a Goal</title><link href="https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal/" rel="alternate" type="text/html" title="Learning with a Goal"/><published>2024-09-15T17:00:00+00:00</published><updated>2024-09-15T17:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal/"><![CDATA[<p>Traditionally, Bayesian optimization (BO) has been perceived as a technique for optimizing expensive objective functions through efficient data sampling, while active learning (AL) is often seen as a way to selectively query data to improve model performance. Recently, Fiore et al. (2024)<d-cite key="di2024active"></d-cite> proposed a unified perspective of BO and AL, arguing that both can be viewed as adaptive sampling schemes guided by common learning principles toward a given optimization goal. In this post, we will explore the key ideas presented in the paper and discuss the implications of this unified perspective.</p> <h2 id="goal-driven-learning">Goal-driven learning</h2> <p>Goal-driven learning<d-cite key="bui2007goal"></d-cite> can described as:</p> <aside class="l-body box-note"> <p>a decision-making process in which each decision is made to acquire specific information about the system of interest that contributes the most to achieve a given a goal.</p> </aside> <p>BO and AL can be regarded as goal-driven procedures, where a surrogate model is built to capture the behavior of a system or effectively inform an optimization procedure to minimize the given objective. This goal-driven process seeks to determine the “best” location of the domain to acquire information about the system, and refine the surrogate model towards the goal. Mathematically, it can be formulated as:</p> <p>\[ x^* = \arg \min_{x \in \mathcal{X}} f(R(x)) \]</p> <p>where \(f\) is the objective and \(R(x)\) is the response of the system. Here, \(f\) may represent the error between the surrogate approximation and the response, such that the goal is to minimize the error to improve the accuracy of the surrogate. Alternatively, \(f\) may also represents a performance indicator based on the response, so the goal is to minimize this indicator to improve the system’s performance.</p> <h2 id="adaptive-sampling">Adaptive sampling</h2> <p>BO and AL use adaptive sampling schemes to efficiently accomplish a given goal while adapting to the previously collected information:</p> <ul> <li>In BO, the goal is to minimize the objective function by iteratively selecting the next location to sample based on the surrogate model. The surrogate model is updated after each sample to refine the approximation and guide the next decision.</li> <li>In AL, the goal is to minimize the generalization error of a model by iteratively selecting the next sample to improve the model’s performance. The model is updated after each sample to refine the approximation and guide the next decision.</li> </ul> <p>We can further classify adaptive sampling schemes into three main categories:</p> <table> <thead> <tr> <th>Category</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Adaptive Probing</td> <td>Methods that do not rely on a surrogate model and directly probe the system to gather information in a sequential manner.</td> </tr> <tr> <td>Adaptive Modeling</td> <td>Methods that compute a surrogate model independently, without using it to inform the sampling process.</td> </tr> <tr> <td>Adaptive Learning</td> <td>Methods that use information from the surrogate model to make decisions and update the model based on those decisions.</td> </tr> </tbody> </table> <p>The key distinction between these categories lies in the concept of <em>goal-driven learning</em>, which is the distinctive element of the adaptive learning class. In this paradigm, the learner actively gathers information from the surrogate model to guide its decisions towards the desired goal, and the surrogate model is consequently improved by the outcome of these decisions.</p> <p>In contrast, the adaptive probing and adaptive modeling classes do not exhibit this goal-driven learning characteristic. Adaptive probing methods operate without the aid of a surrogate, while adaptive modeling approaches compute a surrogate that is not directly used to inform the sampling process.</p> <p><img src="/assets/img/adaptive_sampling.png" alt="transformer" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> Classification of adaptive sampling techniques: Where adaptive sampling and active learning meet.</p> </div> <h2 id="learning-and-infill-criteria">Learning and infill criteria</h2> <p>The strong synergy between AL and BO is rooted in the substantial analogy between the learning criteria that drive the AL procedure and the infill criteria that characterize the BO scheme.</p> <h3 id="learning-criteria">Learning criteria</h3> <p>Learning criteria establish a metric for quantifying the gains of all the possible learner decisions, and prescribe an optimal decision based in information acquired from the surrogate model. In AL, there are 3 essential learning criteria<d-cite key="he2014active"></d-cite>:</p> <ol> <li><strong>Informativeness:</strong> The sampling policy is driven by the goal of acquiring the <em>most informative samples</em>, i.e., the ones that are expected to contribute the maximum information.</li> <li><strong>Representativeness:</strong> The sampling policy aims to select <em>samples that are representative</em> of the target domain, exploiting the structure of the problem to direct queries to locations</li> <li><strong>Diversity:</strong> The sampling policy seeks to select <em>samples that are diverse</em>, i.e., well-spread across the domain, preventing the concentration of queries in small local regions.</li> </ol> <p><img src="/assets/img/learning_criteria.png" alt="transformer" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Illustration of the three learning criteria in watering optimization problem: (a) informativeness, (b) representativeness, and (c) diversity.</p> </div> <h3 id="infill-criteria">Infill criteria</h3> <p>On the other hand, the infill criteria in BO provides a measure of the information gain that would result from sampling at a particular location. The most common infill criteria are<d-cite key="garnett2023bayesian"></d-cite>:</p> <ol> <li><strong>Global exploration:</strong> This criterion focuses on choosing samples in regions of <em>high predictive uncertainty</em>, enhancing global awareness of the search space. However, this approach may not direct resources optimally towards the specific goal.</li> <li><strong>Local exploitation:</strong> This criterion prioritizes choosing samples in regions with <em>high predictive mean</em>, focusing the search on promising areas. Yet, it may result in less accurate knowledge of the overall objective function distribution.</li> </ol> <aside class="l-body box-important"> <p>Overall, we can observe a strong correspondence between the learning criteria in AL and the infill criteria in BO:</p> <ul> <li><strong>The “informativeness” criterion in AL aligns with the “local exploitation” criterion in BO</strong>, as both aim to maximize the information gain.</li> <li>Similarly, <strong>the “representativeness” and “diversity” criteria in AL correspond to the “global exploration” criterion in BO</strong>, as they seek to ensure that the sampling process is well-distributed across the domain.</li> </ul> </aside> <h2 id="takeaways">Takeaways</h2> <p>BO and AL have traditionally been viewed as distinct fields with separate goals and methodologies. However, this paper provides a unified perspective that highlights the shared principles underlying both fields. By recognizing the synergy between BO and AL, we can leverage the strengths of each field to develop more powerful and efficient learning algorithms.</p>]]></content><author><name></name></author><category term="bayesian-optimization"/><category term="active-learning"/><summary type="html"><![CDATA[A unified perspective of Bayesian optimization and active learning]]></summary></entry></feed>