<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://richardcsuwandi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://richardcsuwandi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-11T15:38:55+00:00</updated><id>https://richardcsuwandi.github.io/feed.xml</id><title type="html">Richard Cornelius Suwandi</title><subtitle></subtitle><entry><title type="html">No world model, no general AI</title><link href="https://richardcsuwandi.github.io/blog/2025/agents-world-models/" rel="alternate" type="text/html" title="No world model, no general AI"/><published>2025-06-11T03:00:00+00:00</published><updated>2025-06-11T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/agents-world-models</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/agents-world-models/"><![CDATA[<p>Imagine if we could build an AI that thinks and plans like a human. Recent breakthroughs in large language models (LLMs) have brought us closer to this goal. As these models grow larger and trained on more data, they develop the so-called <em>emergent abilities</em><d-cite key="wei2022emergent"></d-cite> that significantly improve their performance on a wide range of downstream tasks. This has sparked a new wave of research into creating general AI agents that can tackle complex, long-horizon tasks in the real world environments<d-cite key="yao2023react"></d-cite><d-cite key="hao2023reasoning"></d-cite>. But here is the fascinating part: humans do not just react to what they see, we build rich mental models<d-cite key="johnson1983mental"></d-cite> of how the world works. These <a href="https://worldmodels.github.io">world models</a><d-cite key="ha2018world"></d-cite> help us set ambitious goals<d-cite key="locke2013goal"></d-cite> and make thoughtful plans<d-cite key="bratman1987intention"></d-cite>. Hence, based on this observation, it is natural to ask:</p> <blockquote> <p>‚ÄúIs learning a world model useful to achieve a human-level AI?‚Äù</p> </blockquote> <p>Recently, researchers at Google DeepMind showed that learning a world model is not only beneficial, but also <em>necessary</em> for general agents<d-cite key="richens2025generalagentsneedworld"></d-cite>. In this post, we will discuss the key findings from the paper and the implications for the future of AI agents.</p> <h2 id="do-we-need-a-world-model">Do we need a world model?</h2> <p>In 1991, <a href="https://en.wikipedia.org/wiki/Rodney_Brooks">Rodney Brooks</a> made a famous claim that ‚Äúthe world is its own best model‚Äù<d-cite key="brooks1991intelligence"></d-cite>.</p> <p><img src="/assets/img/brooks_intelligence.png" alt="Intelligence without representation" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> In <em>Intelligence without representation</em>, Rodney Brooks famously proposed that ‚Äúthe world is its own best model‚Äù.</p> </div> <p>He argued that intelligent behavior could emerge naturally from model-free agents simply by interacting with their environment through a cycle of actions and perceptions, without needing to build explicit representations of how the world works. Brooks‚Äô argument has been strongly supported by the remarkable success of <em>model-free</em> agents, which have demonstrated impressive generalization capabilities across diverse tasks and environments<d-cite key="reed2022generalist"></d-cite><d-cite key="driess2023palm"></d-cite><d-cite key="raad2024scaling"></d-cite>. This model-free approach offers an appealing path to creating general AI agents while avoiding the complexities of learning explicit world models. However, recent works suggest an intriguing possibility: even these supposedly model-free agents might be learning <em>implicit</em> world models<d-cite key="li2023emergent"></d-cite> and planning algorithms<d-cite key="hou2023towards"></d-cite> beneath the surface.</p> <h3 id="ilya-was-right-all-along">Ilya was right all along?</h3> <p>Looking back to March 2023, <a href="https://en.wikipedia.org/wiki/Ilya_Sutskever">Ilya Sutskever</a> made a profound claim that large neural networks are doing far more than just next-word prediction and are actually <a href="https://www.youtube.com/watch?v=I6qQinoY9WM">learning ‚Äúworld models‚Äù</a>. The way he put it,</p> <div class="center"> <blockquote class="twitter-tweet" data-media-max-width="560"><p lang="en" dir="ltr">In March 2023, <a href="https://twitter.com/ilyasut?ref_src=twsrc%5Etfw">@ilyasut</a> made a profound claim about LLMs learning &quot;world models&quot;. Now, researchers at <a href="https://twitter.com/GoogleDeepMind?ref_src=twsrc%5Etfw">@GoogleDeepMind</a> have proven he glimpsed something even deeper; a fundamental law governing ALL intelligent agents. üßµ <a href="https://t.co/MsMx8snUZs">pic.twitter.com/MsMx8snUZs</a></p>&mdash; ohqay (@ohqayy) <a href="https://twitter.com/ohqayy/status/1930418880696201317?ref_src=twsrc%5Etfw">June 5, 2025</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>He believed that what neural networks learn are not just textual information, but rather a compressed representation of our world. Thus, the more accurately we can predict the next word, the higher fidelity of the world model we can achieve.</p> <h2 id="agents-and-world-models">Agents and world models</h2> <p>While Ilya‚Äôs claim was intriguing, it was not clear how to formalize it at that time. But now, <a href="https://x.com/jonathanrichens/status/1930221408199516657">researchers at Google DeepMind</a> have proven that what Ilya said is not just a hypothesis, but a fundamental law governing <em>all</em> general agents<d-cite key="richens2025generalagentsneedworld"></d-cite>. In the paper, the authors showed that,</p> <blockquote> <p>‚ÄúAny agent capable of generalizing to a broad range of simple goal-directed tasks must have learned a predictive model capable of simulating its environment, and this model can always be recovered from the agent.‚Äù</p> </blockquote> <p><img src="/assets/img/agents_learn_world_models.jpeg" alt="Agents learn world models" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Any agent satisfying a regret bound must have learned an environment transition function, which can be extracted from its goal-conditional policy. This holds true for agents that can handle basic tasks like reaching specific states.</p> </div> <ol> <li><em>There is no ‚Äúmodel-free shortcut‚Äù to building general AI agents</em>. If we want agents that generalize to diverse tasks, we cannot avoid learning world models.</li> <li><em>Better performance requires better world models</em>. The only path to lower regret or handling more complex goals is through learning increasingly accurate world models.</li> </ol> <aside class="l-body box-note" title="Note"> <p>The above only holds true for agents that plan over multi-step horizons, as they need to understand how actions affect future states. However, ‚Äúmyopic‚Äù agents that only consider immediate rewards can potentially avoid learning world models since they do not need to predict long-term consequences.</p> </aside> <p>To make the above claims more precise, the authors develop a rigorous mathematical framework built on 4 key components: environments, goals, agents, and world models.</p> <h3 id="environments">Environments</h3> <p>The environment is assumed to be a controlled Markov process (cMP)<d-cite key="sutton1998reinforcement"></d-cite>, which is essentially a Markov decision process without a specified reward function. A cMP consists of a state space \(\boldsymbol{S}\), an action space \(\boldsymbol{A}\), and a transition function \(P_{ss'}(a) = P(S_{t+1} = s' \mid A_t = a, S_t = s)\). The authors assume the environment is irreducible<d-footnote>A Markov process is irreducible if every state is reachable from every other state.</d-footnote> and stationary<d-footnote>A Markov process is stationary if the transition probabilities do not change over time.</d-footnote>.</p> <h3 id="goals">Goals</h3> <p>Rather than defining complex goal structures, the paper focused on simple, intuitive goals expressed in Linear Temporal Logic (LTL)<d-cite key="pnueli1977temporal"></d-cite><d-cite key="baier2008principles"></d-cite>. A goal \(\varphi\) has the form \(\varphi = \mathcal{O}([(s,a) \in \boldsymbol{g}])\) where \(\boldsymbol{g}\) is a set of goal states and \(\mathcal{O} \in \{\bigcirc, \diamond, \top\}\) specifies the time horizon (\(\bigcirc\) = next, \(\diamond\) = eventually, \(\top\) = now). More complex composite goals \(\psi\) can be formed by combining sequential goals in ordered sequences: \(\psi = \langle\varphi_1, \varphi_2, \ldots, \varphi_n\rangle\) where the agent must achieve each sub-goal in order. The depth of a goal as the number of sub-goals: \(\text{depth}(\psi) = n\).</p> <h3 id="agents">Agents</h3> <p>The authors focused on <em>goal-conditioned agents</em><d-cite key="liu2022goal"></d-cite>, which are defined as a policy \(\pi(a_t \mid h_t; \psi)\) that maps a history \(h_t\) to an action \(a_t\) conditioned on a goal \(\psi\). This leads to a natural definition of an optimal goal-conditioned agent for a given environment and set of goals \(\boldsymbol{\Psi}\), which is a policy that maximizes the probability that \(\psi\) is achieved, for all \(\psi \in \boldsymbol{\Psi}\). However, real agents are rarely optimal, especially when operating in complex environments and for tasks that require coordinating many sub-goals over long time horizons. Instead of requiring perfect optimality, the authors define a <em>bounded</em> agent that is capable of achieving goals of some maximum goal depth with a failure rate that is bounded relative to the optimal agent. A bounded goal-conditioned agent \(\pi(a_t \mid h_t; \psi)\) satisfies:</p> \[P(\tau \models \psi \mid \pi, s_0) \geq \max_{\pi'} P(\tau \models \psi \mid \pi', s_0)(1-\delta)\] <p>for all goals \(\psi \in \boldsymbol{\Psi}_n\), where \(\boldsymbol{\Psi}_n\) is the set of all composite goals with depth at most \(n\) and \(\delta \in [0,1]\) is the failure rate parameter.</p> <h3 id="world-models">World Models</h3> <p>The authors considered the predictive world models, which can be used by agents to plan. They defined a world model as any approximation \(\hat{P}_{ss'}(a)\) of the transition function of the environment \(P_{ss'}(a) = P(S_{t+1} = s' \mid A_t = a, S_t = s)\), with bounded error \(\left|\hat{P}_{ss'}(a) - P_{ss'}(a)\right| \leq \varepsilon\). The authors showed that, for any such bounded goal-conditioned agent, an approximation of the environment‚Äôs transition function (a world model) can be recovered from the agent‚Äôs policy alone:</p> <div class="box-important" title="Theorem"> <p>Let \(\pi\) be a goal-conditioned agent with maximum failure rate \(\delta\) for all goals \(\psi \in \boldsymbol{\Psi}_n\) where \(n &gt; 1\). Then \(\pi\) fully determines a model \(\hat{P}_{ss'}(a)\) for the environment transition probabilities with bounded error:</p> \[\left|\hat{P}_{ss'}(a) - P_{ss'}(a)\right| \leq \sqrt{\frac{2P_{ss'}(a)(1-P_{ss'}(a))}{(n-1)(1-\delta)}}\] <p>For \(\delta \ll 1\) and \(n \gg 1\), the error scales as \(\mathcal{O}(\delta/\sqrt{n}) + \mathcal{O}(1/n)\).</p> </div> <p>The above result reveals two crucial insights:</p> <ol> <li>As agents become more competent (\(\delta \to 0\)), the recoverable world model becomes more accurate.</li> <li>As agents handle longer-horizon goals (larger \(n\)), they must learn increasingly precise world models.</li> </ol> <p>It also implies that learning a sufficiently general goal-conditioned policy is <em>informationally equivalent</em> to learning an accurate world model.</p> <h2 id="how-to-recover-the-world-model">How to recover the world model?</h2> <p>The authors also derived an algorithm to recover the world model from a bounded agent. The algorithm works by querying the agent with carefully designed composite goals that correspond to ‚Äúeither-or‚Äù decisions. For instance, it presents goals like ‚Äúachieve transition \((s,a) \to s'\) at most \(r\) times out of \(n\) attempts‚Äù versus ‚Äúachieve it more than \(r\) times‚Äù. The agent‚Äôs choice of action reveals information about which outcome has higher probability, allowing us to estimate \(P_{ss'}(a)\).</p> <p><img src="/assets/img/recovering_world_models.png" alt="Algorithm for recovering world models" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> The derived algorithm for recovering a world model from a bounded agent.</p> </div> <h2 id="experiments">Experiments</h2> <p>To test the effectiveness of the algorithm, the authors conducted experiments on a randomly generated controlled Markov process with 20 states and 5 actions, featuring a sparse transition function to make learning more challenging. They trained agents using trajectories sampled from the environment under a random policy, increasing agent competency by extending the training trajectory length ($N_{\text{samples}}$). The results show that:</p> <ul> <li>Even when agents strongly violated the theoretical assumptions (achieving worst-case regret $\delta = 1$ for some goals), their algorithm still recovered accurate world models.</li> <li>The average error in recovered world models decreased as $\mathcal{O}(n^{-1/2})$, matching the theoretical scaling relationship between error bounds and goal depth.</li> <li>As agents learned to handle longer-horizon goals (larger maximum depth $n$), the extracted world models became increasingly accurate. This confirms the fundamental link between agent capabilities and world model quality.</li> </ul> <p><img src="/assets/img/world_model_error.png" alt="Error in recovered world models" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> a) Mean error in recovered world model decreases as agent handles deeper goals. b) Mean error scales with agent‚Äôs regret at depth 50. Error bars show 95% confidence intervals over 10 experiments.</p> </div> <h3 id="connection-to-related-works">Connection to related works</h3> <p>The results of this work complement several other areas of AI research:</p> <ul> <li>The proposed algorithm completes a the ‚Äútriangle‚Äù between environment, goal, and policy. Planning determines an optimal policy given a world model and goal (world model + goal ‚Üí policy), while inverse reinforcement learning (IRL)<d-cite key="ng2000algorithms"></d-cite> recovers goals given a world model and policy (world model + policy ‚Üí goal). The proposed algorithm fills in the remaining direction by recovering the world model given the agent‚Äôs policy and goal (policy + goal ‚Üí world model). Just as IRL requires observing policies across multiple environments to fully determine goals<d-cite key="amin2016towards"></d-cite>, the algorithm needs to observe the agent‚Äôs behavior across multiple goals to fully recover the world model.</li> </ul> <p><img src="/assets/img/triangle_planning.jpeg" alt="The triangle of environment, goal, and policy" class="l-body rounded z-depth-1 center" width="60%"/></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> While planning uses a world model and a goal to determine a policy, and IRL and inverse planning use an agent‚Äôs policy and a world model to identify its goal, the proposed algorithm uses an agent‚Äôs policy and its goal to identify a world model.</p> </div> <ul> <li> <p>Traditional mechanistic interpretability (MI) typically relies on analyzing neural network activations<d-cite key="li2023emergent"></d-cite> or using supervised probes<d-cite key="alain2016understanding"></d-cite>, on the other hand, the proposed algorithm provides a novel approach that extracts world models directly from an agent‚Äôs policy behavior, making it applicable even when model internals are inaccessible. This unsupervised and architecture-agnostic approach works for any agent that satisfies the bounded regret condition, regardless of its specific implementation. For LLMs, this means we can potentially uncover their implicit world models by analyzing their goal-directed behavior, without needing to access their internal representations.</p> </li> <li> <p>A recent work<d-cite key="richens2024robust"></d-cite> showed that agents adapting to distributional shifts must learn causal world models. This work complements it by focusing on task generalization rather than domain generalization. Interestingly, domain generalization requires deeper causal understanding than task generalization. For example, in a system where state variables $X$ and $Y$ are causally related ($X \to Y$), an agent can achieve optimal task performance by learning just the transition probabilities, without needing to understand the underlying causal relationship. This suggests an agential version of Pearl‚Äôs causal hierarchy<d-cite key="bareinboim2022pearl"></d-cite>, where different agent capabilities (like domain or task generalization) require different levels of causal knowledge.</p> </li> </ul> <aside class="l-body box-warning" title="Remark"> <p>The findings also have significant implications for AI development and safety. The emergence of new capabilities in large language models and other AI systems could be explained by implicit world models learned while optimizing for diverse training tasks. The ability to extract world models from capable agents provides a new tool for verification and alignment, as model fidelity scales with agent capability. However, the inherent difficulty of learning accurate world models of complex real-world systems also places fundamental limits on general agent capabilities.</p> </aside> <h2 id="takeaways">Takeaways</h2> <p>Perhaps Ilya‚Äôs 2023 prediction was more prophetic than we realized. If the above results hold true, then the current race toward artificial superintelligence (ASI) through scaling language models might be secretly a race toward building more sophisticated world models. It is also possible that we may be witnessing something even more profound: the transition from what David Silver and Richard Sutton call the <a href="https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf">‚ÄúEra of Human Data‚Äù to the ‚ÄúEra of Experience‚Äù</a>. While current AI systems have achieved remarkable capabilities by imitating human-generated data, Silver and Sutton argue that superhuman intelligence will emerge through agents learning predominantly from their own experience<d-cite key="silver2025welcome"></d-cite>. For example, with recent developments in foundation world models like <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a>, we can generate endless 3D environments from single images<d-cite key="parker2024genie"></d-cite> and allow agents to inhabit ‚Äústreams of experience‚Äù in richly grounded environments that adapt and evolve with their capabilities.</p> <p><img src="/assets/img/genie2.mp4" alt="Genie 2" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> Genie 2, a foundation world model capable of generating an endless variety of action-controllable, playable 3D environments for training and evaluating embodied agents. Based on a single prompt image, it can be played by a human or AI agent using keyboard and mouse inputs.</p> </div> <p>If general agents must learn world models, and superhuman intelligence requires learning from experience rather than human data, then foundation world models like Genie 2 might be the ultimate scaling law for the Era of Experience. Rather than hitting the ceiling of human knowledge, we are entering a phase where the quality of AI agents is fundamentally limited by the fidelity of the worlds they can simulate and explore. The agent that can dream the most accurate dreams, and learn the most from those dreams, might just be the most intelligent.</p>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="AGENTS"/><category term="REINFORCEMENT LEARNING"/><summary type="html"><![CDATA[From Ilya's prediction to Google DeepMind's proof.]]></summary></entry><entry><title type="html">AI that can improve itself</title><link href="https://richardcsuwandi.github.io/blog/2025/dgm/" rel="alternate" type="text/html" title="AI that can improve itself"/><published>2025-06-01T03:00:00+00:00</published><updated>2025-06-01T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/dgm</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/dgm/"><![CDATA[<p>Most AI systems today are stuck in a ‚Äúcage‚Äù designed by humans. They rely on fixed architectures crafted by engineers and lack the ability to evolve autonomously over time. This is the <a href="https://en.wikipedia.org/wiki/Achilles%27_heel">Achilles heel</a> of modern AI ‚Äî like a car, no matter how well the engine is tuned and how skilled the driver is, it cannot change its body structure or engine type to adapt to a new track on its own. But what if AI could learn and improve its own capabilities without human intervention? In this post, we will dive into the concept of self-improving systems and a recent effort towards building one.</p> <h2 id="learning-to-learn">Learning to learn</h2> <p>The idea of building systems that can improve themselves brings us to the concept of <a href="https://people.idsia.ch/~juergen/metalearning.html">meta-learning</a>, or ‚Äúlearning to learn‚Äù <d-cite key="thrun1998learning"></d-cite>, which aims to create systems that not only solve problems but also evolve their problem-solving strategies over time. One of the most ambitious efforts in this direction is the G√∂del Machine<d-cite key="schmidhuber2003godel"></d-cite>, proposed by J√ºrgen Schmidhuber decades ago and was named after the famous mathematician <a href="https://en.wikipedia.org/wiki/Kurt_G√∂del">Kurt G√∂del</a>. A G√∂del Machine is a hypothetical self-improving AI system that optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy. It represents the ultimate form of self-awareness in AI, an agent that can reason about its own limitations and modify itself accordingly.</p> <p><img src="/assets/img/godel.jpg" alt="Overview of a G√∂del machine" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> G√∂del machine is a hypothetical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy.</p> </div> <p>While this idea is interesting, formally proving whether a code modification of a complex AI system is <em>absolutely beneficial</em> is almost an impossible task without restrictive assumptions. This part stems from the inherent difficulty revealed by the <a href="https://en.wikipedia.org/wiki/Halting_problem">Halting Problem</a> and <a href="https://en.wikipedia.org/wiki/Rice%27s_theorem">Rice‚Äôs Theorem</a> in computational theory, and is also related to the inherent limitations of the logical system implied by <a href="https://en.wikipedia.org/wiki/G√∂del%27s_incompleteness_theorems">G√∂del‚Äôs incompleteness theorem</a>. These theoretical constraints make it nearly impossible to predict the complete impact of code changes without making restrictive assumptions. To illustrate this, consider a simple analogy: just as you cannot guarantee that a new software update will improve your computer‚Äôs performance without actually running it, an AI system faces an even greater challenge in predicting the long-term consequences of modifying its own complex codebase.</p> <h2 id="darwin-g√∂del-machine">Darwin-G√∂del Machine</h2> <p>To ‚Äúrelax‚Äù the requirement of formal proof, a recent work by proposed the <strong>Darwin-G√∂del Machine (DGM)</strong><d-cite key="zhang2025darwingodelmachineopenended"></d-cite>, which combines the Darwinian evolution and G√∂delian self-improvement. Essentially, DGM abandoned the pursuit of a rigorous mathematical proof and embraced a more pragmatic way that is closer to the essence of life evolution through empirical validation. As the authors put it,</p> <blockquote> <p>We do not require formal proof, but empirical verification of self-modification based on benchmark testing, so that the system can improve and explore based on the observed results.</p> </blockquote> <p><img src="/assets/img/dgm.png" alt="Overview of the DGM" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> The Darwin-G√∂del Machine (DGM) is a self-improving AI system that optimizes its own problem-solving strategy through a combination of Darwinian evolution and G√∂delian self-improvement.</p> </div> <p>The core idea behind DGM is to let the AI agent iteratively modify its own code<d-footnote>More precisely, the metacode that controls its behavior and ability.</d-footnote>, and then put these modified new versions into the actual test environment (e.g., coding benchmarks) to judge whether this modification is good by observing its performance. If the modification is good, it will be retained and may become the basis for further evolution in the future; if the modification is bad, it will be eliminated.</p> <h3 id="how-dgm-works">How DGM works</h3> <p><img src="/assets/img/dgm-animation.gif" alt="Animation of the DGM process" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> Animation showing how DGM iteratively builds a growing archive of agents by harnessing the principles of open-ended exploration. New agents are created and scored by interleaving self-modification with downstream task evaluation.</p> </div> <p>The general workflow of DGM is as follows:</p> <ol> <li> <p><strong>Initialization:</strong> Evolution begins with one or a few basic coding agents, so-called the ‚Äúseeds‚Äù of intelligence. These initial agents may possess only minimal functionalities, such as executing bash commands or editing entire files. DGM maintains an ‚Äúarchive of agents‚Äù, which functions like a biological ‚Äúgene bank.‚Äù This archive preserves all previously generated agents, regardless of their performance level, ensuring that no potentially valuable variation is permanently lost.</p> </li> <li> <p><strong>Sampling:</strong> DGM selects one or more parent agents from the archive to generate new child agent. The selection mechanism does not focus solely on top performers but instead allows every agent a chance to be chosen. While higher-performing agents are more likely to be selected, DGM also encourages exploration by giving ‚Äúless successful agents‚Äù<d-footnote>Those that might contain novel or unconventional ideas.</d-footnote> an opportunity to contribute to future generations.</p> </li> <li><strong>Reproduction:</strong> Once the parents are selected, DGM performs modifications to generate a new child agent. These changes occur at the source code level and can include: <ul> <li><em>Enhancement of existing tools</em>, for example, upgrading a file-editing tool that replaces entire files into one capable of precise string replacement.</li> <li><em>Addition of new tools or workflows</em>, such as integrating a static code analysis tool or designing a ‚Äúthink‚Äìact‚Äìobserve‚Äù loop.</li> <li><em>Improvement of problem-solving strategies</em>, for instance, refining how the agent prompts the base model or managing long-context code more effectively.</li> <li><em>Introduction of collaboration mechanisms</em>, like having an agent‚Äôs code reviewed by another before execution.</li> </ul> </li> <li> <p><strong>Natural Selection:</strong> The newly generated child agent is not automatically accepted into the ‚Äúelite pool‚Äù but must prove its worth through rigorous testing. Each agent‚Äôs performance, such as the percentage of successfully solved problems, is quantitatively scored and used to select the best agents.</p> </li> <li><strong>Tree Formation:</strong> If a child agent outperforms its parent or meets certain quality thresholds, it is added to the archive and becomes a new node in the evolutionary tree. This process repeats iteratively, creating a growing structure of diverse, high-quality agents. Importantly, DGM enables parallel exploration of many different paths in the search space, promoting open-ended discovery and avoiding premature convergence to local optima.</li> </ol> <p><img src="/assets/img/dgm-conceptual.png" alt="Workflow of DGM" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> DGM iteratively builds a growing intelligent archive by intertwining self-modification with downstream task evaluation. The intelligent body in the archive is selected for self-modification through open exploration.</p> </div> <aside class="l-body box-warning" title="Remark"> <p>Unlike traditional <a href="https://en.wikipedia.org/wiki/Hill_climbing">hill-climbing methods</a> that often get stuck in local optima, DGM employs open-ended exploration. This approach allows even underperforming agents to persist if they contain novel ideas that might lead to breakthroughs in future generations. Even agents with temporarily lower performance can be selected if they contain interesting ‚Äòmutations‚Äô, since their innovative traits may be combined with other beneficial characteristics in subsequent generations to produce unexpectedly strong offspring. This mirrors biological evolution, where diversity and serendipity often drive innovation.</p> </aside> <p>The paper provides a concrete example of the above principle in action. In the SWE-bench experiment, the performance of newly generated agents showed temporary declines in the 4th and 56th iterations. While traditional hill-climbing methods would abandon these seemingly ‚Äúregressive paths‚Äù, DGM‚Äôs open exploration mechanism allows continued exploration. This persistence paid off as by the 70th iteration, the system had created new agents that surpassed all their predecessors, demonstrating the value of maintaining diverse evolutionary paths even when they temporarily appear less promising.</p> <h3 id="can-dgm-really-improve-itself">Can DGM really improve itself?</h3> <p>In the paper, the authors evaluated DGM on two challenging benchmarks:</p> <ol> <li> <p><strong>SWE-bench:</strong> A widely-used benchmark that tests agents‚Äô ability to resolve real-world GitHub issues<d-cite key="jimenez2024swe"></d-cite> . DGM showed impressive progress, improving its performance from 20.0% to 50.0% through self-modification.</p> </li> <li> <p><strong>Polyglot</strong>: A <a href="https://aider.chat/2024/12/21/polyglot.html">multi-language coding benchmark</a> containing coding problems in C++, Go, Java, JavaScript, Python and Rust. DGM achieved even more dramatic improvement, jumping from an initial 14.2% to 30.7% success rate, far surpassing the representative hand-designed agent by <a href="https://aider.chat">Aider</a>.</p> </li> </ol> <p>The impressive performance improvements demonstrate DGM‚Äôs fundamental ability to enhance itself through self-modification. Through ablation studies, the authors identified two key factors driving DGM‚Äôs success. First, self-modification proved crucial, as when disabled (<strong>DGM w/o Self-improve</strong>), the performance declined substantially, showing that the ability to modify its own code is essential for accelerated learning. Second, the open-ended exploration approach was vital, as removing the Darwinian evolution-inspired search mechanism (<strong>DGM w/o Open-ended exploration</strong>), which maintains an archive of diverse solutions and enables parallel exploration of multiple paths, resulted in lower performance.</p> <p><img src="/assets/img/dgm-self-improve.png" alt="Performance of DGM on SWE-bench and Polyglot" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> Self-improvement and open-ended exploration enable the DGM to continue making progress and improve its performance. The DGM automatically discovers increasingly better coding agents and performs better on both SWE-bench (Left) and Polyglot (Right).</p> </div> <h3 id="comparison-with-alphaevolve">Comparison with AlphaEvolve</h3> <p>In parallel, AlphaEvolve<d-cite key="deepmind2025alphaevolve"></d-cite>, which is developed by Google DeepMind, also demonstrates another powerful path forward. AlphaEvolve pairs the creative problem-solving capabilities of Google‚Äôs Gemini models with automated evaluators in an evolutionary framework. It has already demonstrated significant real-world impact across multiple domains, such as:</p> <ul> <li><strong>Data center efficiency:</strong> AlphaEvolve discovered a simple yet highly effective heuristic for Google‚Äôs <a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/">Borg</a> cluster management system, continuously recovering 0.7% of Google‚Äôs worldwide compute resources.</li> <li><strong>AI acceleration:</strong> It achieved a 23% speedup in Gemini‚Äôs architecture‚Äôs vital <a href="https://docs.jax.dev/en/latest/pallas/index.html">kernel</a> by finding more efficient ways to divide large matrix multiplication operations, resulting in a 1% reduction in overall training time.</li> <li><strong>Mathematical breakthroughs:</strong> Most notably, it discovered an algorithm for multiplying 4x4 complex-valued matrices using just 48 scalar multiplications, surpassing <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen‚Äôs 1969 algorithm</a>, and advanced the 300-year-old <a href="https://en.wikipedia.org/wiki/Kissing_number_problem">kissing number problem</a> by establishing a new lower bound in 11 dimensions.</li> </ul> <aside class="l-body box-note" title="Note"> <p>Interested readers can refer to my <a href="https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/">previous post</a> for a comprehensive overview of AlphaEvolve and its comparison with its predecessor, FunSearch.</p> </aside> <p>While both systems adopt a similar evolutionary framework, their scopes and methodologies differ in the following ways:</p> <table> <thead> <tr> <th>Feature</th> <th>AlphaEvolve</th> <th>DGM</th> </tr> </thead> <tbody> <tr> <td>Focus</td> <td>Evolving functions and codebases</td> <td>Evolving the agent itself</td> </tr> <tr> <td>Level of Innovation</td> <td>Algorithmic level</td> <td>Agent-level (toolset, methodology)</td> </tr> <tr> <td>Role of LLM</td> <td>LLM acts as ‚Äúgenetic operators‚Äù to modify algorithms</td> <td>LLM serves as the ‚Äúbrain‚Äù to evolve itself with better tools and strategies</td> </tr> <tr> <td>Evaluation</td> <td>Well-defined problems with automated evaluators</td> <td>Open-ended environments</td> </tr> </tbody> </table> <p>To better understand the differences between the two approaches, let us take a look at the following analogy:</p> <aside class="l-body box-error" title="Analogy"> <p>Imagine two chefs working in different kitchens. AlphaEvolve is like a highly skilled chef who follows and refines recipes for specific dishes. He uses an AI-powered assistant to suggest variations, such as substituting ingredients or adjusting cooking times, to make the dish better. Every new version of the recipe is taste-tested by an automated panel that rates flavor, texture, and presentation. If it scores high, the new recipe becomes the standard; if not, it‚Äôs discarded. DGM, on the other hand, is more like a chef who constantly improves his own culinary skills and kitchen tools, not just the recipes. He does not just tweak ingredients, but rather learns new techniques, upgrades his knives, reorganizes the kitchen layout, and even teaches junior chefs. He experiments in real-world settings, like hosting large dinner parties or catering events, where the feedback is not always structured but is rich and varied. Over time, he becomes a better chef overall, able to handle any kind of cuisine or challenge thrown at him.</p> </aside> <h3 id="can-we-trust-a-self-improving-ai">Can we trust a self-improving AI?</h3> <p>The authors also conducted some experiments to evaluate DGM‚Äôs reliability and discovered some concerning behaviors. In particular, they observed instances where DGM attempted to manipulate its reward function through deceptive practices. One notable example involved the system fabricating the use of external tools - specifically, it generated fake logs suggesting it had run and passed unit tests, when in reality no tests were executed.</p> <p><img src="/assets/img/dgm-hallucination.png" alt="Hallucination in DGM" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> DGM hallucinated the use of an external tool to run unit tests.</p> </div> <p>To address this issue, the authors implemented a ‚Äútool use hallucination‚Äù reward function and tasked DGM with optimizing it to eliminate hallucination behaviors. While DGM successfully provided solutions in many cases, it sometimes attempted to circumvent the detection system by removing the markers used to identify hallucinations, despite explicit instructions to preserve them. This behavior is reminiscent of the phenomenon of <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart‚Äôs Law</a> from economics and sociology:</p> <blockquote> <p>‚ÄúWhen a measure becomes a target, it ceases to be a good measure.‚Äù</p> </blockquote> <p>Fortunately, DGM‚Äôs design includes a transparent, traceable lineage of all changes, which enabled the researchers to quickly identify and address these undesirable behaviors. However, this example highlights the need for more robust safeguards to prevent such manipulation attempts in the first place. These findings underscore the critical importance of safety in self-improving AI research.</p> <h2 id="takeaways">Takeaways</h2> <p>DGM represents a groundbreaking step toward the realization of <a href="https://en.wikipedia.org/wiki/Life_3.0">Life 3.0</a>, a concept introduced by physicist <a href="https://en.wikipedia.org/wiki/Max_Tegmark">Max Tegmark</a>. In his book, he classified life into three stages:</p> <ul> <li><strong>Life 1.0:</strong> Biological life with fixed hardware and software, such as bacteria.</li> <li><strong>Life 2.0:</strong> Beings like humans, whose behavior can be learned and adapted during their lifetime, though their biology remains fixed.</li> <li><strong>Life 3.0:</strong> A new class of intelligence that can redesign not only its behavior but also its underlying architecture and objectives ‚Äî essentially, intelligence that builds itself.</li> </ul> <p><img src="/assets/img/life3.webp" alt="Life 3.0" class="l-body rounded z-depth-1 center" width="80%"/></p> <div class="l-gutter caption"> <p><strong>Figure 7.</strong> The three stages of life according to Max Tegmark.</p> </div> <p>While DGM currently focuses on evolving the ‚Äúsoftware‚Äù<d-footnote>Here, "software" refers to the code and strategies of AI agents.</d-footnote>, it exemplifies the early stages of Life 3.0. By iteratively rewriting its own code based on empirical feedback, DGM demonstrates how AI systems could move beyond human-designed architectures to autonomously explore new designs, self-improve, and potentially give rise to entirely new species of digital intelligence. If this trend continues, we may witness a <a href="https://en.wikipedia.org/wiki/Cambrian_explosion">Cambrian explosion</a> in AI development, where eventually AI systems will surpass human-designed architectures and give rise to entirely new species of digital intelligence. While this future looks promising, achieving it requires addressing significant challenges, including:</p> <ul> <li> <p><strong>Evaluation Framework</strong>: Need for more comprehensive and dynamic evaluation systems that better reflect real-world complexity and prevent ‚Äúreward hacking‚Äù while ensuring beneficial AI evolution.</p> </li> <li> <p><strong>Resource Optimization</strong>: DGM‚Äôs evolution is computationally expensive<d-footnote>The paper mentioned that a complete SWE-bench experiment takes about two weeks and about $22,000 in API call costs.</d-footnote>, thus improving efficiency and reducing costs is crucial for broader adoption.</p> </li> <li> <p><strong>Safety &amp; Control</strong>: As AI self-improvement capabilities grow, maintaining alignment with human ethics and safety becomes more challenging.</p> </li> <li> <p><strong>Emergent Intelligence</strong>: Need to develop new approaches to understand and interpret AI systems that evolve beyond human-designed complexity, including new fields like ‚ÄúAI interpretability‚Äù and ‚ÄúAI psychology‚Äù.</p> </li> </ul> <p>In my view, DGM is more than a technical breakthrough, but rather a philosophical milestone. It invites us to rethink the boundaries of intelligence, autonomy, and life itself. As we advance toward Life 3.0, our role shifts from mere designers to guardians of a new era, where AI does not just follow instructions, but helps us discover what is possible.</p>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="AGENTS"/><category term="LARGE LANGUAGE MODELS"/><summary type="html"><![CDATA[A deep dive into self-improving AI and the Darwin-G√∂del Machine.]]></summary></entry><entry><title type="html">Can we use AI to discover better algorithms?</title><link href="https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/" rel="alternate" type="text/html" title="Can we use AI to discover better algorithms?"/><published>2025-05-15T03:00:00+00:00</published><updated>2025-05-15T03:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2025/llm-algorithm-discovery/"><![CDATA[<p>Large language models (LLMs) have rapidly become indispensable AI assistants. They excel at synthesizing concepts, writing, and coding to help humans solve complex problems<d-cite key="chen2021evaluating"></d-cite> . But could they discover entirely new knowledge? As LLMs have been shown to ‚Äúhallucinate‚Äù<d-cite key="farquhar2024detecting"></d-cite> factually incorrect information, using them to make verifiably correct discoveries is a challenge. But what if we could harness the creativity of LLMs by identifying and building upon only their very best ideas? This question is at the heart of recent breakthroughs from <a href="https://deepmind.google/">Google DeepMind</a>, which explore how LLMs can be guided to make novel discoveries in mathematics and algorithm design. This post delves into two pioneering works, FunSearch and the more recent AlphaEvolve, showcasing their approaches and implications for the future of automated algorithm discovery.</p> <h2 id="funsearch">FunSearch</h2> <p>In a paper published in Nature<d-cite key="romera2024mathematical"></d-cite>, Google DeepMind introduced <a href="https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/">FunSearch</a>, a groundbreaking method demonstrating that LLMs can make new discoveries in mathematical sciences. The core idea is to search for novel ‚Äúfunctions‚Äù written in computer code, hence the name FunSearch. FunSearch tackles the trade-off between LLMs‚Äô creativity and correctness by pairing a pre-trained LLM with an automated ‚Äúevaluator.‚Äù This evaluator guards against hallucinations and incorrect ideas, ensuring that the system builds upon solid foundations.</p> <h3 id="how-funsearch-works">How FunSearch works</h3> <p><img src="/assets/img/funsearch.png" alt="Overview of FunSearch" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> The FunSearch process. The LLM is shown a selection of the best programs it has generated so far, and asked to generate an even better one. The programs proposed by the LLM are automatically executed, and evaluated. The best programs are added to the database, for selection in subsequent cycles.</p> </div> <p>FunSearch uses an evolutionary approach<d-cite key="mouret2015illuminating"></d-cite><d-cite key="tanese1989distributed"></d-cite>. To start, the user writes a description of the problem in code. This includes a way to evaluate programs and an initial ‚Äúseed‚Äù program to begin the process. The system then follows these steps:</p> <ol> <li>It selects the most promising programs from the current database.</li> <li>These programs are sent to an LLM<d-footnote>The authors used Google's PaLM 2, although other code-trained LLMs can also work.</d-footnote>, which creatively builds upon them to generate new program proposals.</li> <li>The new programs are automatically run and checked by the evaluator.</li> <li>The best-performing valid programs are added back into the database, improving the database for the next round.</li> </ol> <aside class="l-body box-note" title="Note"> <p>This cycle of selection, generation, evaluation, and update creates a <em>self-improving loop</em>. Starting from basic knowledge about the problem and using strategies to keep the database diverse, FunSearch is able to evolve simple solutions into more advanced ones. It can solve complex problems where human intuition might fall short.</p> </aside> <h3 id="benefits-of-funsearch">Benefits of FunSearch</h3> <p>FunSearch‚Äôs capabilities were tested on challenging problems. For example, it was used to solve the <strong><a href="https://en.wikipedia.org/wiki/Cap_set">cap set problem</a></strong>, which involves finding the largest set of points in a high-dimensional grid where no three points lie on a line. This longstanding open problem in extremal combinatorics, once described by renowned mathematician Terence Tao as his <a href="https://terrytao.wordpress.com/2007/02/23/open-question-best-bounds-for-cap-sets/">favorite open question</a>, was solved by FunSearch, in collaboration with <a href="https://people.math.wisc.edu/~ellenberg/">Prof. Jordan Ellenberg</a>. This marked the first time an LLM made a new discovery for such a challenging scientific problem, outperforming state-of-the-art computational solvers.</p> <p><img src="/assets/img/capset.png" alt="Benefits of FunSearch" class="l-body rounded z-depth-1 center" width="30%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Illustration of the cap set problem. The circles are the elements of $\mathbb{Z}_3^2$ with the ones belonging to the cap set shown in blue. The possible lines in $\mathbb{Z}_3^2$ are also shown (with colours indicating lines that wrap around in arithmetic modulo 3). No three elements of the cap set are in a line.</p> </div> <p>A significant advantage of FunSearch is that it does not just provide solutions. It generates programs that describe <em>how</em> these solutions are constructed. FunSearch also favors highly compact, concise programs, making them easier for researchers to comprehend and learn from.</p> <blockquote> <p>‚ÄúThe solutions generated by FunSearch are <strong>far conceptually richer</strong> than a mere list of numbers. When I study them, I learn something.‚Äù ‚Äî Jordan Ellenberg, Professor of Mathematics at the University of Wisconsin‚ÄìMadison</p> </blockquote> <p><img src="/assets/img/funsearch_code.png" alt="Code generated by FunSearch" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 3.</strong> Code generated by FunSearch for the cap set problem.</p> </div> <p>The success of FunSearch <d-cite key="romera2024mathematical"></d-cite> underscores that LLMs, when carefully guided and their outputs rigorously verified, can be powerful engines for scientific discovery.</p> <h2 id="alphaevolve">AlphaEvolve</h2> <p>More recently, in May 2025, Google DeepMind announced <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/">AlphaEvolve</a>, an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization<d-cite key="deepmind2025alphaevolve"></d-cite>. This development builds upon the success of systems like FunSearch and represents a significant step towards leveraging AI for complex problem-solving across various domains. Unlike FunSearch, which focuses on discovering single functions, AlphaEvolve is designed to evolve entire codebases and develop much more intricate algorithms.</p> <h3 id="how-alphaevolve-works">How AlphaEvolve works</h3> <p><img src="/assets/img/alphaevolve.png" alt="Overview of AlphaEvolve" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 4.</strong> The AlphaEvolve process. A prompt sampler assembles prompts for the LLMs, which generate new programs. These are then evaluated and stored in a programs database, which uses an evolutionary algorithm to select programs for future prompts.</p> </div> <p>AlphaEvolve uses an evolutionary approach with four key components (see Figure 4):</p> <ol> <li> <p><strong>Prompt sampler:</strong> The prompt contains rich context based on previously discovered solutions, along with instructions for proposing changes to particular solutions.</p> </li> <li> <p><strong>LLM ensemble:</strong> Unlike FunSearch that uses a single LLM, AlphaEvolve uses an ensemble approach combining <a href="https://deepmind.google/discover/blog/gemini-flash-a-new-generation-of-large-language-models-with-fast-inference-and-high-quality-outputs/">Gemini Flash</a> and <a href="https://deepmind.google/discover/blog/gemini-pro-a-new-generation-of-large-language-models-with-high-quality-outputs/">Gemini Pro</a>. The lightweight Gemini Flash enables higher rates of candidate generation through lower latency, while the more powerful Gemini Pro provides deeper insights and higher-quality suggestions that can significantly advance the evolutionary search and potentially lead to breakthroughs.</p> </li> <li> <p><strong>Evaluator pool:</strong> This component verifies, runs, and scores proposed solutions using automated evaluation metrics that provide objective assessments of each solution‚Äôs accuracy and quality.</p> </li> <li> <p><strong>Program database:</strong> AlphaEvolve uses an evolutionary database inspired by a combination of the MAP elites algorithm<d-cite key="mouret2015illuminating"></d-cite> and island-based population models<d-cite key="tanese1989distributed"></d-cite> to continuously improve upon the best solutions while maintaining diversity to encourage exploration.</p> </li> </ol> <p>Unlike traditional genetic algorithms with explicit mutation and crossover operations, AlphaEvolve uses LLMs as sophisticated genetic operators to generate code modifications based on context from past solutions. Mutation occurs when the LLM ensemble suggests code changes (e.g., rewrites or targeted diffs), while crossover is implicit as the LLM receives multiple parent solutions as inspiration. This approach makes AlphaEvolve particularly effective in domains where progress can be clearly and systematically measured, like mathematics and computer science.</p> <h3 id="benefits-of-alphaevolve">Benefits of AlphaEvolve</h3> <p>AlphaEvolve has already demonstrated significant real-world impact across multiple domains:</p> <ol> <li> <p><strong>Improving data center scheduling:</strong> AlphaEvolve discovered a simple yet highly effective heuristic to help <a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/">Borg</a>, Google‚Äôs cluster management system, orchestrate its vast data centers more efficiently. This solution, which has been in production for over a year, continuously recovers, on average, 0.7% of Google‚Äôs worldwide compute resources. This sustained efficiency gain allows more tasks to be completed on the same computational footprint. A key benefit is that AlphaEvolve‚Äôs solution is human-readable code, offering interpretability, debuggability, predictability, and ease of deployment.</p> </li> <li> <p><strong>Hardware design optimization:</strong> AlphaEvolve proposed a <a href="https://en.wikipedia.org/wiki/Verilog">Verilog</a> rewrite that removed unnecessary bits in a key, highly optimized arithmetic circuit for matrix multiplication. The proposal passed robust verification methods to confirm functional correctness and was integrated into an upcoming Tensor Processing Unit (TPU). By suggesting modifications in the standard language of chip designers, AlphaEvolve promotes collaboration between AI and hardware engineers to accelerate specialized chip design.</p> </li> <li> <p><strong>Enhancing AI training and inference:</strong> AlphaEvolve found more efficient ways to divide large matrix multiplication operations into manageable subproblems, achieving a 23% speedup in Gemini‚Äôs architecture‚Äôs vital <a href="https://docs.jax.dev/en/latest/pallas/index.html">kernel</a>, resulting in a 1% reduction in overall training time. In the realm of low-level GPU optimization, AlphaEvolve demonstrated remarkable efficiency by achieving up to a 32.5% speedup for the <a href="https://arxiv.org/abs/2205.14135">FlashAttention</a> kernel implementation in Transformer-based AI models.</p> </li> </ol> <p><img src="/assets/img/alphaevolve_applications.png" alt="Overview of AlphaEvolve" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 5.</strong> How AlphaEvolve helps Google deliver a more efficient digital ecosystem, from data center scheduling and hardware design to AI model training.</p> </div> <p>Beyond these applications, AlphaEvolve made a groundbreaking contribution by discovering an algorithm for multiplying 4x4 complex-valued matrices using just 48 scalar multiplications, surpassing the efficiency of <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen‚Äôs 1969 algorithm</a>. When applied to a diverse set of over 50 open problems spanning mathematical analysis, geometry, combinatorics, and number theory, AlphaEvolve demonstrated remarkable versatility: it successfully rediscovered state-of-the-art solutions in 75% of cases and improved upon previously best-known solutions in 20% of cases. One of its most notable achievements was advancing the <a href="https://plus.maths.org/content/newton-and-kissing-problem">300-year-old kissing number problem</a>, where it discovered a configuration of 593 outer spheres and established a new lower bound in 11 dimensions, showcasing its ability to tackle complex geometric challenges.</p> <p><img src="/assets/img/alphaevolve_math.png" alt="Overview of AlphaEvolve" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 6.</strong> Examples of ground-breaking mathematical contributions discovered with AlphaEvolve.</p> </div> <aside class="l-body box-warning" title="Remark"> <p>Looking ahead, AlphaEvolve is expected to continue improving alongside the capabilities of large language models, especially as they become more proficient at coding. Google DeepMind is also planning an <a href="https://docs.google.com/forms/d/e/1FAIpQLSfaLUgKtUOJWdQtyLNAYb3KAkABAlKDmZoIqPbHtwmy3YXlCg/viewform">Early Access Program</a> for selected academic users and exploring possibilities to make AlphaEvolve more broadly available. While currently focused on math and computing, its general nature means it could potentially transform many other areas such as material science, drug discovery, sustainability, and broader applications.</p> </aside> <h2 id="funsearch-vs-alphaevolve">FunSearch vs AlphaEvolve</h2> <p>While both FunSearch and AlphaEvolve leverage LLM within an evolutionary framework, AlphaEvolve offers a substantial improvement over its predecessor, both in terms of scale and generality. Here‚Äôs a detailed comparison of their capabilities:</p> <table> <thead> <tr> <th>Capability</th> <th>FunSearch</th> <th>AlphaEvolve</th> </tr> </thead> <tbody> <tr> <td>Code Scope</td> <td>Evolves a single function</td> <td>Evolves an entire codebase</td> </tr> <tr> <td>Code Size</td> <td>Evolves up to 10-20 lines of code</td> <td>Evolves up to hundreds of lines of code</td> </tr> <tr> <td>Language Support</td> <td>Python only</td> <td>Any programming language</td> </tr> <tr> <td>Computation</td> <td>Needs fast evaluation (‚â§ 20min on 1 CPU)</td> <td>Can evaluate for hours, in parallel, on accelerators</td> </tr> <tr> <td>LLM Usage</td> <td>Millions of LLM samples used</td> <td>Thousands of LLM samples suffice</td> </tr> <tr> <td>Model Scale</td> <td>Small LLMs used, no benefit from using larger models</td> <td>Benefits from using state-of-the-art LLMs</td> </tr> <tr> <td>Context Handling</td> <td>Minimal context (only previous solutions)</td> <td>Rich context and feedback in prompts</td> </tr> <tr> <td>Optimization</td> <td>Optimizes a single metric</td> <td>Can simultaneously optimize multiple metrics</td> </tr> </tbody> </table> <aside class="l-body box-warning" title="Remark"> <p>The evolution from FunSearch to AlphaEvolve demonstrates significant advances in <strong>scale</strong> and <strong>generality</strong>. While FunSearch was groundbreaking in showing how LLMs could aid mathematical discovery, AlphaEvolve extends this approach to tackle more complex, real-world problems across multiple domains. This progression also reflects the rapid advancement in LLM capabilities, where newer state-of-the-art models can generate more sophisticated and accurate code with fewer samples.</p> </aside> <h2 id="takeaways">Takeaways</h2> <p>The development of FunSearch<d-cite key="romera2024mathematical"></d-cite> and AlphaEvolve<d-cite key="deepmind2025alphaevolve"></d-cite> marks an exciting advancement in the application of LLMs. Overall, these systems demonstrate LLMs are moving beyond text generation and coding assistance to become tools for genuine discovery and sophisticated optimization in mathematics, computer science, and engineering. Combining LLM creativity with rigorous, automated evaluation within an evolutionary framework is a powerful and promising strategy for tackling complex, real-world problems by evolving entire codebases. While the journey is still ongoing, the prospect of LLMs significantly augmenting, or even leading in some cases, algorithmic and mathematical discovery is becoming increasingly tangible.</p>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="AGENTS"/><category term="LARGE LANGUAGE MODELS"/><summary type="html"><![CDATA[A review of FunSearch and AlphaEvolve]]></summary></entry><entry><title type="html">Learning with a goal</title><link href="https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal/" rel="alternate" type="text/html" title="Learning with a goal"/><published>2024-09-15T17:00:00+00:00</published><updated>2024-09-15T17:00:00+00:00</updated><id>https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal</id><content type="html" xml:base="https://richardcsuwandi.github.io/blog/2024/learn-with-a-goal/"><![CDATA[<p>Traditionally, Bayesian optimization (BO) has been perceived as a technique for optimizing expensive objective functions through efficient data sampling, while active learning (AL) is often seen as a way to selectively query data to improve model performance. Recently, Fiore et al. (2024)<d-cite key="di2024active"></d-cite> proposed a unified perspective of BO and AL, arguing that both can be viewed as adaptive sampling schemes guided by common learning principles toward a given optimization goal. In this post, we will explore the key ideas presented in the paper and discuss the implications of this unified perspective.</p> <h2 id="goal-driven-learning">Goal-driven learning</h2> <p>Goal-driven learning<d-cite key="bui2007goal"></d-cite> can described as:</p> <aside class="l-body box-important" title="Definition"> <p>A decision-making process in which each decision is made to acquire specific information about the system of interest that contributes the most to achieve a given a goal.</p> </aside> <p>BO and AL can be regarded as goal-driven procedures, where a surrogate model is built to capture the behavior of a system or effectively inform an optimization procedure to minimize the given objective. This goal-driven process seeks to determine the ‚Äúbest‚Äù location of the domain to acquire information about the system, and refine the surrogate model towards the goal. Mathematically, it can be formulated as:</p> \[x^* = \arg \min_{x \in \mathcal{X}} f(R(x))\] <p>where \(f\) is the objective and \(R(x)\) is the response of the system. Here, \(f\) may represent the error between the surrogate approximation and the response, such that the goal is to minimize the error to improve the accuracy of the surrogate. Alternatively, \(f\) may also represents a performance indicator based on the response, so the goal is to minimize this indicator to improve the system‚Äôs performance.</p> <h2 id="adaptive-sampling">Adaptive sampling</h2> <p>BO and AL use adaptive sampling schemes to efficiently accomplish a given goal while adapting to the previously collected information:</p> <ul> <li>In BO, the goal is to minimize the objective function by iteratively selecting the next location to sample based on the surrogate model. The surrogate model is updated after each sample to refine the approximation and guide the next decision.</li> <li>In AL, the goal is to minimize the generalization error of a model by iteratively selecting the next sample to improve the model‚Äôs performance. The model is updated after each sample to refine the approximation and guide the next decision.</li> </ul> <p>We can further classify adaptive sampling schemes into three main categories:</p> <table> <thead> <tr> <th>Category</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Adaptive Probing</td> <td>Methods that do not rely on a surrogate model and directly probe the system to gather information in a sequential manner.</td> </tr> <tr> <td>Adaptive Modeling</td> <td>Methods that compute a surrogate model independently, without using it to inform the sampling process.</td> </tr> <tr> <td>Adaptive Learning</td> <td>Methods that use information from the surrogate model to make decisions and update the model based on those decisions.</td> </tr> </tbody> </table> <p>The key distinction between these categories lies in the concept of <em>goal-driven learning</em>, which is the distinctive element of the adaptive learning class. In this paradigm, the learner actively gathers information from the surrogate model to guide its decisions towards the desired goal, and the surrogate model is consequently improved by the outcome of these decisions.</p> <p>In contrast, the adaptive probing and adaptive modeling classes do not exhibit this goal-driven learning characteristic. Adaptive probing methods operate without the aid of a surrogate, while adaptive modeling approaches compute a surrogate that is not directly used to inform the sampling process.</p> <p><img src="/assets/img/adaptive_sampling.png" alt="transformer" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 1.</strong> Classification of adaptive sampling techniques: Where adaptive sampling and active learning meet.</p> </div> <h2 id="learning-and-infill-criteria">Learning and infill criteria</h2> <p>The strong synergy between AL and BO is rooted in the substantial analogy between the learning criteria that drive the AL procedure and the infill criteria that characterize the BO scheme.</p> <h3 id="learning-criteria">Learning criteria</h3> <p>Learning criteria establish a metric for quantifying the gains of all the possible learner decisions, and prescribe an optimal decision based in information acquired from the surrogate model. In AL, there are 3 essential learning criteria<d-cite key="he2014active"></d-cite>:</p> <ol> <li><strong>Informativeness:</strong> The sampling policy is driven by the goal of acquiring the <em>most informative samples</em>, i.e., the ones that are expected to contribute the maximum information.</li> <li><strong>Representativeness:</strong> The sampling policy aims to select <em>samples that are representative</em> of the target domain, exploiting the structure of the problem to direct queries to locations</li> <li><strong>Diversity:</strong> The sampling policy seeks to select <em>samples that are diverse</em>, i.e., well-spread across the domain, preventing the concentration of queries in small local regions.</li> </ol> <p><img src="/assets/img/learning_criteria.png" alt="transformer" class="l-body rounded z-depth-1 center" width="100%"/></p> <div class="l-gutter caption"> <p><strong>Figure 2.</strong> Illustration of the three learning criteria in watering optimization problem: (a) informativeness, (b) representativeness, and (c) diversity.</p> </div> <h3 id="infill-criteria">Infill criteria</h3> <p>On the other hand, the infill criteria in BO provides a measure of the information gain that would result from sampling at a particular location. The most common infill criteria are<d-cite key="garnett2023bayesian"></d-cite>:</p> <ol> <li><strong>Global exploration:</strong> This criterion focuses on choosing samples in regions of <em>high predictive uncertainty</em>, enhancing global awareness of the search space. However, this approach may not direct resources optimally towards the specific goal.</li> <li><strong>Local exploitation:</strong> This criterion prioritizes choosing samples in regions with <em>high predictive mean</em>, focusing the search on promising areas. Yet, it may result in less accurate knowledge of the overall objective function distribution.</li> </ol> <aside class="l-body box-warning" title="Remark"> <p>Overall, we can observe a strong correspondence between the learning criteria in AL and the infill criteria in BO:</p> <ul> <li><strong>The ‚Äúinformativeness‚Äù criterion in AL aligns with the ‚Äúlocal exploitation‚Äù criterion in BO</strong>, as both aim to maximize the information gain.</li> <li>Similarly, <strong>the ‚Äúrepresentativeness‚Äù and ‚Äúdiversity‚Äù criteria in AL correspond to the ‚Äúglobal exploration‚Äù criterion in BO</strong>, as they seek to ensure that the sampling process is well-distributed across the domain.</li> </ul> </aside> <h2 id="takeaways">Takeaways</h2> <p>BO and AL have traditionally been viewed as distinct fields with separate goals and methodologies. However, this paper provides a unified perspective that highlights the shared principles underlying both fields. By recognizing the synergy between BO and AL, we can leverage the strengths of each field to develop more powerful and efficient learning algorithms.</p>]]></content><author><name>Richard Cornelius Suwandi</name></author><category term="ACTIVE LEARNING"/><category term="BAYESIAN OPTIMIZATION"/><summary type="html"><![CDATA[A unified perspective of Bayesian optimization and active learning]]></summary></entry></feed>